{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp core.block_types\n",
    "import os\n",
    "from nbdev.showdoc import *\n",
    "if not os.path.exists('settings.ini'):\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block types\n",
    "\n",
    "> Types of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    from graphviz import *\n",
    "    imported_graphviz = True\n",
    "except:\n",
    "    imported_graphviz = False\n",
    "\n",
    "# block_types\n",
    "from block_types.core.data_conversion import DataConverter, PandasConverter\n",
    "from block_types.core.utils import save_csv, save_parquet, save_multi_index_parquet, save_keras_model, save_csv_gz, read_csv, read_csv_gz\n",
    "from block_types.core.utils import DataIO, SklearnIO, NoSaverIO, ModelPlotter\n",
    "from block_types.core.utils import camel_to_snake\n",
    "from block_types.config import defaults as dflt\n",
    "from block_types.utils.utils import set_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Component (ClassifierMixin, TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Base component class used in our Pipeline.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  name: Optional[str] = None,\n",
    "                  data_converter: Optional[DataConverter] = None,\n",
    "                  data_io: Optional[DataIO] = None,\n",
    "                  model_plotter: Optional[ModelPlotter] = None,\n",
    "                  logger=None,\n",
    "                  verbose: int = 0,\n",
    "                  **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize attributes and fields.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : estimator (classifier or transformer) or None, optional\n",
    "            Estimator being wrapped.\n",
    "        name : Pipeline or None, optional\n",
    "            Name of component. If not provided, it is inferred from the name of the \n",
    "            estimator's class, or the name of the custom class defining the componet.\n",
    "        data_converter : DataConverter or None, optional\n",
    "            Converts incoming data to format expected by component, and convert \n",
    "            outgoing result to format expected by caller.\n",
    "        data_io : DataIO or None, optional\n",
    "            Manages data serialization and deserialization.\n",
    "        model_plotter : ModelPlotter or None, optional\n",
    "            Helper object that allows to retrieve information to be shown about this \n",
    "            component, as part of a Pipeline diagram.\n",
    "        logger : logging.logger or None, optional\n",
    "            Logger used to write messages\n",
    "        verbose : int, optional\n",
    "            Verbosity, 0: warning or critical, 1: info, 2: debug.\n",
    "        \"\"\"\n",
    "\n",
    "        # logger used to display messages\n",
    "        if logger is None:\n",
    "            self.logger = set_logger ('block_types', verbose=verbose)\n",
    "        else:\n",
    "            self.logger = logger\n",
    "\n",
    "        # name of current component, for logging and plotting purposes\n",
    "        self._determine_component_name (name, estimator)\n",
    "\n",
    "        # object that manages loading / saving\n",
    "        if data_io is None:\n",
    "            self.data_io = DataIO (component=self, **kwargs)\n",
    "        else:\n",
    "            self.data_io = data_io\n",
    "            self.data_io.setup (self)\n",
    "\n",
    "        # estimator (ML model)\n",
    "        self.estimator = estimator\n",
    "\n",
    "        # data converter\n",
    "        if data_converter is None:\n",
    "            self.data_converter = PandasConverter (**kwargs)\n",
    "        else:\n",
    "            self.data_converter = data_converter\n",
    "\n",
    "        # plotting model component\n",
    "        if model_plotter is None:\n",
    "            self.model_plotter = ModelPlotter (component=self, **kwargs)\n",
    "        else:\n",
    "            self.model_plotter = model_plotter\n",
    "            self.model_plotter.set_component (self)\n",
    "\n",
    "    def _determine_component_name (self, name: Optional[str], estimator) -> None:\n",
    "        \"\"\"\n",
    "        Determines an appropriate name for the component if not provided by input.\n",
    "        \n",
    "        If not provided, it is inferred from the name of the estimator's class, or \n",
    "        the name of the custom class defining the componet.\n",
    "        \"\"\"\n",
    "        self.class_name = self.__class__.__name__\n",
    "        if (self.class_name in __all__) and (estimator is not None):\n",
    "            self.class_name = estimator.__class__.__name__\n",
    "\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = camel_to_snake (self.class_name)\n",
    "\n",
    "    def fit (self, X, y=None, load=True, save=True):\n",
    "        \"\"\"\n",
    "        Estimates the parameters of the component based on given data X and labels y.\n",
    "        \n",
    "        Uses the previously fitted parameters if they're found in disk and overwrite \n",
    "        is False.\n",
    "        \"\"\"\n",
    "        self.logger.info (f'fitting {self.name}')\n",
    "        \n",
    "        previous_estimator = None\n",
    "        if load and not self.data_io.overwrite:\n",
    "            previous_estimator = self.data_io.load_estimator()\n",
    "            \n",
    "        if previous_estimator is None:\n",
    "            X, y = self.data_converter.convert_before_fitting (X, y)\n",
    "            self._fit (X, y)\n",
    "            self.data_converter.convert_after_fitting (X)\n",
    "            if save:\n",
    "                self.data_io.save_estimator ()\n",
    "        else:\n",
    "            self.estimator = previous_estimator\n",
    "            self.logger.info (f'loaded pre-trained {self.name}')\n",
    "        return self\n",
    "\n",
    "    def transform (self, X, load=True, save=True):\n",
    "        \"\"\"\n",
    "        Transforms the data X and returns the transformed data.\n",
    "        \n",
    "        Uses the previously transformed data if it's found in disk and overwrite \n",
    "        is False.\n",
    "        \"\"\"\n",
    "        self.logger.info (f'applying {self.name} transform')\n",
    "        result= self._compute_result (X, self._transform, load=load, save=save)\n",
    "        return result\n",
    "\n",
    "    def predict (self, X, load=True, save=True):\n",
    "        \"\"\"\n",
    "        Predicts binary labels and returns result.\n",
    "        \n",
    "        Uses previously stored predictions if found in disk and overwrite is False.\n",
    "        \"\"\"\n",
    "        self.logger.info (f'applying {self.name} inference')\n",
    "        return self._compute_result (X, self._predict, new_columns=['prediction'], load=load, save=save)\n",
    "\n",
    "    def _compute_result (self, X, result_func, load=True, save=True, **kwargs):\n",
    "        previous_result = None\n",
    "        if load and not self.data_io.overwrite:\n",
    "            previous_result = self.data_io.load_result()\n",
    "        if previous_result is None:\n",
    "            X = self.data_converter.convert_before_transforming (X, **kwargs)\n",
    "            result = result_func (X)\n",
    "            result = self.data_converter.convert_after_transforming (result, **kwargs)\n",
    "            if save:\n",
    "                self.data_io.save_result (result)\n",
    "        else:\n",
    "            result = previous_result\n",
    "            self.logger.info (f'loaded pre-computed result')\n",
    "        return result\n",
    "\n",
    "\n",
    "    def _fit (self, X, y=None):\n",
    "        if self.estimator is not None:\n",
    "            self.estimator.fit (X, y)\n",
    "\n",
    "    def _transform (self, X):\n",
    "        if self.estimator is not None:\n",
    "            return self.estimator.transform (X)\n",
    "        else:\n",
    "            raise NotImplementedError ('estimator is None _transform method probably needs to be implemented in subclass')\n",
    "\n",
    "    def show_result_statistics (self, result=None, training_data_flag=False) -> None:\n",
    "        \"\"\"\n",
    "        Show statistics of transformed data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        result: DataFrame or other data structure or None, optional\n",
    "            Transformed data whose statistics we show. If not provided, it is loaded \n",
    "            from disk.\n",
    "        training_data_flag: bool, optional\n",
    "            If True, transformed training data is loaded, otherwise transformed test \n",
    "            data is loaded.\n",
    "        \"\"\"\n",
    "        if result is None:\n",
    "            self.set_training_data_flag (training_data_flag)\n",
    "            df = self.data_io.load_result()\n",
    "        else:\n",
    "            df = result\n",
    "        \n",
    "        if df is not None:\n",
    "            display (self.name)\n",
    "            if callable(getattr(df, 'describe', None)):\n",
    "                display (df.describe())\n",
    "\n",
    "    def assert_equal (self, path_reference_results: str, assert_equal_func=pd.testing.assert_frame_equal, **kwargs):\n",
    "        \"\"\"\n",
    "        Check whether the transformed data is the same as the reference data stored in given path.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_reference_results: str\n",
    "            Path where reference results are stored. The path does not include the \n",
    "            file name, since this is stored as a field of data_io.\n",
    "        assert_equal_func: function, optional\n",
    "            Function used to check whether the values are the same. By defaut, \n",
    "            `pd.testing.assert_frame_equal` is used, which assumes the data type is \n",
    "            DataFrame.\n",
    "        \n",
    "        \"\"\"\n",
    "        type_result = 'training' if self.data_io.training_data_flag else 'test'\n",
    "        self.logger.info (f'comparing {type_result} results for {self.class_name}')\n",
    "        \n",
    "        self.logger.info (f'loading...')\n",
    "        current_results = self.data_io.load_result ()\n",
    "        if self.data_io.training_data_flag:\n",
    "            path_to_reference_file = Path(path_reference_results) / self.data_io.result_file_name_training\n",
    "        else:\n",
    "            path_to_reference_file = Path(path_reference_results) / self.data_io.result_file_name_test\n",
    "        reference_results = self.data_io._load (path_to_reference_file, self.data_io.result_load_func)\n",
    "        self.logger.info (f'comparing...')\n",
    "        assert_equal_func (current_results, reference_results, **kwargs)\n",
    "        self.logger.info (f'equal results\\n')\n",
    "\n",
    "    # ********************************\n",
    "    # setters\n",
    "    # ********************************\n",
    "    def set_training_data_flag (self, training_data_flag):\n",
    "        self.data_io.set_training_data_flag (training_data_flag)\n",
    "\n",
    "    def set_save_result_flag_test (self, save_result_flag_test):\n",
    "        self.data_io.set_save_result_flag_test (save_result_flag_test)\n",
    "\n",
    "    def set_save_result_flag_training (self, save_result_flag_training):\n",
    "        self.data_io.set_save_result_flag_training (save_result_flag_training)\n",
    "\n",
    "    def set_save_result_flag (self, save_result_flag):\n",
    "        self.data_io.set_save_result_flag (save_result_flag)\n",
    "\n",
    "    def set_overwrite (self, overwrite):\n",
    "        self.data_io.set_overwrite (overwrite)\n",
    "\n",
    "    def set_save_fitting (self, save_fitting):\n",
    "        self.data_io.set_save_fitting (save_fitting)\n",
    "\n",
    "# ******************************************\n",
    "# Subclasses of Component.\n",
    "# Most of these are basically the same as GenericComponent, the only difference being that some parameters\n",
    "# are over-riden when constructing the object, to force a specific behavior\n",
    "# ******************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (Component, name='Component', title_level=3)\n",
    "show_doc (Component.__init__, name='__init__', title_level=4)\n",
    "show_doc (Component.fit, name='fit', title_level=4)\n",
    "show_doc (Component.transform, name='transform', title_level=4)\n",
    "show_doc (Component.predict, name='predict', title_level=4)\n",
    "show_doc (Component.show_result_statistics, name='show_result_statistics', title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SamplingComponent (Component):\n",
    "    \"\"\"\n",
    "    Component that makes use of labels in transform method.\n",
    "    \n",
    "    When calling the transform method, one of the columns of the received data \n",
    "    is assumed to contain the ground-truth labels. This allows the transform \n",
    "    method to modify the number of observations, changing the number of rows in \n",
    "    the data and in the labels. See `PandasConverter` class in \n",
    "    `block_types.core.data_conversion`.\n",
    "    \"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  transform_uses_labels=True,\n",
    "                  **kwargs):\n",
    "\n",
    "        # the SamplingComponent over-rides the following parameters:\n",
    "        super().__init__ (estimator=estimator,\n",
    "                          transform_uses_labels=True,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (SamplingComponent, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SklearnComponent (Component):\n",
    "    \"\"\"\n",
    "    Component that saves estimator parameters in pickle format.\n",
    "    \n",
    "    Convenience subclass used when the estimated parameters can be saved in \n",
    "    pickle format. Note that data transformed by this component is still \n",
    "    saved in parquet format by default, since DataFrame is the default format.\n",
    "    These defaults, however, can change. See `SklearnIO` class in \n",
    "    `core.utils`.\n",
    "    \"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  transform_uses_labels=False,\n",
    "                  **kwargs):\n",
    "\n",
    "        data_io = SklearnIO (**kwargs)\n",
    "        \n",
    "        # the SklearnComponent over-rides the following parameters:\n",
    "        super().__init__ (estimator=estimator,\n",
    "                          data_io = data_io,\n",
    "                          transform_uses_labels=False,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (SklearnComponent, name = 'SklearnComponent', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NoSaverComponent (Component):\n",
    "    \"\"\"Component that does not save any data.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  **kwargs):\n",
    "\n",
    "        data_io = NoSaverIO (**kwargs)\n",
    "        \n",
    "        # the NoSaverComponent over-rides the following parameters:\n",
    "        super().__init__ (estimator=estimator,\n",
    "                          data_io=data_io,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (NoSaverComponent, name = 'SklearnComponent', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OneClassSklearnComponent (SklearnComponent):\n",
    "    \"\"\"Component that uses only normal data (labelled with 0) for fitting parameters.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  **kwargs):\n",
    "        super().__init__ (estimator=estimator,\n",
    "                          **kwargs)\n",
    "\n",
    "    def _fit (self, X, y=None):\n",
    "        assert y is not None, 'y must be provided in OneClassSklearnComponent class'\n",
    "        X = X[y==0]\n",
    "\n",
    "        assert self.estimator is not None, 'estimator must be provided in OneClassSklearnComponent class'\n",
    "        self.estimator.fit (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (OneClassSklearnComponent, name = 'OneClassSklearnComponent', title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# TODO: consider removing the following PandasComponent class, since we already have this as default in Component\n",
    "class PandasComponent (Component):\n",
    "    \"\"\"Component that preserves the DataFrame format for incoming data and results.\n",
    "    \n",
    "    This class is redundant at the moment, since this is done by default by the \n",
    "    `Component` class. We might remove this class in future versions.\n",
    "    See `PandasConverter` in `core.data_conversion` for details on the data \n",
    "    conversion performed.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  data_converter=None,\n",
    "                  transform_uses_labels=False,\n",
    "                  transformed_index=None,\n",
    "                  transformed_columns=None,\n",
    "                  **kwargs):\n",
    "\n",
    "        data_converter = PandasConverter (transform_uses_labels=transform_uses_labels,\n",
    "                                          transformed_index=transformed_index,\n",
    "                                          transformed_columns=transformed_columns,\n",
    "                                          **kwargs)\n",
    "\n",
    "        super().__init__ (estimator=estimator,\n",
    "                          data_converter=data_converter,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (PandasComponent, name='PandasComponent', title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Base `Pipeline` class. For specific pipelines see `core.pipeline` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Pipeline (SamplingComponent):\n",
    "    \"\"\"\n",
    "    Pipeline composed of a list of components that run sequentially.\n",
    "    \n",
    "    During training, the components of the list are trained one after the other, \n",
    "    where one component is fed the result of transforming the data with the list \n",
    "    of components located before in the pipeline.\n",
    "    \n",
    "    The `Pipeline` class is a subclass of `SamplingComponent`, which itself is a \n",
    "    subclass of `Component`. This provides the functionality of `Component` \n",
    "    to any implemented pipeline, such as logging the messages, loading / saving the \n",
    "    results, and convert the data format so that it can work as part of other \n",
    "    pipelines with potentially other data formats.\n",
    "    \n",
    "    The `SamplingComponent` class allows the `transform` method to receive both data and \n",
    "    labels. Furthermore, the Pipeline constructor sets `separate_labels=False` by default,\n",
    "    which means that the `fit` method receives the labels combined with the data in the same \n",
    "    input `X`. The rationale for this is to have some components in the pipeline that are \n",
    "    `SamplingComponent` themselves. Such components need the labels to be passed to the \n",
    "    `transform` method, and this method is called during `fit` for all the components except \n",
    "    the last one.\n",
    "    \"\"\"\n",
    "    def __init__ (self, separate_labels = False, **kwargs):\n",
    "        \"\"\"Assigns attributes and calls parent constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        separate_labels: bool, optional\n",
    "            whether or not the fit method receives the labels in a separate `y` vector \n",
    "            or in the same input `X`, as an additional variable. See description of \n",
    "            Pipeline class for more details.\n",
    "        \"\"\"\n",
    "\n",
    "        self.components = []\n",
    "\n",
    "        # we need to create pipeline before calling super().__init__(), since the constructor of Component calls\n",
    "        # a method that is overriden in Pipeline, and this method makes use of components field\n",
    "        super().__init__ (separate_labels = separate_labels, \n",
    "                          **kwargs)\n",
    "\n",
    "        self.set_training_data_flag(False)\n",
    "\n",
    "    @classmethod\n",
    "    def create_pipeline(cls, components, **kwargs):\n",
    "        \"\"\"Create `Pipeline` object of class `cls`, given `components` list.\"\"\"\n",
    "        pipeline = cls(**kwargs)\n",
    "        pipeline.components = components\n",
    "        pipeline.set_training_data_flag(False)\n",
    "        return pipeline\n",
    "\n",
    "    def _fit (self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit components of the pipeline, given data X and labels y.\n",
    "        \n",
    "        By default, y will be None, and the labels are part of `X`, as a variable.\n",
    "        \"\"\"\n",
    "        self.set_training_data_flag (True)\n",
    "        for component in self.components[:-1]:\n",
    "            X = component.fit_transform (X, y)\n",
    "        self.components[-1].fit (X, y)\n",
    "        # self.set_training_data_flag (False)\n",
    "\n",
    "    def _predict (self, X):\n",
    "        \"\"\"Transform data with components of pipeline, and predict labels with last component. \n",
    "        \n",
    "        In the current implementation, we consider prediction a form of mapping, \n",
    "        and therefore a special type of transformation.\"\"\"\n",
    "        self.set_training_data_flag (False)\n",
    "        for component in self.components:\n",
    "            X = component.transform (X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def construct_diagram (self, training_data_flag=None, include_url=False, port=4000, project='block_types'):\n",
    "        \"\"\"\n",
    "        Construct diagram of the pipeline components, data flow and dimensionality.\n",
    "        \n",
    "        By default, we use test data to show the number of observations \n",
    "        in the output of each component. This can be changed passing \n",
    "        `training_data_flag=True`\n",
    "        \"\"\"\n",
    "        training_data_flag = self.get_training_data_flag (training_data_flag)\n",
    "\n",
    "        if include_url:\n",
    "            base_url = f'http://localhost:{port}/{project}'\n",
    "        else:\n",
    "            URL = ''\n",
    "\n",
    "        node_name = 'data'\n",
    "        output = 'train / test'\n",
    "\n",
    "        f = Digraph('G', filename='fsm2.svg')\n",
    "        f.attr('node', shape='circle')\n",
    "\n",
    "        f.node(node_name)\n",
    "\n",
    "        f.attr('node', shape='box')\n",
    "        for component in self.components:\n",
    "            last_node_name = node_name\n",
    "            last_output = output\n",
    "            node_name = component.model_plotter.get_node_name()\n",
    "            if include_url:\n",
    "                URL = f'{base_url}/{component.model_plotter.get_module_path()}.html#{node_name}'\n",
    "            f.node(node_name, URL=URL)\n",
    "            f.edge(last_node_name, node_name, label=last_output)\n",
    "            output = component.model_plotter.get_edge_name(training_data_flag=training_data_flag)\n",
    "\n",
    "        last_node_name = node_name\n",
    "        node_name = 'output'\n",
    "        f.attr('node', shape='circle')\n",
    "        f.edge(last_node_name, node_name, label=output)\n",
    "\n",
    "        return f\n",
    "\n",
    "    def show_result_statistics (self, training_data_flag=None):\n",
    "        \"\"\"\n",
    "        Show statistics about results obtained by each component. \n",
    "        \n",
    "        By default, this is shown on test data, although this can change setting \n",
    "        `training_data_flag=True`\n",
    "        \"\"\"\n",
    "        training_data_flag = self.get_training_data_flag (training_data_flag)\n",
    "\n",
    "        for component in self.components:\n",
    "            component.show_result_statistics(training_data_flag=training_data_flag)\n",
    "\n",
    "    def show_summary (self, training_data_flag=None):\n",
    "        \"\"\"\n",
    "        Show list of pipeline components, data flow and dimensionality.\n",
    "        \n",
    "        By default, we use test data to show the number of observations \n",
    "        in the output of each component. This can be changed passing \n",
    "        `training_data_flag=True`\n",
    "        \"\"\"\n",
    "        training_data_flag = self.get_training_data_flag (training_data_flag)\n",
    "\n",
    "        node_name = 'data'\n",
    "        output = 'train / test'\n",
    "\n",
    "        for i, component in enumerate(self.components):\n",
    "            node_name = component.model_plotter.get_node_name()\n",
    "            output = component.model_plotter.get_edge_name(training_data_flag=training_data_flag)\n",
    "            print (f'{\"-\"*100}')\n",
    "            print (f'{i}: {node_name} => {output}')\n",
    "\n",
    "\n",
    "    def get_training_data_flag (self, training_data_flag=None):\n",
    "        if training_data_flag is None:\n",
    "            if self.data_io.training_data_flag is not None:\n",
    "                training_data_flag = self.data_io.training_data_flag\n",
    "            else:\n",
    "                training_data_flag = False\n",
    "\n",
    "        return training_data_flag\n",
    "\n",
    "    def assert_equal (self, path_reference_results, assert_equal_func=pd.testing.assert_frame_equal, **kwargs):\n",
    "        \"\"\"Compare results stored in current run against reference results stored in given path.\"\"\"\n",
    "\n",
    "        for component in self.components:\n",
    "            component.assert_equal (path_reference_results, assert_equal_func=assert_equal_func, **kwargs)\n",
    "        self.logger.info ('both pipelines give the same results')\n",
    "        print ('both pipelines give the same results')\n",
    "\n",
    "    # *************************\n",
    "    # setters\n",
    "    # *************************\n",
    "    def set_training_data_flag (self, training_data_flag):\n",
    "        super().set_training_data_flag (training_data_flag)\n",
    "        for component in self.components:\n",
    "            component.set_training_data_flag (training_data_flag)\n",
    "\n",
    "    def set_save_result_flag_test (self, save_result_flag_test):\n",
    "        super().set_save_result_flag_test (save_result_flag_test)\n",
    "        for component in self.components:\n",
    "            component.set_save_result_flag_test (save_result_flag_test)\n",
    "\n",
    "    def set_save_result_flag_training (self, save_result_flag_training):\n",
    "        super().set_save_result_flag_training (save_result_flag_training)\n",
    "        for component in self.components:\n",
    "            component.set_save_result_flag_training (save_result_flag_training)\n",
    "\n",
    "    def set_save_result_flag (self, save_result_flag):\n",
    "        super().set_save_result_flag (save_result_flag)\n",
    "        for component in self.components:\n",
    "            component.set_save_result_flag (save_result_flag)\n",
    "\n",
    "    def set_overwrite (self, overwrite):\n",
    "        super().set_overwrite (overwrite)\n",
    "        for component in self.components:\n",
    "            component.set_overwrite (overwrite)\n",
    "\n",
    "    def set_save_fitting (self, save_fitting):\n",
    "        super().set_save_fitting (save_fitting)\n",
    "        for component in self.components:\n",
    "            component.set_save_fitting (save_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (Pipeline, title_level=3)\n",
    "show_doc (Pipeline.__init__, name='__init__', title_level=4)\n",
    "show_doc (Pipeline.construct_diagram, name='construct_diagram', title_level=4)\n",
    "show_doc (Pipeline.show_summary, name='show_summary', title_level=4)\n",
    "show_doc (Pipeline.show_result_statistics, name='show_result_statistics', title_level=4)\n",
    "show_doc (Pipeline.assert_equal, name='assert_equal', title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pipeline_factory (pipeline_class, **kwargs):\n",
    "    \"\"\"Creates a pipeline object given its class `pipeline_class`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline_class : class or str\n",
    "        Name of the pipeline class used for creating the object. \n",
    "        This can be either of type string or class.\n",
    "    \"\"\"\n",
    "    if type(pipeline_class) is str:\n",
    "        Pipeline = eval(pipeline_class)\n",
    "    elif type(pipeline_class) is type:\n",
    "        Pipeline = pipeline_class\n",
    "    else:\n",
    "        raise ValueError (f'pipeline_class needs to be either string or class, we got {pipeline_class}')\n",
    "\n",
    "    return Pipeline (**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc (pipeline_factory, title_level=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (csp_new)",
   "language": "python",
   "name": "csp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
