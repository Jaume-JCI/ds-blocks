{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd85e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp core.compose\n",
    "import os\n",
    "from nbdev.showdoc import *\n",
    "if not os.path.exists('settings.ini'):\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bfb86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compose components\n",
    "\n",
    "> Classes and utilities for composed components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f392e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "from block_types.core.block_types import Component, PandasComponent, SamplingComponent\n",
    "from block_types.core.data_conversion import PandasConverter\n",
    "from block_types.core.utils import PandasIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bbae6-9f63-4b84-8813-a45a58c1215e",
   "metadata": {},
   "source": [
    "## MultiComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca1edc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiComponent (SamplingComponent):\n",
    "    \"\"\"\n",
    "    Component containing a list of components inside.\n",
    "    \n",
    "    The list must contain at least one component. \n",
    "    \n",
    "    See `Pipeline` class.\n",
    "    \"\"\"\n",
    "    def __init__ (self, separate_labels = False, **kwargs):\n",
    "        \"\"\"Assigns attributes and calls parent constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        separate_labels: bool, optional\n",
    "            whether or not the fit method receives the labels in a separate `y` vector \n",
    "            or in the same input `X`, as an additional variable. See description of \n",
    "            Pipeline class for more details.\n",
    "        \"\"\"\n",
    "        if not hasattr (self, 'components'):\n",
    "            self.components = []\n",
    "        if not hasattr (self, 'finalized_component_list'):\n",
    "            self.finalized_component_list = False\n",
    "        \n",
    "        # we need to call super().__init__() *after* having creating the `components` field,\n",
    "        # since the constructor of Component calls a method that is overriden in Pipeline, \n",
    "        # and this method makes use of the mentioned `components` field\n",
    "        super().__init__ (separate_labels = separate_labels, \n",
    "                          **kwargs)\n",
    "\n",
    "        self.set_split ('whole')\n",
    "        \n",
    "    \n",
    "    def register_components (self, *components):\n",
    "        \"\"\"\n",
    "        Registering component in `self.components` list.\n",
    "        \n",
    "        Every time that a new component is set as an attribute of the pipeline,\n",
    "        this component is added to the list `self.components`. Same \n",
    "        mechanism as the one used by pytorch's `nn.Module`\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'components'):\n",
    "            self.components = []\n",
    "            self.finalized_component_list = False\n",
    "        if not self.finalized_component_list:\n",
    "            self.components += components\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        \"\"\"\n",
    "        See register_components\n",
    "        \"\"\"\n",
    "        super().__setattr__(k, v)\n",
    "        \n",
    "        if isinstance(v, Component):\n",
    "            self.register_components(v)\n",
    "            if hasattr(v, 'nick_name'):\n",
    "                self.logger.warning (f'{v} already has a nick_name: {v.nick_name}')\n",
    "                warnings.warn (f'{v} already has a nick_name: {v.nick_name}')\n",
    "            v.nick_name = k\n",
    "            \n",
    "    def add_component (self, component):\n",
    "        if not hasattr(self, 'finalized_component_list'):\n",
    "            self.finalized_component_list = False\n",
    "        finalized_component_list = self.finalized_component_list\n",
    "        self.finalized_component_list = False\n",
    "        self.register_components(component)\n",
    "        self.finalized_component_list = finalized_component_list\n",
    "        \n",
    "        if hasattr(component, 'nick_name'):\n",
    "            self.logger.warning (f'{component} already has a nick_name: {component.nick_name}')\n",
    "            warnings.warn (f'{component} already has a nick_name: {component.nick_name}')\n",
    "        component.nick_name = component.name\n",
    "        if not hasattr(self, component.name):\n",
    "            self.__setattr__ (component.name, component)\n",
    "        \n",
    "    def set_components (self, *components):\n",
    "        self.components = components\n",
    "        self.finalized_component_list = True\n",
    "        for component in components:\n",
    "            if hasattr(component, 'nick_name'):\n",
    "                self.logger.warning (f'{component} already has a nick_name: {component.nick_name}')\n",
    "                warnings.warn (f'{component} already has a nick_name: {component.nick_name}')\n",
    "            component.nick_name = component.name\n",
    "            if not hasattr(self, component.name):\n",
    "                self.__setattr__ (component.name, component)\n",
    "        \n",
    "    def clear_descendants (self):\n",
    "        self.cls = Bunch ()\n",
    "        self.obj = Bunch ()\n",
    "        self.full_obj = Bunch ()\n",
    "        self.full_cls = Bunch ()\n",
    "        for component in self.components:\n",
    "            if isinstance(component, MultiComponent):\n",
    "                component.clear_descendants ()\n",
    "            \n",
    "    def gather_descendants (self, root='', nick_name=True):\n",
    "        if not hasattr (self, 'cls'):\n",
    "            self.cls = Bunch ()\n",
    "            self.obj = Bunch ()\n",
    "            self.full_obj = Bunch ()\n",
    "            self.full_cls = Bunch ()\n",
    "            \n",
    "        if hasattr(self, 'nick_name'):\n",
    "            name = self.nick_name if nick_name else self.name\n",
    "        else:\n",
    "            name = self.name\n",
    "        self.hierarchy_path = f'{root}{name}'\n",
    "        for component in self.components:\n",
    "            self._insert_descendant (self.cls, component, component.class_name)\n",
    "            self._insert_descendant (self.obj, component, component.name)\n",
    "            \n",
    "            name = component.nick_name if nick_name else component.name\n",
    "            component_hierarchy_path = f'{self.hierarchy_path}.{name}'\n",
    "            self._insert_descendant (self.full_cls, component_hierarchy_path, component.class_name)\n",
    "            self._insert_descendant (self.full_obj, component_hierarchy_path, component.name)\n",
    "            if isinstance(component, MultiComponent):\n",
    "                component.gather_descendants (root=f'{self.hierarchy_path}.',\n",
    "                                              nick_name=nick_name)\n",
    "                for name in component.cls:\n",
    "                    self._insert_descendant (self.cls, component.cls[name], name)\n",
    "                    self._insert_descendant (self.full_cls, component.full_cls[name], name)\n",
    "                for name in component.obj:\n",
    "                    self._insert_descendant (self.obj, component.obj[name], name)\n",
    "                    self._insert_descendant (self.full_obj, component.full_obj[name], name)\n",
    "                           \n",
    "    def _insert_descendant (self, cmp_dict, component, name):\n",
    "        if name in cmp_dict:\n",
    "            if not isinstance(cmp_dict[name], list):\n",
    "                cmp_dict[name] = [cmp_dict[name]]\n",
    "            if isinstance(component, list):\n",
    "                cmp_dict[name].extend(component)\n",
    "            else:\n",
    "                cmp_dict[name].append(component)\n",
    "        else:\n",
    "            if isinstance(component, list):\n",
    "                cmp_dict[name] = component.copy()\n",
    "            else:\n",
    "                cmp_dict[name] = component\n",
    "        \n",
    "    def construct_diagram (self, split=None, include_url=False, port=4000, project='block_types'):\n",
    "        \"\"\"\n",
    "        Construct diagram of the pipeline components, data flow and dimensionality.\n",
    "        \n",
    "        By default, we use test data to show the number of observations \n",
    "        in the output of each component. This can be changed passing \n",
    "        `split='train'`\n",
    "        \"\"\"\n",
    "        split = self.get_split (split)\n",
    "\n",
    "        if include_url:\n",
    "            base_url = f'http://localhost:{port}/{project}'\n",
    "        else:\n",
    "            URL = ''\n",
    "\n",
    "        node_name = 'data'\n",
    "        output = 'train / test'\n",
    "\n",
    "        f = Digraph('G', filename='fsm2.svg')\n",
    "        f.attr('node', shape='circle')\n",
    "\n",
    "        f.node(node_name)\n",
    "\n",
    "        f.attr('node', shape='box')\n",
    "        for component in self.components:\n",
    "            last_node_name = node_name\n",
    "            last_output = output\n",
    "            node_name = component.model_plotter.get_node_name()\n",
    "            if include_url:\n",
    "                URL = f'{base_url}/{component.model_plotter.get_module_path()}.html#{node_name}'\n",
    "            f.node(node_name, URL=URL)\n",
    "            f.edge(last_node_name, node_name, label=last_output)\n",
    "            output = component.model_plotter.get_edge_name(split=split)\n",
    "\n",
    "        last_node_name = node_name\n",
    "        node_name = 'output'\n",
    "        f.attr('node', shape='circle')\n",
    "        f.edge(last_node_name, node_name, label=output)\n",
    "\n",
    "        return f\n",
    "\n",
    "    def show_result_statistics (self, split=None):\n",
    "        \"\"\"\n",
    "        Show statistics about results obtained by each component. \n",
    "        \n",
    "        By default, this is shown on test data, although this can change setting \n",
    "        `split='train'`\n",
    "        \"\"\"\n",
    "        split = self.get_split (split)\n",
    "\n",
    "        for component in self.components:\n",
    "            component.show_result_statistics(split=split)\n",
    "\n",
    "    def show_summary (self, split=None):\n",
    "        \"\"\"\n",
    "        Show list of pipeline components, data flow and dimensionality.\n",
    "        \n",
    "        By default, we use test data to show the number of observations \n",
    "        in the output of each component. This can be changed passing \n",
    "        `split='train'`\n",
    "        \"\"\"\n",
    "        split = self.get_split (split)\n",
    "\n",
    "        node_name = 'data'\n",
    "        output = 'train / test'\n",
    "\n",
    "        for i, component in enumerate(self.components):\n",
    "            node_name = component.model_plotter.get_node_name()\n",
    "            output = component.model_plotter.get_edge_name(split=split)\n",
    "            print (f'{\"-\"*100}')\n",
    "            print (f'{i}: {node_name} => {output}')\n",
    "\n",
    "\n",
    "    def get_split (self, split=None):\n",
    "        if split is None:\n",
    "            if self.data_io.split is not None:\n",
    "                split = self.data_io.split\n",
    "            else:\n",
    "                split = 'whole'\n",
    "\n",
    "        return split\n",
    "\n",
    "    def assert_equal (self, path_reference_results, assert_equal_func=pd.testing.assert_frame_equal, **kwargs):\n",
    "        \"\"\"Compare results stored in current run against reference results stored in given path.\"\"\"\n",
    "\n",
    "        for component in self.components:\n",
    "            component.assert_equal (path_reference_results, assert_equal_func=assert_equal_func, **kwargs)\n",
    "        self.logger.info ('both pipelines give the same results')\n",
    "        print ('both pipelines give the same results')\n",
    "        \n",
    "    def load_estimator (self):\n",
    "        for component in self.components:\n",
    "            component.load_estimator ()\n",
    "\n",
    "    # *************************\n",
    "    # setters\n",
    "    # *************************\n",
    "    def set_split (self, split):\n",
    "        super().set_split (split)\n",
    "        for component in self.components:\n",
    "            component.set_split (split)\n",
    "\n",
    "    def set_save_splits (self, save_splits):\n",
    "        super().set_save_splits (save_splits)\n",
    "        for component in self.components:\n",
    "            component.set_save_splits (save_splits)\n",
    "            \n",
    "    def set_load_model (self, load_model):\n",
    "        super().set_load_model (load_model)\n",
    "        for component in self.components:\n",
    "            component.set_load_model (load_model)\n",
    "\n",
    "    def set_save_model (self, save_model):\n",
    "        super().set_save_model (save_model)\n",
    "        for component in self.components:\n",
    "            component.set_save_model (save_model)\n",
    "            \n",
    "    def set_save_result (self, save_result):\n",
    "        super().set_save_result (save_result)\n",
    "        for component in self.components:\n",
    "            component.set_save_result (save_result)\n",
    "            \n",
    "    def set_load_result (self, load_result):\n",
    "        super().set_load_result (load_result)\n",
    "        for component in self.components:\n",
    "            component.set_load_result (load_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedce8a4-be52-4e77-8069-f67c4f215ca6",
   "metadata": {},
   "source": [
    "### Usage examples / tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771790d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loading / saving all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295af9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from block_types.core.utils import PickleIO\n",
    "from block_types.utils.utils import remove_previous_results\n",
    "\n",
    "path_results = 'multi_component_loading_saving'\n",
    "remove_previous_results (path_results=path_results)\n",
    "\n",
    "# 1. by setting components as attributes:\n",
    "class NewComposition(MultiComponent):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        data_io = PickleIO (**kwargs)\n",
    "        super().__init__(data_io=data_io,\n",
    "                         **kwargs)\n",
    "        \n",
    "        self.tr1 = Component(FunctionTransformer (lambda x: x*3),\n",
    "                             data_io=data_io,\n",
    "                             name='tr1')\n",
    "        self.tr2 = Component(FunctionTransformer (lambda x: x*2),\n",
    "                             data_io=data_io,\n",
    "                             name='tr2')\n",
    "        \n",
    "    def _apply (self, X):\n",
    "        return self.tr1 (X) + self.tr2(X)\n",
    "\n",
    "X = np.array([1,2,3])\n",
    "    \n",
    "composition1 = NewComposition (path_results=path_results)\n",
    "result1 = composition1 (X)\n",
    "\n",
    "composition2 = NewComposition (path_results=path_results)\n",
    "result2 = composition2.data_io.load_result()\n",
    "assert (result1==result2).all()\n",
    "\n",
    "resut_tr1_2 = composition2.tr1.data_io.load_result()\n",
    "resut_tr2_2 = composition2.tr2.data_io.load_result()\n",
    "assert (resut_tr1_2==composition1.tr1(X)).all()\n",
    "assert (resut_tr2_2==composition1.tr2(X)).all()\n",
    "\n",
    "assert sorted(os.listdir (f'{path_results}/whole'))==['new_composition_result.pk', 'tr1_result.pk', 'tr2_result.pk']\n",
    "\n",
    "composition1.set_split ('validation')\n",
    "result1b = composition1 (X)\n",
    "assert sorted(os.listdir (f'{path_results}/validation'))==['new_composition_result.pk', 'tr1_result.pk', 'tr2_result.pk']\n",
    "\n",
    "remove_previous_results (path_results=f'{path_results}/whole')\n",
    "\n",
    "resut_tr1_2 = composition2.tr1.data_io.load_result(split='validation')\n",
    "resut_tr2_2 = composition2.tr2.data_io.load_result()\n",
    "\n",
    "assert (resut_tr1_2==composition1.tr1(X)).all()\n",
    "assert resut_tr2_2 is None\n",
    "\n",
    "composition2.set_split('validation')\n",
    "resut_tr1_2 = composition2.tr1.data_io.load_result()\n",
    "resut_tr2_2 = composition2.tr2.data_io.load_result()\n",
    "\n",
    "assert (resut_tr1_2==composition1.tr1(X)).all()\n",
    "assert (resut_tr2_2==composition1.tr2(X)).all()\n",
    "\n",
    "remove_previous_results (path_results=path_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9752e6-d651-448c-9a23-be5b72b49542",
   "metadata": {},
   "source": [
    "#### Getting descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b4a043-0929-42f9-9d49-ca475b22bf28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Intermediate (MultiComponent):\n",
    "    def __init__ (self, name=None, **kwargs):\n",
    "        super().__init__ (name=name, **kwargs)\n",
    "        self.first = Component (name='first_component', **kwargs)\n",
    "        self.second = Component (name='second_component', **kwargs)\n",
    "\n",
    "class Higher (MultiComponent):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.first = Intermediate (name='first_intermediate', **kwargs)\n",
    "        self.second = Intermediate (name='second_intermediate', **kwargs)\n",
    "        self.gather_descendants(nick_name=False)\n",
    "\n",
    "higher = Higher()\n",
    "\n",
    "assert sorted(higher.obj.keys())==['first_component', 'first_intermediate', 'second_component', 'second_intermediate']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(higher.obj.items()))\n",
    "assert list(types)==[list, Intermediate, list, Intermediate]\n",
    "\n",
    "sorted_list = list(sorted(higher.obj.items()))\n",
    "types = map(type, sorted_list[0][1])\n",
    "assert list(types)==[Component,Component]\n",
    "\n",
    "sorted_list = list(sorted(higher.obj.items()))\n",
    "types = map(type, sorted_list[2][1])\n",
    "assert list(types)==[Component,Component]\n",
    "\n",
    "sorted_keys=list(sorted(higher.cls.keys()))\n",
    "assert sorted_keys == ['Component', 'Intermediate']\n",
    "\n",
    "assert list(map(type,higher.cls[sorted_keys[0]]))==[Component, Component, Component, Component]\n",
    "\n",
    "assert list(map(type,higher.cls[sorted_keys[1]]))==[Intermediate, Intermediate]\n",
    "\n",
    "\n",
    "# ***********************\n",
    "# recursive behaviour: higher.first\n",
    "intermediate = higher.first\n",
    "assert sorted(intermediate.obj.keys())==['first_component', 'second_component']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))\n",
    "assert list(types)==[Component, Component]\n",
    "\n",
    "sorted_keys=list(sorted(intermediate.cls.keys()))\n",
    "assert sorted_keys==['Component']\n",
    "\n",
    "assert list(map(type,intermediate.cls[sorted_keys[0]]))==[Component,Component]\n",
    "\n",
    "# **********************************************\n",
    "# recursive behaviour: higher.second\n",
    "# **********************************************\n",
    "intermediate = higher.second\n",
    "assert sorted(intermediate.obj.keys())==['first_component', 'second_component']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))\n",
    "assert list(types)==[Component, Component]\n",
    "\n",
    "sorted_keys=list(sorted(intermediate.cls.keys()))\n",
    "assert sorted_keys==['Component']\n",
    "\n",
    "assert list(map(type,intermediate.cls[sorted_keys[0]]))==[Component,Component]\n",
    "\n",
    "# **********************************************\n",
    "# full hierarchical paths\n",
    "# **********************************************\n",
    "assert list(sorted(higher.full_cls.keys()))==['Component', 'Intermediate']\n",
    "assert higher.full_cls['Intermediate']==['higher.first_intermediate', 'higher.second_intermediate']\n",
    "assert higher.full_cls['Component']==['higher.first_intermediate.first_component',\n",
    "  'higher.first_intermediate.second_component',\n",
    "  'higher.second_intermediate.first_component',\n",
    "  'higher.second_intermediate.second_component']\n",
    "\n",
    "assert higher.first.full_cls['Component']==['higher.first_intermediate.first_component',\n",
    "  'higher.first_intermediate.second_component']\n",
    "\n",
    "assert list(sorted(higher.full_obj))==['first_component', 'first_intermediate', 'second_component', 'second_intermediate']\n",
    "\n",
    "assert higher.full_obj['first_intermediate']=='higher.first_intermediate'\n",
    "\n",
    "assert higher.full_obj['first_component']==['higher.first_intermediate.first_component',\n",
    "  'higher.second_intermediate.first_component']\n",
    "\n",
    "assert higher.full_obj['second_component']==['higher.first_intermediate.second_component',\n",
    "  'higher.second_intermediate.second_component']\n",
    "\n",
    "assert higher.full_obj['second_intermediate']=='higher.second_intermediate'\n",
    "\n",
    "assert list(sorted(higher.second.full_obj))==['first_component', 'second_component']\n",
    "\n",
    "assert higher.second.full_obj['first_component']=='higher.second_intermediate.first_component'\n",
    "\n",
    "assert higher.second.full_obj['second_component']=='higher.second_intermediate.second_component'\n",
    "\n",
    "assert higher.hierarchy_path=='higher'\n",
    "\n",
    "assert higher.first.hierarchy_path=='higher.first_intermediate'\n",
    "\n",
    "# with nick_names\n",
    "higher.clear_descendants()\n",
    "higher.gather_descendants(nick_name=True)\n",
    "\n",
    "assert higher.full_cls['Component']==['higher.first.first',\n",
    " 'higher.first.second',\n",
    " 'higher.second.first',\n",
    " 'higher.second.second']\n",
    "\n",
    "assert higher.full_obj['first_intermediate']=='higher.first'\n",
    "assert higher.full_obj['first_component']==['higher.first.first', 'higher.second.first']\n",
    "assert higher.full_obj['second_component']==['higher.first.second', 'higher.second.second']\n",
    "assert higher.full_obj['second_intermediate']=='higher.second'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff6fef9-0eb2-411e-8af7-a9414dd522bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# **********************************************\n",
    "# second example\n",
    "# **********************************************\n",
    "class Intermediate (MultiComponent):\n",
    "    def __init__ (self, name=None, **kwargs):\n",
    "        super().__init__ (name=name, **kwargs)\n",
    "        self.first = Component (name=f'{name}_first_component', **kwargs)\n",
    "        self.second = Component (name=f'{name}_second_component', **kwargs)\n",
    "\n",
    "class Higher (MultiComponent):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.first = Intermediate (name='first_intermediate', **kwargs)\n",
    "        self.second = Intermediate (name='second_intermediate', **kwargs)\n",
    "        self.gather_descendants(nick_name=False)\n",
    "\n",
    "higher = Higher()\n",
    "\n",
    "assert sorted(higher.obj.keys())==['first_intermediate', 'first_intermediate_first_component', 'first_intermediate_second_component', 'second_intermediate', 'second_intermediate_first_component', 'second_intermediate_second_component']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(higher.obj.items()))\n",
    "\n",
    "assert list(types)==[Intermediate, Component, Component, Intermediate, Component, Component]\n",
    "\n",
    "sorted_keys=list(sorted(higher.cls.keys()))\n",
    "assert sorted_keys == ['Component', 'Intermediate']\n",
    "\n",
    "assert list(map(type,higher.cls[sorted_keys[0]]))==[Component, Component, Component, Component]\n",
    "assert list(map(type,higher.cls[sorted_keys[1]]))==[Intermediate, Intermediate]\n",
    "\n",
    "# ***********************\n",
    "# recursive behaviour: higher.first\n",
    "intermediate = higher.first\n",
    "assert sorted(intermediate.obj.keys())==['first_intermediate_first_component', 'first_intermediate_second_component']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))\n",
    "assert list(types)==[Component, Component]\n",
    "\n",
    "sorted_keys=list(sorted(intermediate.cls.keys()))\n",
    "assert sorted_keys==['Component']\n",
    "\n",
    "assert list(map(type,intermediate.cls[sorted_keys[0]]))==[Component,Component]\n",
    "\n",
    "# ***********************\n",
    "# recursive behaviour: higher.second\n",
    "intermediate = higher.second\n",
    "assert sorted(intermediate.obj.keys())==['second_intermediate_first_component', 'second_intermediate_second_component']\n",
    "\n",
    "# check types\n",
    "types = map(lambda x: type(x[1]), sorted(intermediate.obj.items()))\n",
    "assert list(types)==[Component, Component]\n",
    "\n",
    "sorted_keys=list(sorted(intermediate.cls.keys()))\n",
    "assert sorted_keys==['Component']\n",
    "\n",
    "assert list(map(type,intermediate.cls[sorted_keys[0]]))==[Component,Component]\n",
    "\n",
    "\n",
    "# **********************************************\n",
    "# full hierarchical paths\n",
    "# **********************************************\n",
    "assert list(sorted(higher.full_cls))==['Component', 'Intermediate']\n",
    "\n",
    "assert higher.full_cls['Component']==['higher.first_intermediate.first_intermediate_first_component',\n",
    "  'higher.first_intermediate.first_intermediate_second_component',\n",
    "  'higher.second_intermediate.second_intermediate_first_component',\n",
    "  'higher.second_intermediate.second_intermediate_second_component']\n",
    "\n",
    "assert higher.full_cls['Intermediate']==['higher.first_intermediate', 'higher.second_intermediate']\n",
    "\n",
    "assert list(higher.first.full_cls)==['Component']\n",
    "\n",
    "assert higher.first.full_cls['Component']==['higher.first_intermediate.first_intermediate_first_component',\n",
    "  'higher.first_intermediate.first_intermediate_second_component']\n",
    "\n",
    "assert sorted(list(higher.full_obj))==['first_intermediate',\n",
    " 'first_intermediate_first_component',\n",
    " 'first_intermediate_second_component',\n",
    " 'second_intermediate',\n",
    " 'second_intermediate_first_component',\n",
    " 'second_intermediate_second_component']\n",
    "\n",
    "assert higher.full_obj['first_intermediate']=='higher.first_intermediate'\n",
    "\n",
    "assert higher.full_obj['first_intermediate_first_component']=='higher.first_intermediate.first_intermediate_first_component'\n",
    "\n",
    "assert higher.full_obj['first_intermediate_second_component']=='higher.first_intermediate.first_intermediate_second_component'\n",
    "\n",
    "assert higher.full_obj['second_intermediate']=='higher.second_intermediate'\n",
    "\n",
    "assert higher.full_obj['second_intermediate_first_component']=='higher.second_intermediate.second_intermediate_first_component'\n",
    "\n",
    "assert higher.full_obj['second_intermediate_second_component']=='higher.second_intermediate.second_intermediate_second_component'\n",
    "\n",
    "assert list(sorted(higher.second.full_obj))==['second_intermediate_first_component', 'second_intermediate_second_component']\n",
    "\n",
    "assert higher.second.full_obj['second_intermediate_first_component']=='higher.second_intermediate.second_intermediate_first_component'\n",
    "\n",
    "assert higher.second.full_obj['second_intermediate_second_component']=='higher.second_intermediate.second_intermediate_second_component'\n",
    "\n",
    "assert higher.hierarchy_path=='higher'\n",
    "\n",
    "assert higher.first.hierarchy_path=='higher.first_intermediate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac147d-d580-4adf-91aa-6922ee440dcf",
   "metadata": {},
   "source": [
    "#### Passing parameters specific of hierarchy level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a8e181-aea2-4c6c-90c6-0e1f0b77868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "class Intermediate (MultiComponent):\n",
    "    def __init__ (self, name=None, z=6, h=10, x=3, **kwargs):\n",
    "        super().__init__ (name=name, **kwargs)\n",
    "        self.first = Component (name='first_component', **kwargs)\n",
    "        self.second = Component (name='second_component', **kwargs)\n",
    "\n",
    "class Higher (MultiComponent):\n",
    "    def __init__ (self, x=2, y=3, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.first = Intermediate (name='first_intermediate', **kwargs)\n",
    "        self.second = Intermediate (name='second_intermediate', **kwargs)\n",
    "        self.gather_descendants()\n",
    "\n",
    "higher = Higher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c968bae8-3a94-4cea-8947-d357e4dfbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert higher.x==2 and higher.y==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ee4f94f-e57d-407c-bf8e-edf81295c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(AttributeError):\n",
    "    print (higher.z)\n",
    "with pytest.raises(AttributeError):\n",
    "    print (higher.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4649a711-29d6-4c4a-89a3-839c77ade6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert higher.first.x==3 and higher.first.z==6 and higher.first.h==10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27500e82-65a4-4744-9e29-065115d1f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "level=dict(\n",
    "    x=10\n",
    ")\n",
    "higher = Higher (y=5, z=10, level_0=level)\n",
    "assert higher.x==10 and higher.y==5 and higher.first.x==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c20df6bb-e536-4665-ac4c-dec66338c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "level=dict(\n",
    "    x=10\n",
    ")\n",
    "higher = Higher (y=5, z=10, level_1=level)\n",
    "assert higher.x==2 and higher.y==5 and higher.first.x==10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cfe10-762e-4a67-a4b7-956256301858",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa0a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Pipeline (MultiComponent):\n",
    "    \"\"\"\n",
    "    Pipeline composed of a list of components that run sequentially.\n",
    "    \n",
    "    During training, the components of the list are trained one after the other, \n",
    "    where one component is fed the result of transforming the data with the list \n",
    "    of components located before in the pipeline.\n",
    "    \n",
    "    The `Pipeline` class is a subclass of `SamplingComponent`, which itself is a \n",
    "    subclass of `Component`. This provides the functionality of `Component` \n",
    "    to any implemented pipeline, such as logging the messages, loading / saving the \n",
    "    results, and convert the data format so that it can work as part of other \n",
    "    pipelines with potentially other data formats.\n",
    "    \n",
    "    Being a subclass of `SamplingComponent`, the `transform` method \n",
    "    receives an input data  `X` that contains both data and labels. \n",
    "    \n",
    "    Furthermore, the Pipeline constructor sets `separate_labels=False` by default,\n",
    "    which means that the `fit` method also receives an input data `X` that contains \n",
    "    not only data but also labels. This is necessary because some of the components in \n",
    "    the pipeline might be of class `SamplingComponent`, and such components \n",
    "    need the input data `X` to contain labels when calling `transform` (and note that \n",
    "    this method is called when calling `fit` on a pipeline, since we do `fit_transform`\n",
    "    on all the components except for the last one)\n",
    "    \"\"\"\n",
    "    def __init__ (self, **kwargs):\n",
    "        \"\"\"Assigns attributes and calls parent constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        separate_labels: bool, optional\n",
    "            whether or not the fit method receives the labels in a separate `y` vector \n",
    "            or in the same input `X`, as an additional variable. See description of \n",
    "            Pipeline class for more details.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__ (**kwargs)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit components of the pipeline, given data X and labels y.\n",
    "        \n",
    "        By default, y will be None, and the labels are part of `X`, as a variable.\n",
    "        \"\"\"\n",
    "        X = self._fit_apply_except_last (X, y)\n",
    "        self.components[-1].fit (X, y)\n",
    "    \n",
    "    def _fit_apply (self, X, y=None, **kwargs):\n",
    "        X = self._fit_apply_except_last (X, y, **kwargs)\n",
    "        return self.components[-1].fit_apply (X, y, **kwargs)\n",
    "\n",
    "    def _fit_apply_except_last (self, X, y, **kwargs):\n",
    "        for component in self.components[:-1]:\n",
    "            X = component.fit_apply (X, y, **kwargs)\n",
    "        return X\n",
    "    \n",
    "    def _apply (self, X):\n",
    "        \"\"\"Transform data with components of pipeline, and predict labels with last component. \n",
    "        \n",
    "        In the current implementation, we consider prediction a form of mapping, \n",
    "        and therefore a special type of transformation.\"\"\"\n",
    "        for component in self.components:\n",
    "            X = component.transform (X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02171cf-cadd-4315-8efa-292648ee3a31",
   "metadata": {},
   "source": [
    "### Usage examples / tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261ce6c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Tests for `fit_apply` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d17546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test `fit_apply` method\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n",
    "from block_types.core.block_types import PickleSaverComponent\n",
    "from block_types.utils.utils import remove_previous_results\n",
    "\n",
    "# Transform1: custom Transform\n",
    "class Transform1 (Component):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.estimator= Bunch(sum = 1)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.estimator.sum = X.sum(axis=0)\n",
    "    \n",
    "    def _apply (self, x):\n",
    "        return x*1000 + self.estimator.sum\n",
    "    \n",
    "class Transform2 (Component):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.estimator= Bunch(maxim = 1)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.estimator.maxim = X.max(axis=0)\n",
    "    \n",
    "    def _apply (self, x):\n",
    "        return x*100 + self.estimator.maxim\n",
    "\n",
    "class NewPipeline (Pipeline):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        \n",
    "        # custom transform\n",
    "        self.tr1 = Transform1(**kwargs) \n",
    "        \n",
    "        # slklearn transform\n",
    "        self.tr2 = Transform2(**kwargs) \n",
    "\n",
    "pipeline = NewPipeline()\n",
    "x = np.array([3,4,5])\n",
    "r1 = pipeline.fit_apply (x.reshape(-1,1))\n",
    "print (r1)\n",
    "\n",
    "x1 = x * 1000 + sum(x)\n",
    "x2 = x1 * 100 + max(x1)\n",
    "assert (r1.ravel()==x2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bac1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NewMulti (MultiComponent):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        \n",
    "        # custom transform\n",
    "        self.tr1 = Transform1(**kwargs) \n",
    "        \n",
    "        # slklearn transform\n",
    "        self.tr2 = Transform2(**kwargs) \n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.tr1.fit (X)\n",
    "        self.tr2.fit (X)\n",
    "        \n",
    "    def _apply (self, X, y=None):\n",
    "        X1=self.tr1.apply (X)\n",
    "        X2=self.tr2.apply (X)\n",
    "        return X1+X2\n",
    "\n",
    "new_multi = NewMulti()\n",
    "r2 = new_multi.fit_apply (x)\n",
    "print (r2)\n",
    "x2b = 100 * x + max(x)\n",
    "assert (r2.ravel()==(x1 + x2b)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aacd2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Adding new components to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test automatic creation of pipeline components\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# 1. by setting components as attributes:\n",
    "class NewPipeline(Pipeline):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tr1 = Component(FunctionTransformer (lambda x: x+1))\n",
    "        self.tr2 = Component(FunctionTransformer (lambda x: x*2))\n",
    "pipeline = NewPipeline()\n",
    "result = pipeline.transform (3)\n",
    "print (result)\n",
    "assert result == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae2602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. by using `set_components`\n",
    "class NewPipeline(Pipeline):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        tr1 = Component(FunctionTransformer (lambda x: x+1))\n",
    "        tr2 = Component(FunctionTransformer (lambda x: x*2))\n",
    "        self.set_components (tr1, tr2)\n",
    "        \n",
    "        # the following transform is not added to the pipeline component list:\n",
    "        self.tr3 = Component(FunctionTransformer (lambda x: x+1))\n",
    "        \n",
    "        # The reason is that once set_components is called, the component list \n",
    "        # is frozen and inmutable setting new components by attribute doesn't \n",
    "        # result in adding them to the component list\n",
    "        \n",
    "pipeline = NewPipeline()\n",
    "result = pipeline.transform (3)\n",
    "\n",
    "assert result == 8\n",
    "assert len(pipeline.components) == 2\n",
    "print (result, len(pipeline.components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11918856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. after calling `set_components()`, we can add new components with `add_component()`\n",
    "class NewPipeline(Pipeline):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        tr1 = Component(FunctionTransformer (lambda x: x+1))\n",
    "        tr2 = Component(FunctionTransformer (lambda x: x*2))\n",
    "        self.set_components (tr1, tr2)\n",
    "        \n",
    "        tr3 = Component(FunctionTransformer (lambda x: x+2))\n",
    "        self.add_component(tr3)\n",
    "        \n",
    "pipeline = NewPipeline()\n",
    "result = pipeline.transform (3)\n",
    "\n",
    "assert result == 10\n",
    "assert len(pipeline.components) == 3\n",
    "print (result, len(pipeline.components))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6221cf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Tests for `load_estimator` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b77022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test `load_estimator` method\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import Bunch\n",
    "from block_types.core.block_types import PickleSaverComponent\n",
    "from block_types.utils.utils import remove_previous_results\n",
    "\n",
    "# Transform1: custom Transform\n",
    "class Transform1 (PickleSaverComponent):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.estimator= Bunch(inv_c = 1)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.estimator.inv_c = X.ravel()[0]\n",
    "    \n",
    "    def _apply (self, x):\n",
    "        return x / self.estimator.inv_c\n",
    "\n",
    "class NewPipeline (Pipeline):\n",
    "    \n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        \n",
    "        # custom transform\n",
    "        self.tr1 = Transform1(**kwargs) \n",
    "        \n",
    "        # slklearn transform\n",
    "        self.tr2 = PickleSaverComponent(StandardScaler(), **kwargs)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.tr1.fit (X)\n",
    "        self.tr2.fit (X)\n",
    "\n",
    "# remove any previously stored \n",
    "path_results = 'pipeline_loading_saving'\n",
    "remove_previous_results (path_results=path_results)\n",
    "\n",
    "pipeline = NewPipeline(path_results=path_results, save_test_result=False)\n",
    "pipeline.fit (np.array([3,4,5]).reshape(-1,1))\n",
    "result1 = pipeline.transform (np.array([300,400,500]).reshape(-1,1))\n",
    "print (pipeline.tr2.estimator.mean_)\n",
    "\n",
    "del pipeline \n",
    "pipeline = NewPipeline(path_results=path_results, save_test_result=False)\n",
    "pipeline.load_estimator ()\n",
    "print (pipeline.tr2.estimator.mean_)\n",
    "result2 = pipeline.transform (np.array([300,400,500]).reshape(-1,1))\n",
    "\n",
    "np.testing.assert_array_equal (result1, result2)\n",
    "\n",
    "# remove stored files resulting from running the current test\n",
    "remove_previous_results (path_results=path_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eaf554",
   "metadata": {
    "tags": []
   },
   "source": [
    "### make_pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_pipeline(*components, cls=Pipeline, **kwargs):\n",
    "    \"\"\"Create `Pipeline` object of class `cls`, given `components` list.\"\"\"\n",
    "    pipeline = cls (**kwargs)\n",
    "    pipeline.components = list(components)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1 = Component(FunctionTransformer (lambda x: x+1))\n",
    "tr2 = Component(FunctionTransformer (lambda x: x*2))\n",
    "pipeline = make_pipeline (tr1, tr2)\n",
    "result = pipeline.transform (3)\n",
    "\n",
    "print (result)\n",
    "assert result == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6dcb6-b5ff-455d-b74e-00cac7761206",
   "metadata": {},
   "source": [
    "### pipeline_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pipeline_factory (pipeline_class, **kwargs):\n",
    "    \"\"\"Creates a pipeline object given its class `pipeline_class`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline_class : class or str\n",
    "        Name of the pipeline class used for creating the object. \n",
    "        This can be either of type string or class.\n",
    "    \"\"\"\n",
    "    if type(pipeline_class) is str:\n",
    "        Pipeline = eval(pipeline_class)\n",
    "    elif type(pipeline_class) is type:\n",
    "        Pipeline = pipeline_class\n",
    "    else:\n",
    "        raise ValueError (f'pipeline_class needs to be either string or class, we got {pipeline_class}')\n",
    "\n",
    "    return Pipeline (**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b6623-604d-4790-bafc-9c1404b202f4",
   "metadata": {},
   "source": [
    "### PandasPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ab14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PandasPipeline (Pipeline):\n",
    "    \"\"\"\n",
    "    Pipeline that saves results in parquet format, and preserves DataFrame format.\n",
    "    \n",
    "    See `Pipeline` class for an explanation of using `separate_labels=False`\n",
    "    \"\"\"\n",
    "    def __init__ (self, \n",
    "                  data_converter=None,\n",
    "                  data_io=None,\n",
    "                  separate_labels=False,\n",
    "                  **kwargs):\n",
    "        if data_converter is None:\n",
    "            data_converter = PandasConverter (separate_labels=separate_labels,\n",
    "                                              **kwargs)\n",
    "        if data_io is None:\n",
    "            data_io = PandasIO (**kwargs)\n",
    "        super().__init__ (self, \n",
    "                          data_converter=data_converter,\n",
    "                          data_io=data_io,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a93851-c398-4a0c-8139-7c0d285ee4c1",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fc7eb-5cc4-4721-ba16-368d0c46c7a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ColumnSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ColumnSelector (Component):\n",
    "    def __init__ (self, \n",
    "                  columns=[],\n",
    "                  **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.columns = columns\n",
    "    \n",
    "    def _apply (self, df):\n",
    "        return df[self.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85176fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame ({'x1': list(range(5)),\n",
    "                    'x2': list(range(5,10)),\n",
    "                    'x3': list(range(15,20)),\n",
    "                    'x4': list(range(25,30))\n",
    "                   })\n",
    "dfr = ColumnSelector(columns=['x2','x4']).transform(df)\n",
    "assert (dfr==df[['x2','x4']]).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a1be0-0087-41c1-aa13-31f96c5c172d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Concat (Component):\n",
    "    def __init__ (self, \n",
    "                  **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        \n",
    "    def _apply (self, *dfs):\n",
    "        return pd.concat(list(dfs), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a7ba3-954b-4367-9e57-b4d7ec9000a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _BaseColumnTransformer (MultiComponent):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.concat = Concat (**kwargs)\n",
    "    \n",
    "    def _fit (self, df, y=None):\n",
    "        for component in self.components:\n",
    "            component.fit (df)\n",
    "        return self\n",
    "    \n",
    "    def _apply (self, df):\n",
    "        dfs = []\n",
    "        for component in self.components:\n",
    "            dfs.append (component.transform (df))\n",
    "        df_result = self.concat.transform (*dfs)\n",
    "        return df_result\n",
    "    \n",
    "class ColumnTransformer (_BaseColumnTransformer):\n",
    "    def __init__ (self, *transformers, **kwargs):\n",
    "        self.components = make_column_transformer_pipelines (*transformers, **kwargs)\n",
    "        super().__init__ (**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96deb0d4-6ea7-4ca3-93e9-e4116012b06e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0de61d-786a-438c-ab75-f62629e8d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Identity (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super ().__init__ (**kwargs)\n",
    "        \n",
    "    def _apply (self, X):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b9e71-2fd6-4520-bf38-e289df9fe291",
   "metadata": {
    "tags": []
   },
   "source": [
    "### make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84612d42-4080-4bdc-bffb-d288e246ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def make_column_transformer_pipelines (*transformers, **kwargs):\n",
    "    pipelines = []\n",
    "    for name, transformer, columns in transformers:\n",
    "        if (type(transformer) is str) and transformer == 'passthrough':\n",
    "            transformer = Identity (**kwargs)\n",
    "        pipeline = make_pipeline(ColumnSelector(columns, **kwargs), \n",
    "                                 transformer, \n",
    "                                 name = name,\n",
    "                                 **kwargs)\n",
    "        pipelines.append (pipeline)\n",
    "    \n",
    "    return pipelines\n",
    "\n",
    "\n",
    "def make_column_transformer (*transformers, **kwargs):\n",
    "    transformers_with_name = []\n",
    "    for transformer, columns in transformers:\n",
    "        columns_name = ''.join([x[0] for x in columns])\n",
    "        if len(columns_name) > 5:\n",
    "            columns_name = columns_name[:5]\n",
    "        if (type(transformer) is str) and transformer == 'passthrough':\n",
    "            transformer_name = 'pass'\n",
    "        elif hasattr(transformer, 'name'):\n",
    "            transformer_name = transformer.name\n",
    "        else:\n",
    "            transformer_name = transformer.__class__.__name__\n",
    "        name = f'{transformer_name}_{columns_name}'\n",
    "        transformers_with_name.append ((name, transformer, columns))\n",
    "    \n",
    "    pipelines = make_column_transformer_pipelines (*transformers_with_name, **kwargs)\n",
    "    column_transformer = _BaseColumnTransformer ()\n",
    "    column_transformer.components = pipelines\n",
    "    return column_transformer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797f513-28eb-4561-8fc7-f0aa382522fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Tests / usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "df = pd.DataFrame ({'cont1': list(range(5)),\n",
    "                    'cont2': list(range(5,10)),\n",
    "                    'cont3': list(range(15,20)),\n",
    "                    'cont4': list(range(25,30)),\n",
    "                    'cat_1': list([1,2,3,2,1]),\n",
    "                    'cat_2': list([0,1,1,0,0])\n",
    "                    })\n",
    "\n",
    "tr1 = Component(FunctionTransformer (lambda x: x+1), name='tr1')\n",
    "tr2 = PandasComponent(FunctionTransformer (lambda x: x*2), transformed_columns=['cont2_bis','cat_1'], name='tr2')\n",
    "\n",
    "column_transformer = make_column_transformer (\n",
    "    (tr1, ['cont2', 'cont4']),\n",
    "    (tr2, ['cont2', 'cat_1'])\n",
    ")\n",
    "dfr = column_transformer.transform(df)\n",
    "\n",
    "# display and test\n",
    "display(dfr)\n",
    "assert (dfr[['cont2','cont4']] == tr1(df[['cont2','cont4']])).all().all()\n",
    "assert (dfr[['cont2_bis','cat_1']] == tr2(df[['cont2','cat_1']])).all().all()\n",
    "assert (dfr.columns == ['cont2','cont4', 'cont2_bis','cat_1']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d988280",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = make_column_transformer (\n",
    "    (tr1, ['cont1', 'cont4']),\n",
    "    ('passthrough', ['cont2', 'cat_1'])\n",
    ")\n",
    "dfr = column_transformer.transform(df)\n",
    "\n",
    "# display and test\n",
    "display(dfr)\n",
    "assert (dfr[['cont1','cont4']] == tr1(df[['cont1','cont4']])).all().all()\n",
    "assert (dfr[['cont2','cat_1']] == df[['cont2','cat_1']]).all().all()\n",
    "assert (dfr.columns == ['cont1','cont4', 'cont2','cat_1']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b6b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTimes100 (Component):\n",
    "    def _fit (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        \n",
    "        dfr = pd.DataFrame ({'c1_times100': self.sum.values[0]*100 + X.iloc[:,0].values,\n",
    "                             'c2_times100': self.sum.values[1]*100 + X.iloc[:,1].values,\n",
    "                             'c2_times1000': self.sum.values[1]*1000 + X.iloc[:,1].values})\n",
    "        return dfr\n",
    "        \n",
    "tr1 = SumTimes100 ()\n",
    "tr2 = PandasComponent(FunctionTransformer (lambda x: x*2), name='tr2')\n",
    "\n",
    "column_transformer = make_column_transformer (\n",
    "    (tr1, ['cont2', 'cont4']),\n",
    "    (tr2, ['cont2', 'cat_1'])\n",
    ")\n",
    "dfr = column_transformer.fit_transform(df)\n",
    "\n",
    "# display & test\n",
    "display(dfr)\n",
    "assert (dfr.columns == ['c1_times100','c2_times100', 'c2_times1000','cont2', 'cat_1']).all()\n",
    "assert (dfr['c1_times100'] == sum(df.cont2)*100+df.cont2).all()\n",
    "assert (dfr['c2_times100'] == sum(df.cont4)*100+df.cont4).all()\n",
    "assert (dfr['c2_times1000'] == sum(df.cont4)*1000+df.cont4).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebb69a",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## MultiSplitComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450548a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiSplitComponent (MultiComponent):\n",
    "    def __init__ (self, \n",
    "                  component=None, \n",
    "                  fit_to = 'training',\n",
    "                  fit_additional = [],\n",
    "                  apply_to = ['training', 'validation', 'test'],\n",
    "                  raise_error_if_split_doesnot_exist=False,\n",
    "                  raise_warning_if_split_doesnot_exist=True,\n",
    "                  **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        if component is not None:\n",
    "            self.set_components (component)\n",
    "            self.component = component\n",
    "        \n",
    "        self.fit_to = fit_to\n",
    "        self.fit_additional = fit_additional\n",
    "        self.apply_to = apply_to\n",
    "        self.raise_error_if_split_doesnot_exist=raise_error_if_split_doesnot_exist\n",
    "        self.raise_warning_if_split_doesnot_exist=raise_warning_if_split_doesnot_exist\n",
    "    \n",
    "    def _fit (self, X, y=None):\n",
    "        if not isinstance(X, dict):\n",
    "            X = {self.fit_to: X}\n",
    "        component = self.components[0]\n",
    "        additional_data = {}\n",
    "        for split in self.fit_additional:\n",
    "            if split not in ['validation', 'test']:\n",
    "                raise ValueError (f'split {split} not valid')\n",
    "            if split in X.keys():\n",
    "                additional_data[f'{split}_data'] = X[split]\n",
    "            else:\n",
    "                self._issue_error_or_warning (split, X)\n",
    "        \n",
    "        component.fit(X[self.fit_to], y=y, split='training', **additional_data)\n",
    "    \n",
    "    def _issue_error_or_warning (self, split, X):\n",
    "        message = f'split {split} not found in X keys ({X.keys()})'\n",
    "        if self.raise_error_if_split_doesnot_exist:\n",
    "            raise RuntimeError (message)\n",
    "        elif self.raise_warning_if_split_doesnot_exist:\n",
    "            warnings.warn (message)\n",
    "    \n",
    "    def _apply (self, X, apply_to = None, **kwargs):\n",
    "        apply_to = self.apply_to if apply_to is None else apply_to\n",
    "        apply_to = apply_to if isinstance(apply_to, list) else [apply_to]\n",
    "        if not isinstance(X, dict):\n",
    "            key = apply_to[0] if len(apply_to)==1 else 'test'\n",
    "            X = {key: X}\n",
    "            input_not_dict = True\n",
    "        else:\n",
    "            input_not_dict = False\n",
    "            \n",
    "        component = self.components[0]\n",
    "        result = {}\n",
    "        for split in apply_to:\n",
    "            if split in X.keys():\n",
    "                result[split] = component.apply (X[split], split=split, **kwargs)\n",
    "            else:\n",
    "                self._issue_error_or_warning (split, X)\n",
    "        \n",
    "        if input_not_dict:\n",
    "            result = result[key]\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ea301",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tests / usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9767a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# example 1: apply transform on multiple splits\n",
    "data = dict(training = np.array([1,2,3]).reshape(-1,1),\n",
    "            validation = np.array([10,20,30]).reshape(-1,1),\n",
    "            test = np.array([100,200,300]).reshape(-1,1)\n",
    "            )\n",
    "\n",
    "class Transform1 (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.estimator= Bunch(sum = 1)\n",
    "        \n",
    "    def _fit (self, X, y=None):\n",
    "        self.estimator.sum = X.sum(axis=0)\n",
    "    \n",
    "    def _apply (self, x):\n",
    "        return x*1000 + self.estimator.sum\n",
    "    \n",
    "multi_transform1 = MultiSplitComponent (component = Transform1())\n",
    "result = multi_transform1.fit_transform (data)\n",
    "\n",
    "assert type(result) is dict\n",
    "assert result.keys() == data.keys()\n",
    "for split in result.keys():\n",
    "    assert (result[split]==sum(data['training'].ravel())+data[split]*1000).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8205aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2: fit method gets training, validation and test\n",
    "# we apply the transform to only test\n",
    "class Transform2 (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "        self.estimator= Bunch(maxim = 1)\n",
    "        \n",
    "    def _fit (self, X, y=None, validation_data=None, test_data=None):\n",
    "        self.estimator.maxim = X.max(axis=0)\n",
    "        \n",
    "        print (f'validation_data: {validation_data}')\n",
    "        print (f'test_data: {test_data}')\n",
    "        \n",
    "        self.data = dict (validation=validation_data,\n",
    "                          test=test_data)\n",
    "    \n",
    "    def _apply (self, x):\n",
    "        return x*100 + self.estimator.maxim\n",
    "        \n",
    "tr2 = Transform2()\n",
    "multi_transform2 = MultiSplitComponent (component=tr2,\n",
    "                                        fit_additional = ['validation', 'test'])\n",
    "\n",
    "# we apply the transform to only test data\n",
    "result = multi_transform2.fit_transform (data, apply_to='test')\n",
    "\n",
    "assert type(result) is dict\n",
    "assert list(result.keys()) == ['test']\n",
    "for split in result.keys():\n",
    "    assert (result[split]==max(data['training'].ravel())+data[split]*100).all()\n",
    "\n",
    "for split in ['validation', 'test']:\n",
    "    assert (tr2.data[split] == data[split]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abace90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that we can chain transformations\n",
    "\n",
    "result = multi_transform1.fit_transform (data)\n",
    "result = multi_transform2.fit_transform (result, apply_to='test')\n",
    "\n",
    "import pytest \n",
    "\n",
    "#check that we have no error if split does not exist\n",
    "result = multi_transform1.fit_transform (data, apply_to=['training', 'validation'])\n",
    "result = multi_transform2.fit_transform (result, apply_to=['test'])\n",
    "assert len(result)==0\n",
    "\n",
    "#check that we have an error if we set the flag `raise_error_if_split_doesnot_exist=True`\n",
    "multi_transform2.raise_error_if_split_doesnot_exist = True\n",
    "result = multi_transform1.fit_transform (data, apply_to=['training', 'validation'])\n",
    "with pytest.raises (RuntimeError):\n",
    "    result = multi_transform2.fit_transform (result, apply_to=['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b6354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check loading / saving\n",
    "from block_types.utils.utils import remove_previous_results\n",
    "from block_types.core.utils import PickleIO\n",
    "\n",
    "path_results = 'results_multi_split'\n",
    "\n",
    "remove_previous_results (path_results=path_results)\n",
    "\n",
    "tr = PickleSaverComponent (FunctionTransformer (lambda x: x*2),\n",
    "                name='times2',\n",
    "                path_results=path_results)\n",
    "\n",
    "multi_transform = MultiSplitComponent (component=tr,\n",
    "                                       apply_to = ['validation', 'test'],\n",
    "                                       path_results = path_results,\n",
    "                                       data_io=PickleIO (path_results = path_results))\n",
    "\n",
    "result = multi_transform (data)\n",
    "\n",
    "multi_transform2 = MultiSplitComponent (data_io=PickleIO (path_results = path_results))\n",
    "\n",
    "result2 = multi_transform2.data_io.load_result ()\n",
    "\n",
    "for k in result.keys():\n",
    "    assert (result[k] == result2[k]).all()\n",
    "\n",
    "assert result.keys()==result2.keys()\n",
    "\n",
    "assert sorted(os.listdir(path_results))==['test', 'validation', 'whole']\n",
    "\n",
    "assert (tr.data_io.load_result(split='test') == result['test']).all()\n",
    "\n",
    "assert (tr.data_io.load_result(split='validation') == result['validation']).all()\n",
    "\n",
    "assert os.listdir(f'{path_results}/validation')==['times2_result.pk']\n",
    "\n",
    "assert os.listdir(f'{path_results}/test')==['times2_result.pk']\n",
    "\n",
    "assert os.listdir(f'{path_results}/whole')==['multi_split_component_result.pk']\n",
    "\n",
    "remove_previous_results (path_results=path_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (block_types)",
   "language": "python",
   "name": "block_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
