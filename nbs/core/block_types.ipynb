{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0490500e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp core.block_types\n",
    "from nbdev.showdoc import *\n",
    "from block_types.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=['dummy'])\n",
    "    \n",
    "from block_types.core.block_types import __all__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d01038",
   "metadata": {},
   "source": [
    "# Block types\n",
    "\n",
    "> Types of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e370aa98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partialmethod\n",
    "from typing import Optional, Union\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import joblib\n",
    "from IPython.display import display\n",
    "\n",
    "# block_types\n",
    "from block_types.core.data_conversion import (DataConverter, NoConverter, PandasConverter, \n",
    "                                              StandardConverter, GenericConverter, \n",
    "                                              data_converter_factory)\n",
    "from block_types.core.utils import (save_csv,  save_parquet,  save_multi_index_parquet, \n",
    "                                    save_keras_model,  save_csv_gz, read_csv, read_csv_gz)\n",
    "from block_types.core.utils import DataIO, SklearnIO, PandasIO, NoSaverIO\n",
    "from block_types.core.utils import data_io_factory\n",
    "from block_types.core.utils import ModelPlotter, Profiler, Comparator\n",
    "from block_types.core.utils import camel_to_snake, snake_to_camel\n",
    "from block_types.utils.utils import (set_logger, delete_logger, replace_attr_and_store,  \n",
    "                                     get_specific_dict_param, get_hierarchy_level)\n",
    "import block_types.config.bt_defaults as dflt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb54de3-5c12-42df-9058-ac2234569fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.utils import Bunch\n",
    "from pathlib import Path\n",
    "\n",
    "import block_types.config.bt_defaults as dflt\n",
    "from block_types.utils.utils import remove_previous_results, check_last_part\n",
    "from block_types.core.data_conversion import DataConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c79275f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class Component ():\n",
    "    \"\"\"Base component class used in our Pipeline.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  estimator=None,\n",
    "                  name: Optional[str] = None,\n",
    "                  class_name: Optional[str] = None,\n",
    "                  suffix: Optional[str] = None,\n",
    "                  group: str = dflt.group,\n",
    "                  root=None,\n",
    "                  overwrite_field: bool = dflt.overwrite_field,\n",
    "                  error_if_present: bool = dflt.error_if_present,\n",
    "                  ignore:set = set(),\n",
    "                  but: Union[str, list] = '',\n",
    "                  data_converter: Optional[DataConverter] = None,\n",
    "                  data_io: Optional[DataIO] = None,\n",
    "                  model_plotter: Optional[ModelPlotter] = None,\n",
    "                  profiler: Optional[Profiler] = None,\n",
    "                  comparator: Optional[Comparator] = None,\n",
    "                  apply = None,\n",
    "                  direct_apply: bool = False,\n",
    "                  direct_fit: bool = False,\n",
    "                  direct_fit_apply: bool = False,\n",
    "                  error_if_apply: bool = False,\n",
    "                  error_if_fit: bool = False,\n",
    "                  error_if_fit_apply: bool = False,\n",
    "                  logger=None,\n",
    "                  verbose: int = dflt.verbose,\n",
    "                  name_logger:str = dflt.name_logger,\n",
    "                  mode_logger:str = dflt.mode_logger,\n",
    "                  **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize attributes and fields.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : estimator (classifier or transformer) or None, optional\n",
    "            Estimator being wrapped.\n",
    "        name : Pipeline or None, optional\n",
    "            Name of component. If not provided, it is inferred from the name of the \n",
    "            estimator's class, or the name of the custom class defining the componet.\n",
    "        data_converter : DataConverter or None, optional\n",
    "            Converts incoming data to format expected by component, and convert \n",
    "            outgoing result to format expected by caller.\n",
    "        data_io : DataIO or None, optional\n",
    "            Manages data serialization and deserialization.\n",
    "        model_plotter : ModelPlotter or None, optional\n",
    "            Helper object that allows to retrieve information to be shown about this \n",
    "            component, as part of a Pipeline diagram.\n",
    "        logger : logging.logger or None, optional\n",
    "            Logger used to write messages\n",
    "        verbose : int, optional\n",
    "            Verbosity, 0: warning or critical, 1: info, 2: debug.\n",
    "        \"\"\"\n",
    "\n",
    "        assert not isinstance(estimator, Component), 'estimator cannot be an instance of Component'\n",
    "        \n",
    "        # name of current component, for logging and plotting purposes\n",
    "        self._determine_component_name (name, estimator, class_name=class_name, suffix=suffix, apply=apply)\n",
    "        \n",
    "        # obtain hierarchy_level\n",
    "        self.hierarchy_level = get_hierarchy_level (base_class=Component)\n",
    "        \n",
    "        # store __init__ attrs into `self`\n",
    "        but = ', '.join (but) if isinstance(but, list) else but\n",
    "        but = (but + ', ') if len(but)>0 else but\n",
    "        but = but + 'ignore, but, overwrite_field, error_if_present, path_results, path_models, apply'\n",
    "        if isinstance (ignore, str): ignore = set(re.split(', *', ignore))\n",
    "        ignore.update ({'name', 'class_name', 'suffix', 'apply'})\n",
    "        replace_attr_and_store (base_class=Component, but=but, \n",
    "                                error_if_present=error_if_present, overwrite=overwrite_field, \n",
    "                                ignore=ignore)\n",
    "        \n",
    "        if self.logger is None:\n",
    "            self.logger = set_logger (self.name_logger, verbose=self.verbose, mode=self.mode_logger)\n",
    "            \n",
    "        # obtain class-specific kwargs\n",
    "        kwargs = self.obtain_config_params (**kwargs)\n",
    "\n",
    "        # object that manages loading / saving\n",
    "        if self.data_io is None:\n",
    "            self.data_io = DataIO (component=self, **kwargs)\n",
    "        else:\n",
    "            if 'data_io' in kwargs:\n",
    "                del kwargs['data_io']\n",
    "            self.data_io = data_io_factory (self.data_io, component=self, **kwargs)\n",
    "            \n",
    "        self.path_results = self.data_io.path_results\n",
    "        self.path_models = self.data_io.path_models\n",
    "\n",
    "        # data converter\n",
    "        if self.data_converter is None:\n",
    "            # TODO: have DataConverter store a reference to component, and use the logger from that reference.\n",
    "            self.data_converter = GenericConverter (**kwargs)\n",
    "        else:\n",
    "            if 'data_converter' in kwargs:\n",
    "                del kwargs['data_converter']\n",
    "            self.data_converter = data_converter_factory (self.data_converter, \n",
    "                                                          **kwargs)\n",
    "        # plotting model component\n",
    "        if self.model_plotter is None:\n",
    "            self.model_plotter = ModelPlotter (component=self, **kwargs)\n",
    "        else:\n",
    "            self.model_plotter.set_component (self)\n",
    "            \n",
    "        # profiling computational cost\n",
    "        if self.profiler is None:\n",
    "            self.profiler = Profiler (self, **kwargs)\n",
    "        \n",
    "        # comparing results against other implementations of this component\n",
    "        if self.comparator is None:\n",
    "            self.comparator = Comparator (self, **kwargs)\n",
    "        elif type(self.comparator) is type:\n",
    "            self.comparator = self.comparator (self, **kwargs)\n",
    "            \n",
    "        # determine and assign apply and fit functions\n",
    "        self.assign_apply_and_fit_functions (apply=apply)\n",
    "    \n",
    "    def __repr__ (self):\n",
    "        return f'Component {self.class_name} (name={self.name})'\n",
    "            \n",
    "    def reset_logger (self):\n",
    "        delete_logger (self.name_logger)\n",
    "        \n",
    "    def get_specific_data_io_parameters (self, tag, **kwargs):\n",
    "        suffix = f'_{tag}'\n",
    "        n = len(suffix)\n",
    "        return {k[:-n]:kwargs[k] \n",
    "                for k in kwargs if k.endswith (suffix) and k[:-n] in DataIO.specific_params}\n",
    "    \n",
    "    def obtain_config_params (self, tag=None, **kwargs):\n",
    "        \"\"\"Overwrites parameters in kwargs with those found in a dictionary of the same name \n",
    "        as the component.\n",
    "        \n",
    "        Checks if there is a parameter whose name is the name of the class or the name given \n",
    "        to this component. In that case, it overwrites the parameters in kwargs with those \n",
    "        found in that dictionary. The parameters in kwargs can be used as *global* parameters\n",
    "        for multiple components, while parameters specific of one component can be overwritten \n",
    "        using a dictionary with the name of that component. See example below.\n",
    "        \"\"\"\n",
    "        k = get_specific_dict_param (self, **kwargs)\n",
    "        \n",
    "        if k is not None:\n",
    "            config = kwargs.copy()\n",
    "            config.update (config[k])\n",
    "        else:\n",
    "            config = kwargs\n",
    "            \n",
    "        if tag is not None:\n",
    "            if tag == '__name__': tag = self.name\n",
    "            self.tag = tag\n",
    "            config.update (self.get_specific_data_io_parameters (tag, **kwargs))\n",
    "            \n",
    "        config.update(verbose=self.verbose, \n",
    "                      logger=self.logger)\n",
    "        \n",
    "        return config\n",
    "\n",
    "    def _determine_component_name (self, name: str, estimator, class_name:Optional[str]=None,\n",
    "                                   suffix:Optional[str]=None, apply=None) -> None:\n",
    "        \"\"\"\n",
    "        Determines an appropriate name for the component if not provided by input.\n",
    "        \n",
    "        If not provided, it is inferred from the name of the estimator's class, or \n",
    "        the name of the custom class defining the componet.\n",
    "        \"\"\"\n",
    "        if class_name is not None:\n",
    "            self.class_name = class_name\n",
    "        else:\n",
    "            self.class_name = self.__class__.__name__\n",
    "            if self.class_name in __all__:\n",
    "                if estimator is not None: self.class_name = estimator.__class__.__name__\n",
    "                if apply is not None and hasattr (apply, '__name__'): \n",
    "                    self.class_name = snake_to_camel (apply.__name__)\n",
    "\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        else:\n",
    "            self.name = camel_to_snake (self.class_name)\n",
    "        \n",
    "        self.suffix = suffix\n",
    "        if self.suffix is not None:\n",
    "            self.name = f'{self.name}_{self.suffix}'\n",
    "            \n",
    "    def create_estimator (self, **kwargs):\n",
    "        self.estimator = Bunch(**kwargs)\n",
    "            \n",
    "    def fit_like (self, *X, y=None, load=None, save=None, split=None,\n",
    "                  func='_fit', validation_data=None, test_data=None, \n",
    "                  sequential_fit_apply=False, converter_args={}, **kwargs):\n",
    "        \"\"\"\n",
    "        Estimates the parameters of the component based on given data X and labels y.\n",
    "        \n",
    "        Uses the previously fitted parameters if they're found in disk and load \n",
    "        is True.\n",
    "        \"\"\"\n",
    "        self.profiler.start_timer ()\n",
    "        if self.error_if_fit and func=='_fit': raise RuntimeError (f'{self.name} should not call fit')\n",
    "        if self.error_if_fit_apply and func=='_fit_apply': \n",
    "            raise RuntimeError (f'{self.name} should not call fit_apply')\n",
    "        X = self.data_converter.convert_single_tuple_for_fitting (X)\n",
    "        X = X + (y, ) if y is not None else X\n",
    "        \n",
    "        if split is not None:\n",
    "            self.original_split = self.data_io.split\n",
    "            self.set_split (split)\n",
    "\n",
    "        self.logger.info (f'fitting {self.name} (using {self.data_io.split} data)')\n",
    "            \n",
    "        previous_estimator = None\n",
    "        if self.data_io.can_load_model (load):\n",
    "            previous_estimator = self.data_io.load_estimator()\n",
    "            \n",
    "        already_computed = False\n",
    "        if previous_estimator is not None:\n",
    "            if func=='_fit':\n",
    "                already_computed = True\n",
    "            elif func=='_fit_apply':\n",
    "                previous_result = None\n",
    "                if self.data_io.can_load_result (load):\n",
    "                    previous_result = self.data_io.load_result (split=split)\n",
    "                already_computed = previous_result is not None\n",
    "            else:\n",
    "                raise ValueError (f'function {func} not valid')\n",
    "            \n",
    "        if not already_computed:\n",
    "            X = copy.deepcopy (X) if self.data_converter.inplace else X\n",
    "            if func=='_fit_apply': \n",
    "                X = self.data_converter.convert_before_fit_apply (\n",
    "                    *X, sequential_fit_apply=sequential_fit_apply, **converter_args)\n",
    "                X = self.data_converter.convert_no_tuple (X)\n",
    "            elif func=='_fit':\n",
    "                X = self.data_converter.convert_before_fitting (*X)\n",
    "            else:\n",
    "                raise ValueError (f'function {func} not valid')\n",
    "            additional_data= self._add_validation_and_test (validation_data, test_data)\n",
    "            if func=='_fit':\n",
    "                if len(kwargs) > 0: raise AttributeError (f'kwargs: {kwargs} not valid')\n",
    "                self.profiler.start_no_overhead_timer ()\n",
    "                self._fit (*X, **additional_data)\n",
    "            elif func=='_fit_apply':\n",
    "                assert self.fit_apply_func is not None, ('object must have _fit_apply method or one of '\n",
    "                                                    'its aliases implemented when func=\"_fit_apply\"')\n",
    "                self.profiler.start_no_overhead_timer ()\n",
    "                result = self.fit_apply_func (*X, **additional_data, **kwargs)\n",
    "            else:\n",
    "                raise ValueError (f'function {func} not valid')\n",
    "            self.profiler.finish_no_overhead_timer (method=func, split=self.data_io.split)\n",
    "            if func=='_fit':\n",
    "                _ = self.data_converter.convert_after_fitting (*X)\n",
    "            elif func=='_fit_apply':\n",
    "                result = self.data_converter.convert_after_fit_apply (\n",
    "                    result, sequential_fit_apply=sequential_fit_apply, **converter_args)\n",
    "                if self.data_io.can_save_result (save, split):\n",
    "                    self.data_io.save_result (result, split=split)\n",
    "            else:\n",
    "                raise ValueError (f'function {func} not valid')\n",
    "            if self.data_io.can_save_model (save):\n",
    "                self.data_io.save_estimator ()\n",
    "        else:\n",
    "            self.set_estimator (previous_estimator)\n",
    "            self.logger.info (f'loaded pre-trained {self.name}')\n",
    "            if func=='_fit_apply':\n",
    "                result = previous_result\n",
    "                self.logger.info (f'loaded pre-computed result')\n",
    "            \n",
    "        self.profiler.finish_timer (method=func, split=self.data_io.split)\n",
    "            \n",
    "        if split is not None:\n",
    "            self.set_split (self.original_split)\n",
    "            \n",
    "        if func=='_fit':\n",
    "            return self\n",
    "        else:\n",
    "            return result\n",
    "     \n",
    "    fit = partialmethod (fit_like, func='_fit')\n",
    "    \n",
    "    def fit_apply (self, *X, y=None, load_model=None, save_model=None, \n",
    "                   load_result=None, save_result=None, func='_fit', \n",
    "                   validation_data=None, test_data=None, sequential_fit_apply=False, \n",
    "                   **kwargs):\n",
    "        \n",
    "        if self.error_if_fit_apply: raise RuntimeError (f'{self.name} should not call fit_apply')\n",
    "        \n",
    "        X = self.data_converter.convert_single_tuple_for_fitting (X)\n",
    "        X = X + (y, ) if y is not None else X\n",
    "        \n",
    "        if self.fit_apply_func is not None:\n",
    "            return self.fit_like (*X, \n",
    "                                  load=load_model, save=save_model, \n",
    "                                  func='_fit_apply', validation_data=validation_data,\n",
    "                                  test_data=test_data, sequential_fit_apply=sequential_fit_apply,\n",
    "                                  **kwargs)\n",
    "        else: \n",
    "            if not self.direct_fit:\n",
    "                kwargs_fit = dict(load=load_model, save=save_model, \n",
    "                                  validation_data=validation_data, \n",
    "                                  test_data=test_data, \n",
    "                                  sequential_fit_apply=sequential_fit_apply)\n",
    "            else:\n",
    "                kwargs_fit = dict()\n",
    "            if not self.direct_apply:\n",
    "                kwargs_apply = dict (load=load_result, save=save_result, fit_apply=True, \n",
    "                                     sequential_fit_apply=sequential_fit_apply, **kwargs)\n",
    "            else:\n",
    "                kwargs_apply = kwargs\n",
    "            return self.fit (*X, **kwargs_fit).apply (*X, **kwargs_apply)\n",
    "    \n",
    "    def _add_validation_and_test (self, validation_data, test_data):\n",
    "        additional_data = {}\n",
    "        def add_data (data, split_name):\n",
    "            if data is not None:\n",
    "                if isinstance(data, tuple):\n",
    "                    if len(data) > 0:\n",
    "                        newX = data[0]\n",
    "                    else:\n",
    "                        self.logger.warning (f'empty {split_name}')\n",
    "                        newX = None\n",
    "                    if len(data) == 2:\n",
    "                        newy = data[1]\n",
    "                    elif len(data)==1:\n",
    "                        newy = None\n",
    "                    elif len(data)>2:\n",
    "                        raise ValueError (f'{split_name} must have at most 2 elements')\n",
    "                else:\n",
    "                    newX = data\n",
    "                    newy = None\n",
    "                newX, newy = self.data_converter.convert_before_fitting (newX, newy)\n",
    "                if newy is not None:\n",
    "                    additional_data[split_name] = (newX, newy)\n",
    "                else:\n",
    "                    additional_data[split_name] = newX\n",
    "        \n",
    "        add_data (validation_data, 'validation_data')\n",
    "        add_data (test_data, 'test_data')\n",
    "        \n",
    "        return additional_data\n",
    "    \n",
    "    # aliases\n",
    "    fit_transform = fit_apply\n",
    "    fit_predict = fit_apply\n",
    "\n",
    "    def __call__ (self, *X, load=None, save=None, fit_apply=False, sequential_fit_apply=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Transforms the data X and returns the transformed data.\n",
    "        \n",
    "        Uses the previously transformed data if it's found in disk and load \n",
    "        is True.\n",
    "        \"\"\"\n",
    "        self.profiler.start_timer ()\n",
    "        if self.direct_apply: return self.result_func (*X, **kwargs)\n",
    "        if self.error_if_apply: raise RuntimeError (f'{self.name} should not call apply')\n",
    "        assert self.result_func is not None, 'apply function not implemented'\n",
    "        result = self._compute_result (X, self.result_func, load=load, save=save, fit_apply=fit_apply, \n",
    "                                       sequential_fit_apply=sequential_fit_apply, **kwargs)\n",
    "        return result\n",
    "\n",
    "    def _assign_fit_func (self):\n",
    "        self.fit_func = None\n",
    "        self.estimator_fit_func = None\n",
    "        if callable(getattr(self, '_fit', None)):\n",
    "            self.fit_func = self._fit\n",
    "        elif self.estimator is not None and callable(getattr(self.estimator, 'fit', None)):\n",
    "            self.fit_func = self.estimator.fit\n",
    "            self.estimator_fit_func = 'fit'\n",
    "    \n",
    "    def _assign_result_func (self):\n",
    "        implemented = []\n",
    "        self.result_func = None\n",
    "        self.estimator_result_func = None\n",
    "        if callable(getattr(self, '_apply', None)):\n",
    "            self.result_func = self._apply\n",
    "            implemented += [self.result_func]\n",
    "        if callable(getattr(self, '_transform', None)):\n",
    "            self.result_func = self._transform\n",
    "            implemented += [self.result_func]\n",
    "        if callable(getattr(self, '_predict', None)):\n",
    "            self.result_func = self._predict\n",
    "            implemented += [self.result_func]\n",
    "        if len(implemented)==0:\n",
    "            if self.estimator is not None and callable(getattr(self.estimator, 'transform', None)):\n",
    "                self.result_func = self.estimator.transform\n",
    "                self.estimator_result_func = 'transform'\n",
    "                implemented += [self.result_func]\n",
    "            if self.estimator is not None and callable(getattr(self.estimator, 'predict', None)):\n",
    "                self.result_func = self.estimator.predict\n",
    "                self.estimator_result_func = 'predict'\n",
    "                implemented += [self.result_func]\n",
    "        if len(implemented) > 1:\n",
    "            raise AttributeError (f'{self.class_name} must have only one of _transform, _apply, '\n",
    "                                  f'or _predict methods implemented => found: {implemented}')\n",
    "        \n",
    "    def _assign_fit_apply_func (self):\n",
    "        implemented = []\n",
    "        self.fit_apply_func = None\n",
    "        self.estimator_fit_apply_func = None\n",
    "        if callable(getattr(self, '_fit_apply', None)):\n",
    "            self.fit_apply_func = self._fit_apply\n",
    "            implemented += [self.fit_apply_func]\n",
    "        if callable(getattr(self, '_fit_transform', None)):\n",
    "            self.fit_apply_func = self._fit_transform\n",
    "            implemented += [self.fit_apply_func]\n",
    "        if callable(getattr(self, '_fit_predict', None)):\n",
    "            self.fit_apply_func = self._fit_predict\n",
    "            implemented += [self.fit_apply_func]\n",
    "        if len(implemented)==0:\n",
    "            if self.estimator is not None and callable(getattr(self.estimator, 'fit_transform', None)):\n",
    "                self.fit_apply_func = self.estimator.fit_transform\n",
    "                self.estimator_fit_apply_func = 'fit_transform'\n",
    "                implemented += [self.fit_apply_func]\n",
    "            if self.estimator is not None and callable(getattr(self.estimator, 'fit_predict', None)):\n",
    "                self.fit_apply_func = self.estimator.fit_predict\n",
    "                self.estimator_fit_apply_func = 'fit_predict'\n",
    "                implemented += [self.fit_apply_func]\n",
    "        if len(implemented) > 1:\n",
    "            raise AttributeError (f'{self.class_name} must have only one of fit_transform, fit_apply, '\n",
    "                                  f'or fit_predict methods implemented => found: {implemented}')\n",
    "        \n",
    "    def assign_apply_and_fit_functions (self, apply=None):\n",
    "        \"\"\"Determine and assign apply and fit functions.\"\"\"\n",
    "        if apply is not None: self._apply = apply\n",
    "        self._assign_result_func ()\n",
    "        self._assign_fit_apply_func ()\n",
    "        self._assign_fit_func ()\n",
    "        self.is_model = True\n",
    "        if self.fit_func is None:\n",
    "            self._fit = self._fit_\n",
    "            if self.fit_apply_func is None:\n",
    "                self.is_model = False\n",
    "        else:\n",
    "            self._fit = self.fit_func\n",
    "        if self.direct_apply:\n",
    "            self.set_apply (self.result_func)\n",
    "        if not self.is_model:\n",
    "            self.fit = self._fit_\n",
    "            # self.set_fit_apply (self.apply)\n",
    "        else:\n",
    "            if self.direct_fit:\n",
    "                self.fit = self.fit_func\n",
    "            if self.direct_fit_apply:\n",
    "                self.set_fit_apply (self.fit_apply_func)\n",
    "            \n",
    "    # aliases for transform method\n",
    "    apply = __call__\n",
    "    transform = __call__\n",
    "    predict = partialmethod (__call__, converter_args=dict(new_columns=['prediction']))\n",
    "\n",
    "    def _compute_result (self, X, result_func, load=None, save=None, split=None,\n",
    "                         converter_args={}, fit_apply=False, \n",
    "                         sequential_fit_apply=False, **kwargs):\n",
    "        \n",
    "        if split is not None:\n",
    "            self.original_split = self.data_io.split\n",
    "            self.set_split (split)\n",
    "            \n",
    "        self.logger.debug (f'applying {self.name} (on {self.data_io.split} data)')\n",
    "            \n",
    "        previous_result = None\n",
    "        if self.data_io.can_load_result (load):\n",
    "            previous_result = self.data_io.load_result (split=split)\n",
    "        if previous_result is None:\n",
    "            X = self.data_converter.convert_single_tuple_for_transforming (X)\n",
    "            X = self.data_converter.convert_before_transforming (\n",
    "                *X, fit_apply=fit_apply, sequential_fit_apply=sequential_fit_apply, **converter_args)\n",
    "            X = self.data_converter.convert_no_tuple (X)\n",
    "            self.profiler.start_no_overhead_timer ()\n",
    "            X = self.data_converter.convert_single_tuple_for_result_func (X)\n",
    "            result = result_func (*X, **kwargs)\n",
    "            self.profiler.finish_no_overhead_timer ('apply', self.data_io.split)\n",
    "            result = self.data_converter.convert_after_transforming (\n",
    "                result, fit_apply=fit_apply, sequential_fit_apply=sequential_fit_apply, **converter_args)\n",
    "            if self.data_io.can_save_result (save, split):\n",
    "                self.data_io.save_result (result, split=split)\n",
    "        else:\n",
    "            result = previous_result\n",
    "            self.logger.info (f'loaded pre-computed result')\n",
    "            \n",
    "        self.profiler.finish_timer ('apply', self.data_io.split)\n",
    "        if split is not None:\n",
    "            self.set_split (self.original_split)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def _fit_ (self, *X, **kwargs):\n",
    "        return self\n",
    "                        \n",
    "    def show_result_statistics (self, result=None, split=None) -> None:\n",
    "        \"\"\"\n",
    "        Show statistics of transformed data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        result: DataFrame or other data structure or None, optional\n",
    "            Transformed data whose statistics we show. If not provided, it is loaded \n",
    "            from disk.\n",
    "        training_data_flag: bool, optional\n",
    "            If True, transformed training data is loaded, otherwise transformed test \n",
    "            data is loaded.\n",
    "        \"\"\"\n",
    "        if result is None:\n",
    "            df = self.load_result(split=split)\n",
    "        else:\n",
    "            df = result\n",
    "        \n",
    "        if df is not None:\n",
    "            display (self.name)\n",
    "            if callable(getattr(df, 'describe', None)):\n",
    "                display (df.describe())\n",
    "            elif isinstance(df, np.ndarray) or isinstance(df, list):\n",
    "                df = pd.DataFrame (df)\n",
    "                display (df.describe())\n",
    "                \n",
    "    def remove_non_pickable_fields (self):\n",
    "        pass\n",
    "\n",
    "    # ********************************\n",
    "    # exposing some data_io and data_converters methods\n",
    "    # ********************************\n",
    "    def load_estimator (self):\n",
    "        estimator = self.data_io.load_estimator ()\n",
    "        if estimator is not None:\n",
    "            self.set_estimator (estimator)\n",
    "            \n",
    "    def load_result (self, split=None, path_results=None, result_file_name=None):\n",
    "        return self.data_io.load_result (split=split, path_results=path_results, \n",
    "                                         result_file_name=result_file_name)\n",
    "    \n",
    "    def assert_equal (self, item1, item2=None, split=None, raise_error=True, **kwargs):\n",
    "        return self.comparator.assert_equal (item1, item2=item2, split=split, \n",
    "                                             raise_error=raise_error, **kwargs)\n",
    "        \n",
    "    # ********************************\n",
    "    # setters\n",
    "    # ********************************\n",
    "    def set_split (self, split):\n",
    "        self.data_io.set_split (split)\n",
    "    \n",
    "    def set_save_splits (self, save_splits):\n",
    "        self.data_io.set_save_splits (save_splits)\n",
    "\n",
    "    def set_save_model (self, save_model):\n",
    "        self.data_io.set_save_model (save_model)\n",
    "        \n",
    "    def set_load_model (self, load_model):\n",
    "        self.data_io.set_load_model (load_model)\n",
    "        \n",
    "    def set_save_result (self, save_result):\n",
    "        self.data_io.set_save_result (save_result)\n",
    "        \n",
    "    def set_load_result (self, load_result):\n",
    "        self.data_io.set_load_result (load_result)\n",
    "        \n",
    "    def set_data_io (self, data_io, copy=False):\n",
    "        self.data_io = copy.copy(data_io) if copy else data_io\n",
    "        self.data_io.setup (self)\n",
    "\n",
    "    def set_name (self, name):\n",
    "        self.name = name\n",
    "        self.data_io.set_file_names (name)\n",
    "        \n",
    "    def set_estimator (self, estimator):\n",
    "        self.estimator = estimator\n",
    "        if self.estimator_result_func is not None:\n",
    "            self.result_func = getattr (self.estimator, self.estimator_result_func, None)\n",
    "            assert callable (self.result_func)\n",
    "        if self.estimator_fit_apply_func is not None:\n",
    "            self.fit_apply_func = getattr (self.estimator, self.estimator_fit_apply_func, None)\n",
    "            assert callable (self.fit_apply_func)\n",
    "        if self.estimator_fit_func is not None:\n",
    "            self.fit_func = getattr (self.estimator, self.estimator_fit_func, None)\n",
    "            assert callable (self.fit_func)\n",
    "            self._fit = self.fit_func\n",
    "            assert self.is_model\n",
    "            \n",
    "    def set_apply (self, result_func):\n",
    "        self.apply = result_func\n",
    "        self.__call__ = result_func\n",
    "        self.transform = result_func\n",
    "        self.predict = result_func\n",
    "    \n",
    "    def set_fit_apply (self, fit_apply_func):\n",
    "        self.fit_apply = fit_apply_func\n",
    "        self.fit_transform = fit_apply_func\n",
    "        self.fit_predict = fit_apply_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a295c75-06e7-4443-bb62-4b49d160cd13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuring component with global and specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9141a8b2-c0cb-4181-9adc-9186bda43181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_config ():\n",
    "    \n",
    "    # **********************************************************************\n",
    "    # test obtain_config_params method\n",
    "    # **********************************************************************\n",
    "    tr = Component(name='sky')\n",
    "    config = dict(first=1,\n",
    "                  second=2,\n",
    "                  third=3,\n",
    "                  sky=dict (second=4)\n",
    "                 )\n",
    "    config_r = tr.obtain_config_params (**config)\n",
    "    logger = set_logger (dflt.name_logger, verbose=dflt.verbose)\n",
    "    assert config_r=={'first': 1, 'second': 4, 'third': 3, 'sky': {'second': 4}, 'verbose': dflt.verbose, 'logger': logger}\n",
    "    assert config == {'first': 1, 'second': 2, 'third': 3, 'sky': {'second': 4}}\n",
    "\n",
    "    # **********************************************************************\n",
    "    # test that component saves results when using global \n",
    "    # parameter save=True\n",
    "    # **********************************************************************\n",
    "    class MyTransform (Component):\n",
    "        def __init__ (self,**kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "            self.create_estimator ()\n",
    "\n",
    "        def _fit (self, X, y=None):\n",
    "            self.estimator.mu = X.mean()\n",
    "        def _transform (self, X):\n",
    "            return X-self.estimator.mu\n",
    "\n",
    "    path_results = 'testing_configuration'\n",
    "    tr = MyTransform (path_results=path_results,\n",
    "                      save = True)\n",
    "\n",
    "    X = np.array([[1,2,3],[4,5,6]])\n",
    "    tr.fit_transform(X)\n",
    "\n",
    "    import os\n",
    "    l = sorted(os.listdir(path_results))\n",
    "    assert l==['models','whole'], f'found: {l}'\n",
    "\n",
    "    # **********************************************************************\n",
    "    # test that component does not save results when we \n",
    "    # use component-specific parameter MyTransform = dict(save=False)\n",
    "    # **********************************************************************\n",
    "    from block_types.utils.utils import remove_previous_results\n",
    "    remove_previous_results (path_results)\n",
    "\n",
    "    tr = MyTransform (data_io = SklearnIO(\n",
    "                                  path_results='testing_configuration',\n",
    "                                  save = True,\n",
    "                                  MyTransform = dict(save=False)\n",
    "                                )\n",
    "                     )\n",
    "    tr.fit_transform(X)\n",
    "    import pytest\n",
    "    with pytest.raises(FileNotFoundError):\n",
    "        os.listdir(path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b766e045-112e-493d-ab67-1d5780e2c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_config\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_config, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ee8cd-5c8e-4937-95d3-c0d8f07d8352",
   "metadata": {},
   "source": [
    "### Recursively storing attrs across class hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5263af6-3223-4e3d-b04c-cf42944b7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_store_attrs ():\n",
    "    # recursively storing __init__ attrs across hiearchy of classes\n",
    "    class Intermediate (Component):\n",
    "        def __init__ (self, x=3, y=4, **kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "\n",
    "    class Final (Intermediate):\n",
    "        def __init__ (self, z=6, h=[2,3,5], **kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "\n",
    "    o = Final (x=9, h=[1,2,4])\n",
    "    assert o.x==9 and o.y==4 and o.z==6 and o.h==[1,2,4]\n",
    "\n",
    "    o = Final (y=7, z=10, h=[1,2,4], Final={'h': [9,11,10]})\n",
    "    assert o.x==3 and o.y==7 and o.z==10 and o.h==[9,11,10]\n",
    "\n",
    "    # only attributes specific of Final are replaced.\n",
    "    # trying to replace attributes specific of Intermediate \n",
    "    # does not work\n",
    "    o = Final (y=7, z=10, h=[1,2,4], Intermediate={'y': 12})\n",
    "    assert o.x==3 and o.y==7 and o.z==10 and o.h==[1,2,4]\n",
    "\n",
    "    class Intermediate (Component):\n",
    "        def __init__ (self, x=3, y=4, **kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "\n",
    "    class Final (Intermediate):\n",
    "        def __init__ (self, z=6, h=[2,3,5], **kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "\n",
    "    o = Final (x=9, h=[1,2,4], group='group_1', group_1={'y': 10, 'z':60})\n",
    "    assert o.x==9 and o.y==10 and o.z==60 and o.h==[1,2,4]\n",
    "    \n",
    "    \n",
    "     # *******************\n",
    "    # test using same field in B4 and in A3, but\n",
    "    # B4 passes that value to A3 in super(),\n",
    "    # after modifying it\n",
    "    # *****************\n",
    "    class A (Component):\n",
    "        def __init__ (self, x=3, path_results='test_recursive', **kwargs):\n",
    "            path_results = f'{path_results}/another'\n",
    "            super ().__init__ (path_results=path_results, error_if_present=True, \n",
    "                               **kwargs)\n",
    "    \n",
    "    class B (A):\n",
    "        def __init__ (self, x=30, y=10, **kwargs):\n",
    "            x = x*2\n",
    "            super().__init__ (x=x, **kwargs)\n",
    "            self.ab = A (**kwargs)\n",
    "\n",
    "    b = B ()\n",
    "    assert b.x==60 and b.ab.x==3 and b.y==10 and b.path_results==Path('test_recursive/another').resolve()\n",
    "    \n",
    "    b = B (x=6, path_results='new_path')\n",
    "    assert b.x==12 and b.ab.x==3 and b.y==10 and b.path_results==Path('new_path/another').resolve()\n",
    "    \n",
    "    # *******************\n",
    "    # test using same field in C and in A, but\n",
    "    # the field is modified in a parent B\n",
    "    # *****************\n",
    "    class C(B):\n",
    "        def __init__ (self, x=40, z=100, **kwargs):\n",
    "            super().__init__ (x=x, **kwargs)\n",
    "            self.b = B(**kwargs)\n",
    "            \n",
    "    with pytest.raises (RuntimeError):\n",
    "        c = C()\n",
    "        \n",
    "    c = C(ignore={'x'})\n",
    "    assert c.x==80 and c.y==10 and c.z==100 and c.b.x==60 and c.b.y==10\n",
    "    \n",
    "    c = C (x=9, ignore={'x'})\n",
    "    assert c.x==18 and c.y==10 and c.z==100 and c.b.x==60 and c.b.y==10\n",
    "    \n",
    "    assert not hasattr(c, 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e21ae5-42df-44a8-ba91-0de265086d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_store_attrs\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_store_attrs, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125a115",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transform method called with different aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3ca5014-9183-4906-93a5-b6a11c99f939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_aliases ():\n",
    "\n",
    "    # test that we can implement _transform and use all the aliases \n",
    "    # (transform, predict, apply,  __call__)\n",
    "    class MyTransform (Component):\n",
    "        def _transform (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform = MyTransform()\n",
    "    assert my_transform.transform (3) == 6\n",
    "    assert my_transform.predict (3) == 6\n",
    "    assert my_transform.apply (3) == 6\n",
    "    assert my_transform (3) == 6\n",
    "\n",
    "    # test that we can implement _apply and use all the aliases \n",
    "    # (transform, predict, apply and __call__)\n",
    "    class MyTransform2 (Component):\n",
    "        def _apply (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform2 = MyTransform2()\n",
    "    assert my_transform2.transform (3) == 6\n",
    "    assert my_transform2.predict (3) == 6\n",
    "    assert my_transform2.apply (3) == 6\n",
    "    assert my_transform2 (3) == 6\n",
    "\n",
    "    # test that we can implement _predict and use all the aliases \n",
    "    # (transform, predict, apply and __call__)\n",
    "    class MyTransform3 (Component):\n",
    "        def _predict (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform3 = MyTransform3()\n",
    "    assert my_transform3.transform (3) == 6\n",
    "    assert my_transform3.predict (3) == 6\n",
    "    assert my_transform3.apply (3) == 6\n",
    "    assert my_transform3 (3) == 6\n",
    "\n",
    "    # test that an exception is raised if neither _tranform nor _apply are defined\n",
    "    class MyTransform4 (Component):\n",
    "        def _wrong_method (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform4 = MyTransform4 ()\n",
    "\n",
    "    import pytest\n",
    "    with pytest.raises (AssertionError):\n",
    "        my_transform4.transform(3)\n",
    "\n",
    "\n",
    "    # test that an exception is raised if more than one alias is implemented\n",
    "    class MyTransform5 (Component):\n",
    "        def _predict (self, x):\n",
    "            return x*2\n",
    "        def _apply (self, x):\n",
    "            return x*2\n",
    "\n",
    "    import pytest\n",
    "    with pytest.raises(AttributeError):\n",
    "        my_transform5 = MyTransform5 ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97bce96-d9ac-413e-a5b2-ac698633c1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_aliases\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_aliases, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f20147",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calling `predict` is handy when the result is a single array of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c4cb06-a700-44cc-95ac-2d97b57df41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_predict ():\n",
    "# TODO: remove this cell\n",
    "\n",
    "    class MyTransform (Component):\n",
    "        def __init__ (self, **kwargs):\n",
    "            super().__init__ (\n",
    "                data_converter=PandasConverter(**kwargs),\n",
    "                **kwargs)\n",
    "\n",
    "        def _predict (self, x):\n",
    "            return x['a']+x['b']\n",
    "\n",
    "    my_transform = MyTransform()\n",
    "\n",
    "    df = pd.DataFrame ({'a': [10,20,30],'b':[4,5,6]})\n",
    "\n",
    "    pd.testing.assert_frame_equal(my_transform.transform (df).to_frame(), \n",
    "                                  pd.DataFrame ({0: [14,25,36]})\n",
    "                                 )\n",
    "\n",
    "    if False:\n",
    "        pd.testing.assert_frame_equal(my_transform.predict (df), \n",
    "                                      pd.DataFrame ({0: [14,25,36]})\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25b5429a-d338-4bad-83a8-08a068061f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_predict\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_predict, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a8d8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The `transform` method and its aliases can be called with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4782e6b8-9113-4791-9c09-7d337a805fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "def test_component_multiple_inputs ():\n",
    "    # test that we can apply tranform to multiple data items\n",
    "    from block_types.utils.dummies import SumXY\n",
    "    \n",
    "    my_transform = SumXY ()\n",
    "    result = my_transform.transform (3, 4)\n",
    "    print (result)\n",
    "    assert result==7\n",
    "\n",
    "    # test that we can apply tranform to single data items\n",
    "    class MyTransform2 (Component):\n",
    "        def _apply (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform2 = MyTransform2 ()\n",
    "    result = my_transform2.transform (3)\n",
    "    print (result)\n",
    "    assert result==6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98d3af35-996f-4586-8086-35ab94b2acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_multiple_inputs\n",
      "7\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_multiple_inputs, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273a5c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `fit_apply()` and its aliases `fit_transform(), fit_predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ebc873-efb2-492f-9cce-b763b924091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "# example with _fit_apply implemented\n",
    "class TransformWithFitApply (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "    def _fit (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        return X + self.sum\n",
    "    def _fit_apply (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)*10\n",
    "        return X + self.sum\n",
    "\n",
    "    # example without _fit_apply implemented\n",
    "class TransformWithoutFitApply (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "    def _fit (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        return X + self.sum\n",
    "        \n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_fit_apply ():\n",
    "\n",
    "    tr1 = TransformWithFitApply ()\n",
    "    X = np.array ([100, 90, 10])\n",
    "    result = tr1.fit_apply (X)\n",
    "    assert (result==(X+2000)).all()\n",
    "\n",
    "    # same result obtained by aliases\n",
    "    result = tr1.fit_transform (X)\n",
    "    assert (result==(X+2000)).all()\n",
    "    \n",
    "    # different result if we apply fit and apply separately\n",
    "    result = tr1.fit (X).transform (X)\n",
    "    assert (result==(X+200)).all()\n",
    "\n",
    "    # transform without fit_apply\n",
    "    tr2 = TransformWithoutFitApply ()\n",
    "    result = tr2.fit_apply (X)\n",
    "    assert (result==(X+200)).all()\n",
    "\n",
    "    # same result obtained by aliases\n",
    "    result = tr2.fit_transform (X)\n",
    "    assert (result==(X+200)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "861c63fc-75c1-4011-938f-0cce0deaaaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_fit_apply\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_fit_apply, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06874f8-35dc-4664-af15-c1770a4c8cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `fit_apply()` with DataConverters that transform inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021d6435-3590-4ef1-9029-803534e12c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "# example with _fit_apply implemented\n",
    "class MyDataConverter (DataConverter):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super ().__init__ (**kwargs)\n",
    "    def convert_before_fitting (self, *X):\n",
    "        X, y = X if len(X)==2 else (X[0], None)\n",
    "        self.orig = X[0]\n",
    "        X[0] = 0\n",
    "        return X, y\n",
    "    def convert_after_fitting (self, *X):\n",
    "        X, y = X if len(X)==2 else (X, None)\n",
    "        X[0] = self.orig\n",
    "        return X\n",
    "    def convert_before_transforming (self, X, **kwargs):\n",
    "        self.orig2 = X[1]\n",
    "        X[1] = 0\n",
    "        return X\n",
    "    def convert_after_transforming (self, X, **kwargs):\n",
    "        X[1] = self.orig2\n",
    "        return X\n",
    "    def convert_before_fit_apply (self, *X, **kwargs):\n",
    "        _ = self.convert_before_fitting (*X)\n",
    "        return self.convert_before_transforming (*X)\n",
    "        \n",
    "class TransformWithFitApplyDC (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (data_converter=MyDataConverter,**kwargs)\n",
    "    def _fit (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        return X + self.sum\n",
    "    def _fit_apply (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)\n",
    "        return X + self.sum\n",
    "\n",
    "#@pytest.mark.reference_fails\n",
    "def test_fit_apply_inplace ():\n",
    "    tr1 = TransformWithFitApplyDC ()\n",
    "    X = np.array ([100, 90, 10])\n",
    "    result = tr1.fit_apply (X)\n",
    "    assert (result==[100,  90, 110]).all()\n",
    "    assert (X==[100,  90,  10]).all()\n",
    "\n",
    "    tr1 = TransformWithFitApplyDC (inplace=False)\n",
    "    X = np.array ([100, 90, 10])\n",
    "    result = tr1.fit_apply (X)\n",
    "    assert (result==[10, 90, 20]).all()\n",
    "    assert (X==[ 0,  0, 10]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cfad330-8f8f-49be-88e6-ed37d5d8b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_fit_apply_inplace, tag='dummy', do=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3b34a",
   "metadata": {},
   "source": [
    "`_fit_apply()` is called when implemented, otherwise `fit().apply()` is called"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19effdc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting validation_data and test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cd6693-2d0e-43de-8298-0c21ed787159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_validation_test ():\n",
    "    class Transform1 (Component):\n",
    "        def __init__ (self, **kwargs):\n",
    "            super().__init__ (**kwargs)\n",
    "        def _fit (self, X, y=None, validation_data=None, test_data=None):\n",
    "            self.sum = X.sum(axis=0)\n",
    "\n",
    "            print (f'validation_data: {validation_data}')\n",
    "            print (f'test_data: {test_data}')\n",
    "\n",
    "            self.validation_data = validation_data\n",
    "            self.test_data = test_data\n",
    "\n",
    "        def _apply (self, X):\n",
    "            return X + self.sum\n",
    "\n",
    "    tr1 = Transform1 ()\n",
    "    X = np.array ([100, 90, 10])\n",
    "\n",
    "    # case 1: validation_data and test_data are not tuples\n",
    "    validation_data = np.array ([100, 90, 10])*10\n",
    "    test_data = np.array ([100, 90, 10])*100\n",
    "    result = tr1.fit_apply (X, validation_data=validation_data, test_data=test_data)\n",
    "    assert (tr1.validation_data==validation_data).all()\n",
    "    assert (tr1.test_data==test_data).all()\n",
    "\n",
    "    # case 2: validation_data is a tuple, and test_data is not given\n",
    "    result = tr1.fit_apply (X, validation_data=(validation_data,1))\n",
    "    assert (tr1.validation_data[0]==validation_data).all()\n",
    "    assert tr1.validation_data[1]==1\n",
    "    assert tr1.test_data is None\n",
    "\n",
    "    # case 3: validation_data is a tuple with more than 2 elements, exception is raised\n",
    "    import pytest\n",
    "    with pytest.raises(ValueError):\n",
    "        result = tr1.fit_apply (X, validation_data=(validation_data,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "880045ae-ccac-4ce2-aa00-c9ef4e424010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_validation_test\n",
      "validation_data: [1000  900  100]\n",
      "test_data: [10000  9000  1000]\n",
      "validation_data: (array([1000,  900,  100]), 1)\n",
      "test_data: None\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_validation_test, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf9ef7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### saving / loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcd6e989-11d1-4683-9d1d-2327aa9f46fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "\n",
    "# example with _fit_apply implemented\n",
    "class TransformWithoutFitApply2 (Component):\n",
    "    def __init__ (self, error_if_fit_func=False, error_if_apply_func=False,  **kwargs):\n",
    "        super().__init__ (data_io='SklearnIO', **kwargs)\n",
    "        self.estimator = Bunch(sum=None)\n",
    "    def _fit (self, X, y=None):\n",
    "        if self.error_if_fit_func: raise RuntimeError ('fit should not run')\n",
    "        print ('running _fit')\n",
    "        self.estimator.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        if self.error_if_apply_func: raise RuntimeError ('apply should not run')\n",
    "        if self.estimator.sum is None: raise RuntimeError ('fit should be called before apply')\n",
    "        print ('running _apply')\n",
    "        return X + self.estimator.sum\n",
    "\n",
    "Transform1 = TransformWithoutFitApply2\n",
    "\n",
    "class TransformWithFitApply2 (Component):\n",
    "    def __init__ (self, error_if_fit_func=False, error_if_apply_func=False, error_if_fit_apply_func=False, \n",
    "                  **kwargs):\n",
    "        super().__init__ (data_io='SklearnIO', **kwargs)\n",
    "        self.estimator = Bunch(sum=None)\n",
    "    def _fit (self, X, y=None):\n",
    "        if self.error_if_fit_func: raise RuntimeError ('fit should not run')\n",
    "        print ('running _fit')\n",
    "        self.estimator.sum = X.sum(axis=0)\n",
    "    def _apply (self, X):\n",
    "        if self.error_if_apply_func: raise RuntimeError ('apply should not run')\n",
    "        if self.estimator.sum is None: raise RuntimeError ('fit should be called before apply')\n",
    "        print ('running _apply')\n",
    "        return X + self.estimator.sum\n",
    "    def _fit_apply (self, X, y=None):\n",
    "        if self.error_if_fit_apply_func: raise RuntimeError ('fit_apply should not run')\n",
    "        print ('running _fit_apply')\n",
    "        self.estimator.sum = X.sum(axis=0)\n",
    "        return X + self.estimator.sum\n",
    "\n",
    "def component_save_data ():\n",
    "    X = np.array ([100, 90, 10])\n",
    "    return X\n",
    "        \n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_save_load (component_save_data):\n",
    "    \n",
    "    X = component_save_data\n",
    "\n",
    "    path_results = 'component_loading_saving'\n",
    "    remove_previous_results (path_results=path_results)\n",
    "\n",
    "    tr1 = Transform1 (path_results=path_results)\n",
    "    tr1.fit (X)\n",
    "    result = tr1.apply (X)\n",
    "\n",
    "    tr2 = Transform1 (path_results=path_results)\n",
    "    tr2.load_estimator()\n",
    "    assert tr2.estimator.sum == tr1.estimator.sum\n",
    "\n",
    "    result2 = tr2.data_io.load_result ()\n",
    "    assert (result2 == sum(X)+X).all()\n",
    "\n",
    "    import os\n",
    "\n",
    "    assert os.listdir (f'{path_results}/whole')==['transform_without_fit_apply2_result.pk']\n",
    "    assert os.listdir (f'{path_results}/models')==['transform_without_fit_apply2_estimator.pk']\n",
    "\n",
    "    result_b = tr1.apply (X*2, split='test')\n",
    "    result2b = tr2.data_io.load_result (split='test')\n",
    "    assert (result_b==result2b).all()\n",
    "    assert os.listdir (f'{path_results}/test')==['transform_without_fit_apply2_result.pk']\n",
    "\n",
    "    result2b = tr2.data_io.load_result ()\n",
    "    assert (result_b!=result2b).all()\n",
    "\n",
    "    remove_previous_results (path_results=path_results)\n",
    "\n",
    "\n",
    "    # Test that no saving is done if save=False\n",
    "    tr1 = Transform1 (path_results=path_results, save=False)\n",
    "    tr1.fit (X)\n",
    "    result = tr1.apply (X)\n",
    "    assert not os.path.exists(path_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272d5f76-3815-48f1-b758-357d12173ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_save_load\n",
      "running _fit\n",
      "running _apply\n",
      "running _apply\n",
      "running _fit\n",
      "running _apply\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_save_load, component_save_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b1482-3304-4ccc-904a-e2548a09fa4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### running fit / apply depending on whether estimator / result exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6978b4e-2fb3-4006-9898-ad817895d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_run_depend_on_existence ():\n",
    "\n",
    "    path_results = 'component_run_existence'\n",
    "    remove_previous_results (path_results=path_results)\n",
    "\n",
    "    tr1 = TransformWithFitApply2 (path_results=path_results, error_if_fit_func=True, error_if_apply_func=True)\n",
    "    X = np.array ([100, 90, 10])\n",
    "    result = tr1.fit_apply (X)\n",
    "    assert (result==(X+200)).all()\n",
    "\n",
    "    assert os.listdir(f'{path_results}/models')==['transform_with_fit_apply2_estimator.pk']\n",
    "\n",
    "    assert os.listdir(f'{path_results}/whole')==['transform_with_fit_apply2_result.pk']\n",
    "\n",
    "    tr1 = TransformWithFitApply2 (path_results=path_results, error_if_fit_func=True, error_if_apply_func=True,\n",
    "                                  error_if_fit_func_apply=True)\n",
    "    result2 = tr1.fit_apply (X)\n",
    "    assert (result2==(X+200)).all()\n",
    "\n",
    "    assert tr1.estimator=={'sum': 200}\n",
    "\n",
    "    tr2 = TransformWithFitApply2 (path_results=path_results, error_if_fit_func=True, error_if_apply_func=True,\n",
    "                                  error_if_fit_apply_func=True)\n",
    "    result3 = tr2.apply (X)\n",
    "\n",
    "    assert (result3==(X+200)).all()\n",
    "    assert tr2.estimator=={'sum': None}\n",
    "\n",
    "    os.remove (f'{path_results}/models/transform_with_fit_apply2_estimator.pk')\n",
    "\n",
    "    with pytest.raises (RuntimeError):\n",
    "        result3 = tr2.fit_apply (X)\n",
    "\n",
    "    tr2.error_if_fit_apply_func = False\n",
    "    result4 = tr2.fit_apply (X)\n",
    "    assert tr2.estimator=={'sum': 200}\n",
    "    assert (result4==(X+200)).all()\n",
    "\n",
    "    os.remove (f'{path_results}/whole/transform_with_fit_apply2_result.pk')\n",
    "\n",
    "    tr3 = TransformWithFitApply2 (path_results=path_results, error_if_fit_func=True, error_if_apply_func=True,\n",
    "                                  error_if_fit_apply_func=True)\n",
    "    with pytest.raises (RuntimeError):\n",
    "        _ = tr3.apply (X)\n",
    "    with pytest.raises (RuntimeError):\n",
    "        _ = tr3.fit_apply (X)\n",
    "    tr3.error_if_fit_apply_func = False\n",
    "    result5 = tr3.fit_apply (X)\n",
    "    assert tr3.estimator=={'sum': 200}\n",
    "    assert (result5==(X+200)).all()\n",
    "\n",
    "    assert os.listdir (f'{path_results}/whole')==['transform_with_fit_apply2_result.pk']\n",
    "    assert os.listdir (f'{path_results}/models')==['transform_with_fit_apply2_estimator.pk']\n",
    "\n",
    "    remove_previous_results (path_results)\n",
    "\n",
    "    tr4 = TransformWithFitApply2 (path_results=path_results, error_if_fit_func=False, error_if_apply_func=False,\n",
    "                                  error_if_fit_apply_func=True)\n",
    "    result6 = tr4.fit(X).apply (X)\n",
    "    assert tr4.estimator=={'sum': 200}\n",
    "    assert (result6==(X+200)).all()\n",
    "    assert os.listdir (f'{path_results}/whole')==['transform_with_fit_apply2_result.pk']\n",
    "    assert os.listdir (f'{path_results}/models')==['transform_with_fit_apply2_estimator.pk']\n",
    "\n",
    "    remove_previous_results (path_results)\n",
    "\n",
    "    tr5 = TransformWithoutFitApply2 (path_results=path_results, error_if_fit_func=False, error_if_apply_func=False)\n",
    "    result7 = tr5.fit(X).apply (X)\n",
    "    assert tr5.estimator=={'sum': 200}\n",
    "    assert (result7==(X+200)).all()\n",
    "    assert os.listdir (f'{path_results}/whole')==['transform_without_fit_apply2_result.pk']\n",
    "    assert os.listdir (f'{path_results}/models')==['transform_without_fit_apply2_estimator.pk']\n",
    "\n",
    "    remove_previous_results (path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf3ab200-d3fa-47c7-b46e-9558430b044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_run_depend_on_existence\n",
      "running _fit_apply\n",
      "running _fit_apply\n",
      "running _fit_apply\n",
      "running _fit\n",
      "running _apply\n",
      "running _fit\n",
      "running _apply\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_run_depend_on_existence, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1bfa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cc9a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_logger (component_save_data):\n",
    "    \n",
    "    X = component_save_data\n",
    "    \n",
    "    tr1 = Transform1 (verbose=0)\n",
    "    tr1.fit (X)\n",
    "    result = tr1.apply (X)\n",
    "\n",
    "    tr1 = Transform1 (verbose=1)\n",
    "    tr1.fit (X)\n",
    "    result = tr1.apply (X)\n",
    "\n",
    "    tr1 = Transform1 (verbose=2)\n",
    "    tr1.fit (X)\n",
    "    result = tr1.apply (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dca9bf9-d3e9-4ae1-bbf7-e27cedf1ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_logger\n",
      "running _fit\n",
      "running _apply\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fitting transform_without_fit_apply2 (using whole data)\n",
      "fitting transform_without_fit_apply2 (using whole data)\n",
      "applying transform_without_fit_apply2 (on whole data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _fit\n",
      "running _apply\n",
      "running _fit\n",
      "running _apply\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_logger, component_save_data, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d718c74-10c1-4558-a545-dc1f3e95ade5",
   "metadata": {},
   "source": [
    "### Passing data_converter and data_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "504679f1-d66c-42e0-bbaa-aa9a6945c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_data_converter ():\n",
    "    class MyTransform (Component):\n",
    "        def __init__ (self, **kwargs):\n",
    "            super().__init__ (data_converter='PandasConverter',\n",
    "                              **kwargs)\n",
    "        def _apply (self, x):\n",
    "            return x*2\n",
    "\n",
    "    my_transform = MyTransform (separate_labels=False)\n",
    "    assert my_transform.data_converter.separate_labels is False\n",
    "    assert type(my_transform.data_converter) is PandasConverter\n",
    "\n",
    "    # example where data-converter uses class-specific parameters\n",
    "    config = dict(separate_labels=False, MyTransform=dict(separate_labels=True))\n",
    "    my_transform = MyTransform (**config)\n",
    "    assert my_transform.data_converter.separate_labels is True\n",
    "    assert config['separate_labels'] is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407b3294-4329-4cf7-8320-98033c3ae378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_data_converter\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_data_converter, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f785a9e-be39-45f4-9e40-eebafeeab2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example using data_io\n",
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_data_io ():\n",
    "    import pandas as pd\n",
    "    from block_types.utils.utils import remove_previous_results\n",
    "\n",
    "    path_results = 'test_data_io'\n",
    "    remove_previous_results (path_results=path_results)\n",
    "\n",
    "    class MyTransform (Component):\n",
    "        def __init__ (self, **kwargs):\n",
    "            super().__init__ (result_io='pandas',\n",
    "                              **kwargs)\n",
    "        def _fit (self, X, y=None):\n",
    "            self.estimator = Bunch(sum=100)\n",
    "\n",
    "        def _apply (self, x):\n",
    "            return pd.DataFrame ([[1,2],[3,4]], columns=['a','b'])\n",
    "\n",
    "    my_transform = MyTransform (path_results='do_not_use', MyTransform=dict(path_results=path_results))\n",
    "    my_transform.fit (1)\n",
    "    assert os.listdir (f'{path_results}/models')==['my_transform_estimator.pk']\n",
    "\n",
    "    df1 = my_transform.apply (1)\n",
    "    assert os.listdir (f'{path_results}/whole')==['my_transform_result.parquet']\n",
    "\n",
    "    assert not os.path.exists ('do_not_use')\n",
    "\n",
    "    del my_transform\n",
    "    my_transform = MyTransform (path_results='do_not_use', MyTransform=dict(path_results=path_results))\n",
    "    #assert my_transform.estimator is None\n",
    "    my_transform.load_estimator()\n",
    "    assert my_transform.estimator == Bunch(sum=100)\n",
    "\n",
    "    df2 = my_transform.load_result ()\n",
    "    pd.testing.assert_frame_equal (df1, df2)\n",
    "\n",
    "    remove_previous_results (path_results=path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "807fe2a4-912e-4583-b6ca-14bb22631c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_data_io\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_data_io, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723b1ee-662b-4cc9-8a53-877eeebb3fdd",
   "metadata": {},
   "source": [
    "### assert_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7de59b8-b4ed-44e2-9457-c5a2de2dc3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_component_equal ():\n",
    "    path_results = 'assert_equal'\n",
    "    remove_previous_results (path_results=path_results)\n",
    "\n",
    "    class MyTransform (Component):\n",
    "        def __init__ (self, noise=1e-10, different = False, **kwargs):\n",
    "            super().__init__ (result_io='pandas',\n",
    "                              **kwargs)\n",
    "        def _fit (self, X, y=None):\n",
    "            self.estimator = Bunch(sum=100)\n",
    "\n",
    "        def _generate_noise (self):\n",
    "            while True:\n",
    "                noise = np.random.rand() * self.noise\n",
    "                if noise > self.noise/10:\n",
    "                    break\n",
    "            return noise\n",
    "\n",
    "        def _apply (self, x):\n",
    "            df = pd.DataFrame ([[1.0,2.0],[3.0,4.0]], columns=['a','b']) + self._generate_noise ()\n",
    "            if self.different:\n",
    "                df = df+10\n",
    "            x = np.array([[10.0,20.0],[30.0,40.0]]) + self._generate_noise ()\n",
    "            result = dict(sequence=[[1.0,2.0], x+1, dict(vector=x, data=df)],\n",
    "                          array=x+10)\n",
    "            return result\n",
    "\n",
    "    tr = MyTransform ()\n",
    "    tr2= MyTransform ()\n",
    "    tr.assert_equal (tr(1), tr2(1), significant_digits=7)\n",
    "\n",
    "    import pytest\n",
    "    with pytest.raises (AssertionError):\n",
    "        tr = MyTransform (noise=1e-3, verbose=1)\n",
    "        tr2= MyTransform (noise=1e-3, verbose=1)\n",
    "        tr.assert_equal (tr(1), tr2(1), significant_digits=7)\n",
    "\n",
    "    with pytest.raises (AssertionError):\n",
    "        tr = MyTransform (verbose=1, different=True)\n",
    "        tr2= MyTransform (verbose=1)\n",
    "        tr.assert_equal (tr(1), tr2(1))\n",
    "\n",
    "    result = tr.assert_equal (tr(1), tr2(1), raise_error=False)\n",
    "    assert result is False\n",
    "    remove_previous_results (path_results=path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebc49754-774c-46df-8381-617c2b42e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_component_equal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comparing results for my_transform\n",
      "comparing results for my_transform\n",
      "comparing results for my_transform\n",
      "Component my_transform => results are different: [sequence] [2] [data] DataFrame.iloc[:, 0] (column name=\"a\") are different\n",
      "\n",
      "DataFrame.iloc[:, 0] (column name=\"a\") values are different (100.0 %)\n",
      "[index]: [0, 1]\n",
      "[left]:  [11.000000000055376, 13.000000000055376]\n",
      "[right]: [1.0000000000894484, 3.0000000000894484]\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_component_equal, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2b23d-6d7c-4c55-b92c-f02ef5964c74",
   "metadata": {},
   "source": [
    "### set_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b99a78a-e524-400c-88ac-00e7a9b176f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_set_paths ():\n",
    "    def assert_paths (x, path_results, path_models):\n",
    "        base = os.path.abspath('.')\n",
    "        assert x.path_results==Path(f'{base}/{path_results}')\n",
    "        assert x.data_io.path_results==Path(f'{base}/{path_results}')\n",
    "        assert x.path_models==Path(f'{base}/{path_models}')\n",
    "        assert x.data_io.path_models==Path(f'{base}/{path_models}')\n",
    "\n",
    "    path_results = 'test_set_paths_1'\n",
    "    path_models = 'test_set_paths_1'\n",
    "    tr = Component (path_results=path_results)\n",
    "    assert_paths (tr, path_results, path_models)\n",
    "    path_results = 'test_set_paths_2'\n",
    "    tr.data_io.set_path_results (path_results)\n",
    "    assert_paths (tr, path_results, path_models)\n",
    "    path_models='test_set_paths_models_1'\n",
    "    tr.data_io.set_path_models (path_models)\n",
    "    assert_paths (tr, path_results, path_models)\n",
    "\n",
    "    path_results = 'test_set_paths_a'\n",
    "    path_models = 'test_set_paths_models_a'\n",
    "    tr = Component (path_results=path_results, path_models=path_models)\n",
    "    assert_paths (tr, path_results, path_models)\n",
    "\n",
    "    path_results = 'test_set_paths_b'\n",
    "    tr.data_io.set_path_results (path_results)\n",
    "    assert_paths (tr, path_results, path_models)\n",
    "\n",
    "    path_models = 'test_set_paths_models_b'\n",
    "    tr.data_io.set_path_models (path_models)\n",
    "    assert_paths (tr, path_results, path_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fe6eee4-265b-48e1-a7e6-2ee1b6d844d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_set_paths\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_set_paths, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34515c78-5c95-490e-8734-ff206312304b",
   "metadata": {},
   "source": [
    "### determine fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d966393b-f988-4414-be20-51557f0783e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "from block_types.utils.dummies import DummyEstimator\n",
    "\n",
    "class TransformWithoutFit (Component):\n",
    "    def __init__ (self, factor=2, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "    def _apply (self, X):\n",
    "        return X * self.factor\n",
    "    \n",
    "class TransformWithFitApplyOnly (Component):\n",
    "    def __init__ (self, **kwargs):\n",
    "        super().__init__ (**kwargs)\n",
    "    def _apply (self, X):\n",
    "        return X + self.sum\n",
    "    def _fit_apply (self, X, y=None):\n",
    "        self.sum = X.sum(axis=0)*10\n",
    "        return X + self.sum\n",
    "    \n",
    "def test_determine_fit_function ():\n",
    "    # example when there is _fit implemented\n",
    "    component = TransformWithoutFitApply ()\n",
    "    X = np.array ([1,2,3])\n",
    "    component.fit (X)\n",
    "    X2 = np.array ([10,20,30])\n",
    "    r = component (X2)\n",
    "    assert (r == (X.sum() + X2)).all()\n",
    "    assert component.is_model\n",
    "\n",
    "    # example when there is estimator\n",
    "    component = Component (DummyEstimator (2))\n",
    "    X = np.array ([1,2,3])\n",
    "    component.fit (X)\n",
    "    assert component.estimator.sum == 6\n",
    "    X2 = np.array ([10,20,30])\n",
    "    r = component (X2)\n",
    "    assert (r == (X.sum() + X2*2)).all()\n",
    "    assert component.is_model\n",
    "\n",
    "    # example when there is no _fit implemented, and there is no estimator\n",
    "    component = TransformWithoutFit ()\n",
    "    X = np.array ([1,2,3])\n",
    "    component.fit (X)\n",
    "    X2 = np.array ([10,20,30])\n",
    "    r = component (X2)\n",
    "    assert (r == (X2*2)).all()\n",
    "    assert not component.is_model\n",
    "    assert component._fit == component._fit_\n",
    "\n",
    "    # example when there is only fit_apply implemented\n",
    "    component = TransformWithFitApplyOnly ()\n",
    "    X2 = np.array ([10,20,30])\n",
    "    r = component.fit_apply (X2)\n",
    "    assert (r == (X2 + X2.sum(axis=0)*10)).all()\n",
    "    assert component.is_model\n",
    "    assert component._fit == component._fit_\n",
    "    \n",
    "def test_use_fit_from_loaded_estimator ():\n",
    "    path_models = 'test_use_fit_from_loaded_estimator'\n",
    "    component = Component (DummyEstimator (2), path_models=path_models)\n",
    "    X = np.array ([1,2,3])\n",
    "    component.fit (X)\n",
    "    assert (Path (path_models) / 'models').exists()\n",
    "    del component\n",
    "    \n",
    "    estimator1 = DummyEstimator (2)\n",
    "    print (estimator1)\n",
    "    component = Component (estimator1, path_models=path_models)\n",
    "    print ('before loading')\n",
    "    print (component.estimator)\n",
    "    print (component._fit)\n",
    "    print (component.result_func)\n",
    "    \n",
    "    component.load_estimator ()\n",
    "    print ('after loading')\n",
    "    print (component.estimator)\n",
    "    print (component._fit)\n",
    "    print (component.result_func)\n",
    "    \n",
    "    assert component.estimator.sum == 6\n",
    "    assert component.is_model\n",
    "    \n",
    "    X2 = np.array ([10,20,30])\n",
    "    r = component (X2)\n",
    "    assert (r == (X.sum() + X2*2)).all()\n",
    "    \n",
    "    remove_previous_results (path_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e363d63-48bd-4f0d-88d7-5e41ac2d174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_determine_fit_function\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_determine_fit_function, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d7260f6-54fd-4220-9cc0-b71228afd0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_use_fit_from_loaded_estimator\n",
      "<block_types.utils.dummies.DummyEstimator object at 0x7fbfddfe9910>\n",
      "before loading\n",
      "<block_types.utils.dummies.DummyEstimator object at 0x7fbfddfe9910>\n",
      "<bound method DummyEstimator.fit of <block_types.utils.dummies.DummyEstimator object at 0x7fbfddfe9910>>\n",
      "<bound method DummyEstimator.transform of <block_types.utils.dummies.DummyEstimator object at 0x7fbfddfe9910>>\n",
      "after loading\n",
      "<block_types.utils.dummies.DummyEstimator object at 0x7fbfde009dd0>\n",
      "<bound method DummyEstimator.fit of <block_types.utils.dummies.DummyEstimator object at 0x7fbfde009dd0>>\n",
      "<bound method DummyEstimator.transform of <block_types.utils.dummies.DummyEstimator object at 0x7fbfde009dd0>>\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_use_fit_from_loaded_estimator, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54f0a6-b8c4-464e-a08b-19c8347ee41e",
   "metadata": {},
   "source": [
    "### Use direct methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a0f6cba-f69e-4499-989f-594e1b88d130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types   \n",
    "from block_types.utils.dummies import Multiply10direct, Max10direct\n",
    "def test_direct_methods ():\n",
    "    # input\n",
    "    X = np.array ([1,2,3])\n",
    "    \n",
    "    # example where we do not use direct methods\n",
    "    component = Max10direct (verbose=2)\n",
    "    component.fit (X)\n",
    "    r = component (X)\n",
    "    assert (r==X*10+X.max()).all()\n",
    "    \n",
    "    component = Max10direct (verbose=2, error_if_apply=True)\n",
    "    component.fit (X)\n",
    "    with pytest.raises (RuntimeError):\n",
    "        r = component (X)\n",
    "    #assert component.fitted\n",
    "    #assert component.applied\n",
    "    \n",
    "    # example where we use direct methods\n",
    "    component = Max10direct (direct_apply=True, verbose=2, error_if_apply=True)\n",
    "    component.logger.info (f'{\"-\"*100}')\n",
    "    component.logger.info (f'direct_apply={component.direct_apply}, direct_fit={component.direct_fit}, direct_fit_apply={component.direct_fit_apply}\\n')\n",
    "    component.fit (X)\n",
    "    r = component (X)\n",
    "    assert (r==X*10+X.max()).all()\n",
    "    #assert component.fitted\n",
    "    #assert not component.applied\n",
    "    \n",
    "    component = Max10direct (direct_fit=True, verbose=2, error_if_fit=True)\n",
    "    component.logger.info (f'{\"-\"*100}')\n",
    "    component.logger.info (f'direct_apply={component.direct_apply}, direct_fit={component.direct_fit}, direct_fit_apply={component.direct_fit_apply}\\n')\n",
    "    component.fit (X)\n",
    "    r = component.apply (X)\n",
    "    assert (r==X*10+X.max()).all()\n",
    "    #assert not component.fitted\n",
    "    #assert component.applied\n",
    "    \n",
    "    component = Max10direct (direct_apply=True, direct_fit=True, verbose=2, error_if_apply=True, \n",
    "                             error_if_fit=True)\n",
    "    component.logger.info (f'{\"-\"*100}')\n",
    "    component.logger.info (f'direct_apply={component.direct_apply}, direct_fit={component.direct_fit}, direct_fit_apply={component.direct_fit_apply}\\n')\n",
    "    component.fit (X)\n",
    "    r = component.transform (X)\n",
    "    assert (r==X*10+X.max()).all()\n",
    "    #assert not component.fitted\n",
    "    #assert not component.applied\n",
    "        \n",
    "    # example when there is no _fit implemented and we call fit_apply\n",
    "    component = Multiply10direct (verbose=2, error_if_fit=True)\n",
    "    component.logger.info (f'{\"-\"*100}')\n",
    "    component.logger.info (f'direct_apply={component.direct_apply}, direct_fit={component.direct_fit}, direct_fit_apply={component.direct_fit_apply}\\n')\n",
    "    r = component.fit_apply (X)\n",
    "    assert (r==X*10).all()\n",
    "    #assert not component.is_model\n",
    "    assert component.fit == component._fit_\n",
    "    #assert component.fit_apply == component.apply\n",
    "    r2 = component.fit (X).apply (X)\n",
    "    assert (r==X*10).all()\n",
    "\n",
    "    # example when there is no _fit implemented and we want a direct apply call\n",
    "    component = Multiply10direct (verbose=2, direct_apply=True, error_if_apply=True, error_if_fit=True)\n",
    "    component.logger.info (f'{\"-\"*100}')\n",
    "    component.logger.info (f'direct_apply={component.direct_apply}, direct_fit={component.direct_fit}, direct_fit_apply={component.direct_fit_apply}\\n')\n",
    "    r = component.fit_apply (X)\n",
    "    assert (r==X*10).all()\n",
    "    assert not component.is_model\n",
    "    assert component.fit == component._fit_\n",
    "    #assert component.fit_apply == component._apply\n",
    "    #assert component.fit_transform == component._apply\n",
    "    #assert not component.applied\n",
    "    r2 = component.fit (X).apply (X)\n",
    "    assert (r==X*10).all()\n",
    "    #assert not component.applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66cf3201-edd4-4aa3-9507-de7b74d65c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fitting max10direct (using whole data)\n",
      "applying max10direct (on whole data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_direct_methods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fitting max10direct (using whole data)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "direct_apply=True, direct_fit=False, direct_fit_apply=False\n",
      "\n",
      "fitting max10direct (using whole data)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "direct_apply=False, direct_fit=True, direct_fit_apply=False\n",
      "\n",
      "applying max10direct (on whole data)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "direct_apply=True, direct_fit=True, direct_fit_apply=False\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "direct_apply=False, direct_fit=False, direct_fit_apply=False\n",
      "\n",
      "applying multiply10direct (on whole data)\n",
      "applying multiply10direct (on whole data)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "direct_apply=True, direct_fit=False, direct_fit_apply=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_direct_methods, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8cc0a-2906-4c52-8943-1bc98c7d853b",
   "metadata": {},
   "source": [
    "### Pass apply function by parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b95a11c1-7430-4843-bff7-5efae7929264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types   \n",
    "def test_pass_apply ():\n",
    "    component = Component (apply=lambda x: x*10, verbose=2, direct_apply=True, error_if_apply=True)\n",
    "    X = np.array ([1,2,3])\n",
    "    r = component (X)\n",
    "    assert (r==X*10).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa65f938-246c-4c71-8077-e5042ac3d3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_pass_apply\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_pass_apply, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf7f53-f411-4ba4-a2d1-3b46b22a7230",
   "metadata": {},
   "source": [
    "### Get DataIO specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b3a99a0-478e-4cef-963a-7812ccdd2ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types   \n",
    "def test_get_specific_data_io_parameters_for_component ():\n",
    "    component = Component (tag='data', x=3, par=[1,2], path_results='hello', path_results_data='world', \n",
    "                           other='yes', load_result_data = False, save_model_data=True)\n",
    "    check_last_part(component.path_results, 'world')\n",
    "    assert component.data_io.load_result_flag == False\n",
    "    assert component.data_io.save_model_flag == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5717106d-2ce3-42f5-9e45-9f4f9eafdab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_get_specific_data_io_parameters_for_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_get_specific_data_io_parameters_for_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0a9c2-8809-4da4-a9ff-bb409d8d4475",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### get_specific_data_io_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43bbaf5b-6ce9-4ee6-8a79-11168c12c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "def test_get_specific_data_io_parameters ():\n",
    "    component = Component ()\n",
    "    config = component.get_specific_data_io_parameters (\n",
    "        'data', **dict(x=3, par=[1,2], path_results='hello', path_results_data='world', other='yes', \n",
    "                       load_result_data = True))\n",
    "    assert config == dict (path_results='world', load_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c8f158f-3987-4567-8184-ea769c54a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_get_specific_data_io_parameters\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_get_specific_data_io_parameters, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6180ed01-9b69-43f5-b399-d6ce1b8d5c67",
   "metadata": {},
   "source": [
    "### StandardConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90ac8838-abd5-4af9-89f6-665953c56f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types   \n",
    "from block_types.utils.dummies import Min10direct, SumXY\n",
    "\n",
    "def test_standard_converter_in_component ():\n",
    "    component = Min10direct (data_converter='StandardConverter')\n",
    "    \n",
    "    X, y = np.array([1,2,3]), np.array([0,1,0])\n",
    "\n",
    "    Xr = component.fit_apply (X, y)\n",
    "    assert (Xr==X*10+X.min()).all()\n",
    "    \n",
    "    Xr, yr = component.fit_apply (X, y, sequential_fit_apply=True)\n",
    "    assert (Xr==X*10+X.min()).all()\n",
    "    assert (yr==y).all()\n",
    "    \n",
    "    component = SumXY (data_converter='StandardConverter')\n",
    "        \n",
    "    Xr = component.fit_apply ((X,X*2), y=None)\n",
    "    assert (Xr==X+X*2).all()\n",
    "    \n",
    "    Xr = component.fit_apply ((X,X*2), y=None, sequential_fit_apply=True)\n",
    "    assert (Xr==X+X*2).all()\n",
    "    \n",
    "    #Xr, yr = component.fit_apply ((X,X*2), y, sequential_fit_apply=True)\n",
    "    Xr, yr = component.fit_apply ((X,X*2), y, sequential_fit_apply=True)\n",
    "    assert (Xr==X+X*2).all()\n",
    "    assert (yr==y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98523f0f-b886-42c1-a937-8e5693a55ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_standard_converter_in_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_standard_converter_in_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e108f-c549-45ed-84c1-05f58a0e93f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SamplingComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "060c19d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class SamplingComponent (Component):\n",
    "    \"\"\"\n",
    "    Component that makes use of labels in transform method.\n",
    "    \n",
    "    When calling the transform method, one of the columns of the received data \n",
    "    is assumed to contain the ground-truth labels. This allows the transform \n",
    "    method to modify the number of observations, changing the number of rows in \n",
    "    the data and in the labels. See `PandasConverter` class in \n",
    "    `block_types.core.data_conversion`.\n",
    "    \"\"\"\n",
    "    def __init__ (self, estimator=None, transform_uses_labels=True, **kwargs):\n",
    "\n",
    "        # the SamplingComponent over-rides the following parameters:\n",
    "        super().__init__ (estimator=estimator, transform_uses_labels=transform_uses_labels,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c41d5-1edd-42f3-87f6-e11e7189ce18",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5716bec2-0b85-481f-b2c7-bb2962038a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_sampling_component ():\n",
    "    c = SamplingComponent (data_converter='DataConverter')\n",
    "    assert c.transform_uses_labels\n",
    "    assert not hasattr(c.data_converter,'transform_uses_labels')\n",
    "    c = SamplingComponent (data_converter='PandasConverter')\n",
    "    assert c.data_converter.transform_uses_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ba1a044-a0e5-4ea8-9dcc-4deeec2f8b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_sampling_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_sampling_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f2938-5c08-4481-9d5b-734759ef5c18",
   "metadata": {},
   "source": [
    "## SklearnComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7aed9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SklearnComponent (Component):\n",
    "    \"\"\"\n",
    "    Component that saves estimator parameters in pickle format.\n",
    "    \n",
    "    Convenience subclass used when the results can be saved in \n",
    "    pickle format. See `SklearnIO` class in `core.utils`.\n",
    "    \"\"\"\n",
    "    def __init__ (self, estimator=None, data_io='SklearnIO', transform_uses_labels=False, \n",
    "                  **kwargs):\n",
    "\n",
    "        super().__init__ (estimator=estimator, data_io=data_io, transform_uses_labels=False,\n",
    "                          **kwargs)\n",
    "\n",
    "# alias\n",
    "PickleSaverComponent = SklearnComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17903e87-a5eb-4b03-82af-edf9e82eb3ab",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5869eb68-efc9-42b0-a171-da435c9a6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_sklearn_component ():\n",
    "    c = SklearnComponent ()\n",
    "    assert c.data_io.fitting_load_func==joblib.load\n",
    "    assert c.data_io.result_save_func==joblib.dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ef9a6ef-0eac-452a-991f-c8b7b54ffc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_sklearn_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_sklearn_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2bcbbc-dd88-4625-85a6-af08b88afb6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NoSaverComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69747f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NoSaverComponent (Component):\n",
    "    \"\"\"Component that does not save any data.\"\"\"\n",
    "    def __init__ (self, estimator=None, data_io='NoSaverIO', **kwargs):\n",
    "        \n",
    "        super().__init__ (estimator=estimator, data_io=data_io, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122357b-c551-480a-8702-32139c21f5d3",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5df8e99-3665-4fa3-8474-213a31e8ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_no_saver_component ():\n",
    "    c = NoSaverComponent ()\n",
    "    assert c.data_io.__class__.__name__ == 'NoSaverIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f3871f1-707f-42fd-a2b1-4ff1e9e88237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_no_saver_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_no_saver_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a407a7-51fb-4313-a5bd-85c2bedc2ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OneClassSklearnComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f88a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OneClassSklearnComponent (SklearnComponent):\n",
    "    \"\"\"Component that uses only normal data (labelled with 0) for fitting parameters.\"\"\"\n",
    "    def __init__ (self, estimator=None, **kwargs):\n",
    "        super().__init__ (estimator=estimator, **kwargs)\n",
    "\n",
    "    def _fit (self, X, y=None):\n",
    "        assert y is not None, 'y must be provided in OneClassSklearnComponent class'\n",
    "        X = X[y==0]\n",
    "\n",
    "        assert self.estimator is not None, 'estimator must be provided in OneClassSklearnComponent class'\n",
    "        self.estimator.fit (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4cf02-1b41-4b08-b23a-5fc4bcd86dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63c96874-33d5-404a-83a5-1a04332d3fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "\n",
    "def get_data_for_one_class ():\n",
    "    data = np.r_[np.ones ((5,2)), 2*np.ones((5,2))]\n",
    "    y = np.r_[np.ones ((5,)), np.zeros((5,))]\n",
    "    return data, y\n",
    "\n",
    "def test_one_class_sklearn_component ():\n",
    "    path_results = 'one_class_sklearn_component'\n",
    "    remove_previous_results (path_results=path_results)\n",
    "    \n",
    "    data, y = get_data_for_one_class ()\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    result1 = OneClassSklearnComponent (MinMaxScaler()).fit(data,y).transform (data)\n",
    "    result2 = MinMaxScaler().fit(data[y==0]).transform (data)\n",
    "    assert (result1==result2).all().all()\n",
    "    \n",
    "    remove_previous_results (path_results=path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42b136ba-535a-4a60-979e-7043cc6c471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_one_class_sklearn_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_one_class_sklearn_component, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c105e-c015-4904-ac2a-3c53e9ff4468",
   "metadata": {},
   "source": [
    "## PandasComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4778b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PandasComponent (Component):\n",
    "    \"\"\"\n",
    "    Component that preserves the DataFrame format for incoming data and results.\n",
    "    \n",
    "    This component also writes results in parquet format, by default.\n",
    "    See `PandasConverter` in `core.data_conversion` for details on the data \n",
    "    conversion performed.\n",
    "    \"\"\"\n",
    "    def __init__ (self, estimator=None, data_converter='PandasConverter', data_io='PandasIO',\n",
    "                  **kwargs):\n",
    "        super().__init__ (estimator=estimator, data_converter=data_converter, data_io=data_io,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5674eb-ff06-4e66-8ed4-ae0b90cd8ce2",
   "metadata": {},
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcbd7ab4-15e2-429c-9e87-50e45659fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_block_types\n",
    "#@pytest.mark.reference_fails\n",
    "def test_pandas_component ():\n",
    "    c = PandasComponent ()\n",
    "    assert c.data_converter.__class__.__name__ == 'PandasConverter'\n",
    "    assert c.data_io.__class__.__name__ == 'PandasIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e24133fe-457b-42f0-b80f-ef027d1f19db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_pandas_component\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_pandas_component, tag='dummy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test_block_types)",
   "language": "python",
   "name": "test_block_types"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
