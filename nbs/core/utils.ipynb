{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# default_exp core.utils\n",
    "from nbdev.showdoc import *\n",
    "from dsblocks.utils.nbdev_utils import nbdev_setup, TestRunner\n",
    "\n",
    "nbdev_setup ()\n",
    "tst = TestRunner (targets=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb4791",
   "metadata": {},
   "source": [
    "# Component's helpers\n",
    "\n",
    "> Helper classes used for constructing the Component class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff223084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "import re\n",
    "from functools import partialmethod\n",
    "import time\n",
    "import pickle\n",
    "from IPython.display import display\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import joblib\n",
    "import copy\n",
    "\n",
    "try:\n",
    "    from graphviz import *\n",
    "    imported_graphviz = True\n",
    "except:\n",
    "    imported_graphviz = False\n",
    "\n",
    "# dsblocks\n",
    "from dsblocks.core.data_conversion import PandasConverter\n",
    "from dsblocks.config import bt_defaults as dflt\n",
    "from dsblocks.utils.utils import set_logger, get_logging_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6afaf4-f6bd-4920-a80d-e68d43dd0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tests\n",
    "import pytest \n",
    "from dsblocks.core.components import Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd139fe4",
   "metadata": {},
   "source": [
    "## Functions for loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_csv (df, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame `df` to csv file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.dump, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    df.to_csv (path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_parquet (df, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame `df` to parquet file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.dump, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    df.to_parquet (path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_multi_index_parquet (df, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame `df` to multi-index parquet file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.dump, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6527a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_keras_model (model, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Save keras `model` to file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.dump, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    model.save (path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3772694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_csv_gz (df, path, **kwargs):\n",
    "    \"\"\"\n",
    "    Save DataFrame `df` to csv.gz file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.dump, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    df.to_csv (path, compression='gzip', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57953624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv (path, **kwargs):\n",
    "    \"\"\"\n",
    "    Read DataFrame from csv file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.load, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    return pd.read_csv (path, index_col=0, parse_dates=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_csv_gz (path, **kwargs):\n",
    "    \"\"\"\n",
    "    Read DataFrame from csv.gz file indicated in `path`.\n",
    "    \n",
    "    Convenience function that uses the same signature \n",
    "    as joblib.load, which is needed by `DataIO`. \n",
    "    \"\"\"\n",
    "    return pd.read_csv (path, index_col=0, parse_dates=True, compression='gzip', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_keras_model (path, **kwargs):\n",
    "    import tensorflow.keras as keras\n",
    "    return keras.models.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71f4d0-6dd7-409c-aa0b-cd16aed4c866",
   "metadata": {},
   "source": [
    "## Mapping type to loading / saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a5f92-92b0-497b-a553-0730b5720ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "estimator2io = dict(\n",
    "    keras=dict(fitting_load_func=load_keras_model,\n",
    "               fitting_save_func=save_keras_model,\n",
    "               fitting_file_extension=''),\n",
    "    pickle=dict(fitting_load_func=joblib.load,\n",
    "                 fitting_save_func=joblib.dump,\n",
    "                 fitting_file_extension='.pk'),\n",
    "    default=dict(fitting_load_func=joblib.load,\n",
    "                 fitting_save_func=joblib.dump,\n",
    "                 fitting_file_extension='.pk'))\n",
    "\n",
    "result2io = dict(\n",
    "    pandas=dict(result_load_func=pd.read_parquet,\n",
    "                result_save_func=save_multi_index_parquet,\n",
    "                result_file_extension='.parquet'),\n",
    "    pickle=dict(result_load_func=joblib.load,\n",
    "                 result_save_func=joblib.dump,\n",
    "                 result_file_extension='.pk'),\n",
    "    default=dict(result_load_func=joblib.load,\n",
    "                 result_save_func=joblib.dump,\n",
    "                 result_file_extension='.pk')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89417a-cc90-439e-a61e-a3adb603d92c",
   "metadata": {},
   "source": [
    "## DataIO class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30a285a-fd13-40c6-9124-43231d7f8490",
   "metadata": {},
   "source": [
    "### DataIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataIO ():\n",
    "    \n",
    "    specific_params = ['path_results', 'load_result', 'save_result', 'path_models', 'load_model', 'save_model',\n",
    "                       'load', 'save']\n",
    "    \n",
    "    def __init__ (self,\n",
    "                  component=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize common attributes and fields.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        component : Component or None, optional\n",
    "            reference to component that uses the DataIO object. \n",
    "            By storing this reference, the DataIO class behaves very much\n",
    "            like a callback object that keeps the state and internal parameters \n",
    "            of the caller for greater flexibility. \n",
    "            This reference is currently used mainly for accessing the estimator \n",
    "            that is part of the component, since sometimes this estimator is \n",
    "            initialized after the component's construction. The reference is \n",
    "            also used for getting access to the name and class of the \n",
    "            Component,  for determining the names of the files where \n",
    "            results are saved. \n",
    "        path_results : str or Path or None, optional\n",
    "            Path to results folder where estimator's parameters and transformed \n",
    "            data are stored.\n",
    "        fitting_file_name : str or None, optional\n",
    "            Name of the file used for storing the estimator's parameters. \n",
    "            If not provided, the name of the component is used. This name \n",
    "            is usually the name of the class of the component converted to \n",
    "            snake case.\n",
    "        fitting_file_extension : str or None, optional\n",
    "            Extension of the file where the estimator's parameters are \n",
    "            saved. By default, no extension is used.\n",
    "        fitting_load_func : function or None, optional\n",
    "            Function used for loading the stored parameters of the estimator. \n",
    "            If None is given, the parameters are not loaded. This function \n",
    "            must have the following signature:\n",
    "            estimator_parameters = fitting_load_func (path_to_file)\n",
    "        fitting_save_func : function or None, optional\n",
    "            Function used for saving the stored parameters of the estimator. \n",
    "            If None is given, the parameters are not saved. This function \n",
    "            must have the following signature:\n",
    "            fitting_save_func (estimator_parameters, path_to_file)\n",
    "        result_file_extension : str or None, optional\n",
    "            Extension of the file where the transformed data is saved. \n",
    "            By default, no extension is used.\n",
    "        result_file_name : str or None, optional\n",
    "            Name of the file used for storing the transformed data. \n",
    "            If not provided, it constructed based on the name of the component.\n",
    "        result_load_func : function or None, optional\n",
    "            Function used for loading the transformed data. By default, this \n",
    "            function is `pd.read_parquet`. The provided function must have the \n",
    "            following signature:\n",
    "            transformed_data = result_load_func (path_to_file)\n",
    "        result_save_func : function, optional\n",
    "            Function used for saving the transformed data. By default, the \n",
    "            function used is `save_multi_index_parquet` (see above). The \n",
    "            provided function must have the following signature:\n",
    "            result_save_func (transformed_data, path_to_file)\n",
    "        save_splits : dict, optional\n",
    "            Dictionary mapping split names to booleans. Usual split names are 'test', 'validation', 'training'\n",
    "            For each split name, if True the transformed data is saved for that split.\n",
    "        save : bool, optional\n",
    "            If False, neither transformed data nor estimated parameters are saved,\n",
    "            regardless of the other arguments.\n",
    "        load: bool, optional\n",
    "            If False, neither transformed data nor estimated parameters are loaded,\n",
    "            regardless of the other arguments.\n",
    "        \"\"\"\n",
    "\n",
    "        self.component = component\n",
    "        if component is not None:\n",
    "            config = self.component.obtain_config_params (**kwargs)\n",
    "            self._init (**config)\n",
    "            self.setup (component)\n",
    "        else:\n",
    "            self._init (**kwargs)\n",
    "            self._initial_kwargs = kwargs\n",
    "                    \n",
    "    def _init (self, \n",
    "               path_models=None,\n",
    "               fitting_file_name=None,\n",
    "               fitting_file_extension=None,\n",
    "               fitting_load_func=None,\n",
    "               fitting_save_func=None,\n",
    "               estimator_io='pickle',\n",
    "               load_model=True,\n",
    "               save_model=True,\n",
    "               \n",
    "               path_results=dflt.path_results,\n",
    "               result_file_extension=None,\n",
    "               result_file_name=None,\n",
    "               result_load_func=None,\n",
    "               result_save_func=None,\n",
    "               result_io='pickle',\n",
    "               save_splits=dflt.save_splits,\n",
    "               load_result=True,\n",
    "               save_result=True,\n",
    "\n",
    "               load=True,\n",
    "               save=True,\n",
    "               split='whole',\n",
    "               folder='',\n",
    "               stop_propagation=False,\n",
    "               **kwargs):\n",
    "        \n",
    "        self.path_models = path_models\n",
    "        self.path_results = path_results\n",
    "        \n",
    "        # saving / loading estimator parameters\n",
    "        self.fitting_file_name = fitting_file_name\n",
    "        self.fitting_file_extension = fitting_file_extension\n",
    "        self.fitting_load_func = fitting_load_func\n",
    "        self.fitting_save_func = fitting_save_func\n",
    "        self.estimator_io = estimator_io\n",
    "        \n",
    "        # saving / loading transformed data\n",
    "        self.result_file_extension = result_file_extension\n",
    "        self.result_load_func = result_load_func\n",
    "        self.result_save_func = result_save_func\n",
    "        self.result_io = result_io\n",
    "        \n",
    "        # saving / loading transformed training data \n",
    "        self.set_save_splits (save_splits)\n",
    "        \n",
    "        # saving / loading transformed test data \n",
    "        self.result_file_name = result_file_name\n",
    "        \n",
    "        # whether the transformation has been applied to training data (i.e., to be saved in training path)\n",
    "        # or to test data (i.e,. to be saved in test path)\n",
    "        self.split = split\n",
    "        \n",
    "        # global saving and loading\n",
    "        self.set_save (save)\n",
    "        self.set_load (load)\n",
    "        self.set_save_model (save_model)\n",
    "        self.set_load_model (load_model)\n",
    "        self.set_save_result (save_result)\n",
    "        self.set_load_result (load_result)\n",
    "        \n",
    "        self.set_folder (folder)\n",
    "        self.stop_propagation = stop_propagation\n",
    "        \n",
    "    def setup (self, component=None):\n",
    "        \"\"\"\n",
    "        Initialize remaining fields given `component` from which data is saved/loaded.\n",
    "        \n",
    "        We use the name of `component` for inferring the name of the files \n",
    "        where the data is saved / loaded. The reason why we need to do this in a \n",
    "        separate setup method is because the component might be unknown when \n",
    "        constructing the DataIO object in the subclasses below (see for instance \n",
    "        SklearnIO subclass)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.component = component\n",
    "        \n",
    "        if hasattr(self, '_initial_kwargs'):\n",
    "            config = self.component.obtain_config_params (**self._initial_kwargs)\n",
    "            self._init (**config)\n",
    "            del self._initial_kwargs\n",
    "\n",
    "        if self.fitting_file_extension is None:\n",
    "            self.fitting_file_extension = estimator2io[self.estimator_io]['fitting_file_extension']\n",
    "        if self.fitting_load_func is None:\n",
    "            self.fitting_load_func = estimator2io[self.estimator_io]['fitting_load_func']\n",
    "        if self.fitting_save_func is None:\n",
    "            self.fitting_save_func = estimator2io[self.estimator_io]['fitting_save_func']\n",
    "            \n",
    "        if self.result_file_extension is None:\n",
    "            self.result_file_extension = result2io[self.result_io]['result_file_extension']\n",
    "        if self.result_load_func is None:\n",
    "            self.result_load_func = result2io[self.result_io]['result_load_func']\n",
    "        if self.result_save_func is None:\n",
    "            self.result_save_func = result2io[self.result_io]['result_save_func']\n",
    "            \n",
    "        # configuration for saving / loading fitted estimator\n",
    "        if self.fitting_file_name is None:\n",
    "            self.set_fitting_file_name (self.component.name)\n",
    "\n",
    "        # configuration for saving / loading result of transforming training data\n",
    "        if self.result_file_name is None:\n",
    "            self.set_result_file_name (self.component.name)\n",
    "        \n",
    "        if self.path_results is not None:\n",
    "            self.path_results = Path(self.path_results).resolve()\n",
    "            \n",
    "        if self.path_models is None:\n",
    "            self.path_models = self.path_results\n",
    "        else:\n",
    "            self.path_models = Path(self.path_models).resolve()\n",
    "    \n",
    "    def get_path_model_file (self, path_models=None, fitting_file_name=None):\n",
    "        path_models = self.path_models if path_models is None else Path(path_models).resolve()\n",
    "        fitting_file_name = self.fitting_file_name if fitting_file_name is None else fitting_file_name\n",
    "        if path_models is not None:\n",
    "            if self.folder != '':\n",
    "                path_model_file = path_models / self.folder / 'models' / fitting_file_name\n",
    "            else:\n",
    "                path_model_file = path_models / 'models' / fitting_file_name\n",
    "        else:\n",
    "            path_model_file = None\n",
    "        return path_model_file\n",
    "    def load_estimator (self, path_models=None, fitting_file_name=None):\n",
    "        \"\"\"Load estimator parameters.\"\"\"        \n",
    "        path_model_file = self.get_path_model_file (path_models=path_models, \n",
    "                                                    fitting_file_name=fitting_file_name)\n",
    "        estimator = self._load (path=path_model_file, \n",
    "                                load_func=self.fitting_load_func)\n",
    "        return estimator\n",
    "    \n",
    "    def load_estimators (self, **kwargs):\n",
    "        all_exist = True\n",
    "        for component in self.component.components:\n",
    "            estimator = component.data_io.load_estimator (**kwargs)\n",
    "            all_exist = all_exist and (estimator is not None)\n",
    "        return all_exist\n",
    "\n",
    "    def save_estimator (self, path_models=None, fitting_file_name=None):\n",
    "        \"\"\"Save estimator parameters.\"\"\"\n",
    "        if self.component.estimator is not None:\n",
    "            path_model_file = self.get_path_model_file (path_models=path_models, \n",
    "                                                        fitting_file_name=fitting_file_name)\n",
    "            self._save (path_model_file, self.fitting_save_func, self.component.estimator)\n",
    "\n",
    "    def get_path_result_file (self, split=None, path_results=None, result_file_name=None):\n",
    "        split = self.split if split is None else split\n",
    "        path_results = self.path_results if path_results is None else Path(path_results).resolve()\n",
    "        result_file_name = self.result_file_name if result_file_name is None else result_file_name\n",
    "        if path_results is not None:\n",
    "            if self.folder != '':\n",
    "                path_result_file = path_results / self.folder / split / result_file_name\n",
    "            else:\n",
    "                path_result_file = path_results / split / result_file_name\n",
    "        else:\n",
    "            path_result_file = None\n",
    "        return path_result_file\n",
    "    \n",
    "    def load_result (self, split=None, path_results=None, result_file_name=None):\n",
    "        \"\"\"\n",
    "        Load transformed data. \n",
    "        \n",
    "        Transformed training data is loaded if self.training_data_flag=True,\n",
    "        otherwise transformed test data is loaded.\n",
    "        \"\"\"\n",
    "        path_result_file = self.get_path_result_file (split=split, path_results=path_results,\n",
    "                                                      result_file_name=result_file_name)\n",
    "        return self._load (path_result_file, self.result_load_func)\n",
    "\n",
    "    def save_result (self, result, split=None, path_results=None, result_file_name=None):\n",
    "        \"\"\"\n",
    "        Save transformed data.\n",
    "        \n",
    "        Transformed training data is saved if self.training_data_flag=True,\n",
    "        otherwise transformed test data is saved.\n",
    "        \"\"\"\n",
    "        path_result_file = self.get_path_result_file (split=split, path_results=path_results,\n",
    "                                                      result_file_name=result_file_name)\n",
    "        self._save (path_result_file, self.result_save_func, \n",
    "                    result)\n",
    "\n",
    "    def can_load_model (self, load=None):\n",
    "        return load if load is not None else (self.load_flag and self.load_model_flag)\n",
    "\n",
    "    def can_load_result (self, load=None):\n",
    "        return load if load is not None else (self.load_flag and self.load_result_flag)\n",
    "    \n",
    "    def can_save_model (self, save=None):\n",
    "        return save if save is not None else (self.save_flag and self.save_model_flag)\n",
    "    \n",
    "    def can_save_result (self, save=None, split=None):\n",
    "        split = self.split if split is None else split\n",
    "        return save if save is not None else (self.save_flag and \n",
    "                                              self.save_result_flag and\n",
    "                                              self.save_splits.get(split, True))\n",
    "        \n",
    "    def _load (self, path, load_func):\n",
    "        if (path is not None) and path.exists():\n",
    "            self.component.logger.info (f'loading from {path}')\n",
    "            return load_func (path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _save (self, path, save_func, item):\n",
    "        if (path is not None) and (save_func is not None):\n",
    "            self.component.logger.info (f'saving to {path}')\n",
    "            # create parent directory if it does not exist\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # save data using save_func\n",
    "            try:\n",
    "                save_func (item, path)\n",
    "            except Exception as e:\n",
    "                self.component.logger.warning (f'could not write to {path}, exception: {e}')\n",
    "\n",
    "    # ********************************\n",
    "    # setters\n",
    "    # ********************************\n",
    "    def set_split (self, split):\n",
    "        self.split = split\n",
    "        \n",
    "    def set_save_splits (self, save_splits):\n",
    "        self.save_splits = save_splits\n",
    "        \n",
    "    def set_path_results (self, path_results):\n",
    "        self.path_results = path_results\n",
    "        if self.path_results is not None:\n",
    "            self.path_results = Path(self.path_results).resolve()\n",
    "            if self.path_models is None:\n",
    "                self.set_path_models (path_results)\n",
    "        self.component.path_results = self.path_results\n",
    "        self.component.path_models = self.path_models\n",
    "                    \n",
    "    def set_path_models (self, path_models):\n",
    "        self.path_models = path_models\n",
    "        if self.path_models is not None:\n",
    "            self.path_models = Path(self.path_models).resolve()\n",
    "        else:\n",
    "            self.path_models = self.path_results\n",
    "        self.component.path_results = self.path_results\n",
    "        self.component.path_models = self.path_models\n",
    "            \n",
    "        \n",
    "    # global saving and loading\n",
    "    def set_save (self, save):\n",
    "        self.save_flag = save\n",
    "        if not save:\n",
    "            self.set_save_model (False)\n",
    "            self.set_save_result (False)\n",
    "    \n",
    "    def set_load (self, load):\n",
    "        self.load_flag = load\n",
    "        if not load:\n",
    "            self.set_load_model (False)\n",
    "            self.set_load_result (False)\n",
    "        \n",
    "    def set_save_model (self, save):\n",
    "        self.save_model_flag = save if self.save_flag else False\n",
    "    \n",
    "    def set_load_model (self, load):\n",
    "        self.load_model_flag = load if self.load_flag else False\n",
    "        \n",
    "    def set_save_result (self, save):\n",
    "        self.save_result_flag = save if self.save_flag else False\n",
    "    \n",
    "    def set_load_result (self, load):\n",
    "        self.load_result_flag = load if self.load_flag else False\n",
    "        \n",
    "    def set_fitting_file_name (self, name):\n",
    "        self.fitting_file_name = f'{name}_estimator{self.fitting_file_extension}'\n",
    "            \n",
    "    def set_result_file_name (self, name):\n",
    "        self.result_file_name = f'{name}_result{self.result_file_extension}'\n",
    "\n",
    "    def set_file_names (self, name):\n",
    "        self.set_fitting_file_name (name)\n",
    "        self.set_result_file_name (name)\n",
    "        \n",
    "    def _get_folder_name (self, folder):\n",
    "        if folder == '__class__':\n",
    "            if self.component is not None:\n",
    "                return self.component.name\n",
    "            else:\n",
    "                self.logger.warning (f'folder {folder} set on data_io without associated component')\n",
    "                return ''\n",
    "        else:\n",
    "            return folder\n",
    "        \n",
    "    def set_folder (self, folder):\n",
    "        self.folder = self._get_folder_name (folder)\n",
    "        \n",
    "    def chain_folders (self, folder):\n",
    "        folder = self._get_folder_name (folder)\n",
    "        if folder=='':\n",
    "            return\n",
    "        if self.folder == '':\n",
    "            self.folder = folder\n",
    "        else:\n",
    "            self.folder = f'{folder}/{self.folder}'\n",
    "            \n",
    "    def exists_result (self, split=None, path_results=None, result_file_name=None):\n",
    "        path_result_file = self.get_path_result_file (split=split, path_results=path_results,\n",
    "                                                      result_file_name=result_file_name)\n",
    "        return path_result_file is not None and path_result_file.exists ()\n",
    "    \n",
    "    def exists_estimator (self, path_models=None, fitting_file_name=None):\n",
    "        \"\"\"Load estimator parameters.\"\"\"        \n",
    "        path_model_file = self.get_path_model_file (path_models=path_models, \n",
    "                                                    fitting_file_name=fitting_file_name)\n",
    "        return path_model_file is not None and path_model_file.exists ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca920f9-f496-4092-bc71-dbdac026fce1",
   "metadata": {},
   "source": [
    "#### get_path_result_file, get_path_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8f09e-570a-486b-a87e-cc496d97ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "#@pytest.mark.my_mark\n",
    "class DummyComponent:\n",
    "    def __init__ (self, **kwargs):\n",
    "        self.name = 'my_component'\n",
    "    def obtain_config_params (self, **kwargs):\n",
    "        return kwargs\n",
    "\n",
    "dummy_component = DummyComponent ()\n",
    "\n",
    "def test_data_io_folder ():\n",
    "    d = DataIO (component=dummy_component)\n",
    "    assert d.folder==''\n",
    "\n",
    "    assert d.get_path_result_file () is None \n",
    "    assert d.get_path_model_file () is None \n",
    "\n",
    "    d = DataIO (component=dummy_component, \n",
    "            path_results ='/path/to/results',\n",
    "            path_models='/other_path/example')\n",
    "    assert d.get_path_result_file () == Path('/path/to/results/whole/my_component_result.pk')\n",
    "\n",
    "    assert d.get_path_model_file () == Path('/other_path/example/models/my_component_estimator.pk')\n",
    "\n",
    "    d.folder = 'my_folder'\n",
    "    assert d.get_path_result_file () == Path('/path/to/results/my_folder/whole/my_component_result.pk')\n",
    "\n",
    "    assert d.get_path_model_file () == Path('/other_path/example/my_folder/models/my_component_estimator.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a82a93-537f-4833-ab44-91b23ee7986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_data_io_folder, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2469e-0409-4b4c-a662-7fb2d61377c3",
   "metadata": {},
   "source": [
    "#### folder chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfcf44-eae4-42c6-9378-e0bb6c94980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "#@pytest.mark.my_mark\n",
    "def test_data_io_chaining ():\n",
    "    d = DataIO (component=dummy_component)\n",
    "    d.chain_folders ('first')\n",
    "    assert d.folder=='first'\n",
    "    d.chain_folders ('second')\n",
    "    assert d.folder=='second/first'\n",
    "    d.chain_folders ('third')\n",
    "    assert d.folder=='third/second/first'\n",
    "\n",
    "    d.folder\n",
    "\n",
    "    d.chain_folders ('')\n",
    "    assert d.folder=='third/second/first'\n",
    "\n",
    "    d = DataIO (component=dummy_component)\n",
    "    d.chain_folders ('')\n",
    "    assert d.folder==''\n",
    "\n",
    "    d = DataIO (component=dummy_component, folder='new')\n",
    "    d.chain_folders ('')\n",
    "    assert d.folder=='new'\n",
    "    d.chain_folders ('other')\n",
    "    assert d.folder=='other/new'\n",
    "\n",
    "    d = DataIO (component=dummy_component, folder='new',\n",
    "                path_results ='/path/to/results',\n",
    "                path_models='/other_path/example')\n",
    "    d.chain_folders ('other')\n",
    "\n",
    "    assert d.get_path_result_file () == Path('/path/to/results/other/new/whole/my_component_result.pk')\n",
    "\n",
    "    assert d.get_path_model_file () == Path('/other_path/example/other/new/models/my_component_estimator.pk')\n",
    "    \n",
    "    d = DataIO (component=dummy_component, folder='__class__',\n",
    "                path_results ='/path/to/results',\n",
    "                path_models='/other_path/example')\n",
    "    d.chain_folders ('other')\n",
    "    assert d.folder == 'other/my_component'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8cc492-25ad-4312-8dda-e56f925dbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_data_io_chaining, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0542747",
   "metadata": {},
   "source": [
    "## Subclasses of DataIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbe159-ba7f-431f-af7c-466e52590f5d",
   "metadata": {},
   "source": [
    "### PandasIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PandasIO (DataIO):\n",
    "    \"\"\"\n",
    "    Saves results in parquet format by default.\n",
    "    \n",
    "    Results are supposed to be in Pandas DataFrame format.\n",
    "    \"\"\"\n",
    "    def __init__ (self,\n",
    "                  result_file_extension='.parquet',\n",
    "                  result_load_func=pd.read_parquet,\n",
    "                  result_save_func=save_multi_index_parquet,\n",
    "                  **kwargs):\n",
    "        \n",
    "        super().__init__ (result_file_extension=result_file_extension,\n",
    "                          result_load_func=result_load_func,\n",
    "                          result_save_func=result_save_func,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f868b35a-9cfe-48a2-8d8e-d98df1824e98",
   "metadata": {},
   "source": [
    "### PickleIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PickleIO (DataIO):\n",
    "    \"\"\"\n",
    "    DataIO that uses pickle format for saving / loading the estimator. \n",
    "    \n",
    "    It does not restrict the format for saving / loading the result of \n",
    "    the transformation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__ (self,\n",
    "                  fitting_file_extension='.pk',\n",
    "                  fitting_load_func=joblib.load,\n",
    "                  fitting_save_func=joblib.dump,\n",
    "                  \n",
    "                  result_file_extension='.pk',\n",
    "                  result_load_func=joblib.load,\n",
    "                  result_save_func=joblib.dump,\n",
    "                  **kwargs):\n",
    "        \n",
    "        super().__init__ (fitting_file_extension=fitting_file_extension,\n",
    "                          fitting_load_func=fitting_load_func,\n",
    "                          fitting_save_func=fitting_save_func,\n",
    "                          \n",
    "                          result_file_extension=result_file_extension,\n",
    "                          result_load_func=result_load_func,\n",
    "                          result_save_func=result_save_func,\n",
    "                          **kwargs)\n",
    "        \n",
    "SklearnIO = PickleIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04682ef6-b74e-4d96-bfac-068d28705c40",
   "metadata": {},
   "source": [
    "### NoSaverIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NoSaverIO (DataIO):\n",
    "    \"\"\"DataIO that does not load or save anything.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  load=False,\n",
    "                  save=False,\n",
    "                  force_load=False,\n",
    "                  force_save=False,\n",
    "                  **kwargs):\n",
    "        super().__init__ (load=force_load if force_load else False,\n",
    "                          save=force_save if force_save else False,\n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0d593-7156-44c0-9da1-30514548fd71",
   "metadata": {},
   "source": [
    "## Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879e846-731b-46f1-8109-be8986efbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def data_io_factory (data_io, component=None, **kwargs):\n",
    "    if type(data_io) is str:\n",
    "        cls = eval(data_io)\n",
    "    elif type(data_io) is type:\n",
    "        cls = data_io\n",
    "    elif isinstance (data_io, DataIO):\n",
    "        data_io = copy.copy(data_io)\n",
    "        data_io.setup (component=component)\n",
    "        return data_io\n",
    "    else:\n",
    "        raise ValueError (f'invalid converter {data_io}, must be str, '\n",
    "                           'class or object instance of DataIO')\n",
    "    data_io = cls(component=component, **kwargs)\n",
    "    return data_io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71026233-ef51-4fad-a23e-dd20c49f9727",
   "metadata": {},
   "source": [
    "### Example / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130e42a-a11d-457f-b6b4-74f702b86798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "#@pytest.mark.reference_fails\n",
    "def test_data_io_factory ():\n",
    "    data_io = data_io_factory ('NoSaverIO', force_save=False, save=True)\n",
    "    assert type(data_io) is NoSaverIO\n",
    "    assert data_io.save_flag is False\n",
    "\n",
    "    data_io = data_io_factory ('NoSaverIO', force_save=True, save=True)\n",
    "    assert type(data_io) is NoSaverIO\n",
    "    assert data_io.save_flag is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c2a84-769f-4134-9bff-6b9bedb3deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_data_io_factory, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51387c0",
   "metadata": {},
   "source": [
    "## ModelPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ModelPlotter ():\n",
    "    \"\"\"Helper class that provides information about the component used for plotting the pipeline diagram.\"\"\"\n",
    "    def __init__ (self,\n",
    "                  component=None,\n",
    "                  # diagram options\n",
    "                  diagram_node_name = None,\n",
    "                  diagram_edge_name = '',\n",
    "                  diagram_module_path = '',\n",
    "                  **kwargs):\n",
    "\n",
    "        self.set_component (component)\n",
    "        \n",
    "        # diagram options\n",
    "        if diagram_node_name is None:\n",
    "            diagram_node_name = self.component.class_name\n",
    "        self.diagram_node_name = diagram_node_name\n",
    "        self.diagram_edge_name = diagram_edge_name\n",
    "        self.diagram_module_path = diagram_module_path\n",
    "        self.result_shape = {}\n",
    "        \n",
    "    def set_component (self, component=None):\n",
    "        self.component = component\n",
    "\n",
    "    def get_node_name (self):\n",
    "        return self.diagram_node_name\n",
    "\n",
    "    def get_edge_name (self, split=None, load_data=True):\n",
    "        split = self.component.data_io.split if split is None else split\n",
    "        result_shape = self.result_shape[split] if split in self.result_shape else None\n",
    "        if result_shape is None:\n",
    "            if load_data:\n",
    "                df = self.component.data_io.load_result(split=split)\n",
    "                if (df is not None) and hasattr(df, 'shape'):\n",
    "                    result_shape = self.result_shape[split] = df.shape\n",
    "\n",
    "        if result_shape is None:\n",
    "            return self.diagram_edge_name\n",
    "        else:\n",
    "            return f'{self.diagram_edge_name} {result_shape}'\n",
    "\n",
    "    def get_module_path (self):\n",
    "        return self.diagram_module_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a3c5a-109c-4e73-8b7d-92d444cf602c",
   "metadata": {},
   "source": [
    "## Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60cc7c-3499-4278-9b71-3e6a76101951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Profiler ():\n",
    "    def __init__ (self, component, do_profiling=True, **kwargs):\n",
    "        self.component = component\n",
    "        self.name = component.name\n",
    "        if hasattr(component, 'hierarchy_level'):\n",
    "            self.hierarchy_level = component.hierarchy_level\n",
    "        else:\n",
    "            self.hierarchy_level = 0\n",
    "        self.do_profiling=do_profiling\n",
    "        self.times = Bunch(sum=pd.DataFrame (),\n",
    "                          max=pd.DataFrame (),\n",
    "                          min=pd.DataFrame (),\n",
    "                          number=pd.DataFrame ())\n",
    "        keys = list(self.times.keys()).copy()\n",
    "        for k in keys:\n",
    "            self.times[f'novh_{k}']=pd.DataFrame ()\n",
    "    \n",
    "    def start_timer (self):\n",
    "        self.time = time.time()\n",
    "        \n",
    "    def start_no_overhead_timer (self):\n",
    "        self.no_overhead_time = time.time()\n",
    "        \n",
    "    def finish_timer (self, method, split):\n",
    "        self._finish_timer (method, split, suffix='', measured_time=self.time)\n",
    "        \n",
    "    def finish_no_overhead_timer (self, method, split):\n",
    "        self._finish_timer (method, split, suffix='novh_', measured_time=self.no_overhead_time)\n",
    "    \n",
    "    def _finish_timer (self, method, split, suffix='', measured_time=None):\n",
    "        if method.startswith('_'):\n",
    "            method = method[1:]\n",
    "        total_time = time.time() - measured_time\n",
    "        df=self.times[f'{suffix}sum']\n",
    "        if method in df.index and split in df.columns:\n",
    "            df.loc[method, split] += total_time\n",
    "            self.times[f'{suffix}number'].loc[method, split] += 1\n",
    "            self.times[f'{suffix}max'].loc[method, split] = max(self.times[f'{suffix}max'].loc[method, split], total_time)\n",
    "            self.times[f'{suffix}min'].loc[method, split] = min(self.times[f'{suffix}min'].loc[method, split], total_time)\n",
    "        else:\n",
    "            df.loc[method, split] = total_time\n",
    "            self.times[f'{suffix}number'].loc[method, split] = 1\n",
    "            self.times[f'{suffix}max'].loc[method, split] = total_time\n",
    "            self.times[f'{suffix}min'].loc[method, split] = total_time\n",
    "            \n",
    "    def _compute_avg (self, df_sum, df_number):\n",
    "        df_avg = df_sum.copy()\n",
    "        columns = [c for c in df_avg.columns if c != ('leaf', '')]\n",
    "        df_avg[columns] = df_avg[columns] / df_number[columns]\n",
    "        return df_avg\n",
    "            \n",
    "    def retrieve_times (self, is_leaf=False):\n",
    "        retrieved_times = Bunch()\n",
    "        for k in self.times:\n",
    "            df = self.times[k]\n",
    "            columns = pd.MultiIndex.from_product([list(df.columns),list(df.index)])\n",
    "            index = pd.MultiIndex.from_product([[self.hierarchy_level], [self.name]])\n",
    "            df = pd.DataFrame (index=index, \n",
    "                               columns = columns, data=df.values.reshape(1,-1))\n",
    "            df['leaf']=is_leaf\n",
    "            retrieved_times[k] = df\n",
    "        \n",
    "        retrieved_times['avg']= self._compute_avg (retrieved_times['sum'], \n",
    "                                                   retrieved_times['number'])\n",
    "        retrieved_times['novh_avg']= self._compute_avg (retrieved_times['novh_sum'], \n",
    "                                                        retrieved_times['novh_number'])\n",
    "        return retrieved_times\n",
    "    \n",
    "    def combine_times (self, df_list):\n",
    "        df_dict = Bunch()\n",
    "        for k in df_list[0].keys():\n",
    "            df_dict[k] = pd.concat([x[k] for x in df_list])\n",
    "            df_dict[k] = df_dict[k].sort_index()\n",
    "        df_novh_avg = df_dict['novh_avg']\n",
    "        df_dict['no_overhead_total'] = df_novh_avg[df_novh_avg.leaf].sum(axis=0)\n",
    "        \n",
    "        df_avg = self.retrieve_times ()['avg']\n",
    "        df_dict['overhead_total'] = df_avg -  df_dict['no_overhead_total']\n",
    "        df_dict['no_overhead_total'] = df_dict['no_overhead_total'].to_frame().T\n",
    "        return df_dict\n",
    "    \n",
    "    def analyze_overhead (self, df_dict):\n",
    "        def data (df): \n",
    "            df = df[[c for c in df if c[0] != 'leaf']]\n",
    "            return df.values\n",
    "        component_ovh = data (df_dict.avg)-data(df_dict.novh_avg)\n",
    "        \n",
    "        df_dict['component_ovh'] = pd.DataFrame (component_ovh,\n",
    "                                                 columns=[c for c in df_dict.avg if c[0] != 'leaf'],\n",
    "                                                 index=df_dict.avg.index)\n",
    "        is_leaf=df_dict.avg[('leaf','')]\n",
    "        \n",
    "        total_non_leaf_time = np.nansum(data(df_dict.avg[~is_leaf]).ravel())\n",
    "        total_leaf_time = np.nansum(data(df_dict.avg[is_leaf]).ravel())\n",
    "        total_non_leaf_ovh = total_non_leaf_time - total_leaf_time\n",
    "\n",
    "        total_ovh=np.nansum(data(df_dict.overhead_total).ravel())\n",
    "\n",
    "        total_leaf_ovh = np.nansum(component_ovh[is_leaf.values].ravel())\n",
    "        \n",
    "        df_dict['overhead_summary'] = pd.DataFrame ({'total': [total_ovh],\n",
    "                                                     'total leaf': [total_leaf_ovh],\n",
    "                                                     'total non-leaf': [total_non_leaf_ovh],\n",
    "                                                     'remaining / unexplained': [\n",
    "                                                         total_ovh-total_leaf_ovh-total_non_leaf_ovh]},\n",
    "                                                    index=[0])\n",
    "        return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bee117-ae0c-44c3-8a49-a9648573d5b9",
   "metadata": {},
   "source": [
    "### Example / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc196b8-bba3-4f04-ba71-92bfb3a0e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "#@pytest.mark.reference_fails\n",
    "def test_profiler ():\n",
    "    class A():\n",
    "        def __init__ (self, name='comp_a', time=1, hierarchy_level=0, **kwargs):\n",
    "            self.name = name\n",
    "            self.time = time\n",
    "            self.hierarchy_level = hierarchy_level\n",
    "            self.profiler = Profiler (self, **kwargs)\n",
    "        def apply (self, split='test', method='apply'):\n",
    "            self.profiler.start_timer ()\n",
    "            self.profiler.start_no_overhead_timer ()\n",
    "            time.sleep(self.time)\n",
    "            self.profiler.finish_no_overhead_timer (method, split)\n",
    "            time.sleep(self.time/10)\n",
    "            self.profiler.finish_timer (method, split)\n",
    "        def fit (self, split='training', method='fit'):\n",
    "            self.profiler.start_timer ()\n",
    "            self.profiler.start_no_overhead_timer ()\n",
    "            time.sleep(self.time*2)\n",
    "            self.profiler.finish_no_overhead_timer (method, split)\n",
    "            time.sleep(self.time/10)\n",
    "            self.profiler.finish_timer (method, split)\n",
    "\n",
    "    a = A(name='comp_a', time=0.25)\n",
    "    a.fit ()\n",
    "    a.apply ()\n",
    "    display(a.profiler.times['sum'])\n",
    "\n",
    "    #assert np.floor(a.profiler.times.sum.loc['fit','training']*10)==5 and np.floor(a.profiler.times.sum.loc['apply','test']*100)==28\n",
    "    assert a.profiler.times.number.loc['fit','training']==1.0 and a.profiler.times.number.loc['apply','test']==1.0\n",
    "\n",
    "    a.apply ()\n",
    "\n",
    "    #assert np.floor(a.profiler.times.sum.loc['fit','training']*10)==5 and np.floor(a.profiler.times.sum.loc['apply','test']*100)==55\n",
    "    assert a.profiler.times.number.loc['fit','training']==1.0 and a.profiler.times.number.loc['apply','test']==2.0\n",
    "    assert np.isnan(a.profiler.times.number.loc['fit','test']) and np.isnan(a.profiler.times.number.loc['apply','training'])\n",
    "\n",
    "    df = a.profiler.retrieve_times(is_leaf=True)\n",
    "\n",
    "    #assert float(np.floor(df.avg.loc[:,('test','apply')]*100))==27\n",
    "\n",
    "    #assert float(np.floor(df.sum.loc[:,('test','apply')]*100))==55\n",
    "\n",
    "    assert ((df.max >= df.min) | df.max.isna()).all().all()\n",
    "\n",
    "    assert (df.max.loc[:,('test','apply')] > df.min.loc[:,('test','apply')]).all()\n",
    "\n",
    "    b = A(name='comp_b', time=0.30)\n",
    "    b.apply (split='validation', method='fit_apply')\n",
    "    b.fit ()\n",
    "\n",
    "    df = a.profiler.retrieve_times(is_leaf=True)\n",
    "    df2 = b.profiler.retrieve_times(is_leaf=True)\n",
    "    dfd = b.profiler.combine_times ([df, df2])\n",
    "    display(dfd.avg)\n",
    "\n",
    "    assert dfd.avg.shape==(2,8)\n",
    "    assert (dfd.avg.index.get_level_values(0)==[0,0]).all()\n",
    "    assert (dfd.avg.index.get_level_values(1)==['comp_a', 'comp_b']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838449bf-d913-4d49-b815-f6a5f6ea2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_profiler, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788d42d-a70e-4e91-8f29-aa07c99f56c9",
   "metadata": {},
   "source": [
    "## Comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a907ff-65aa-4fb1-9a6a-76e244fbada6",
   "metadata": {},
   "source": [
    "### Comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05335cc-9133-4583-89da-e6cfe2c6d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Comparator ():\n",
    "    def __init__ (self, component=None, data_io=None, name='comparator', **kwargs):\n",
    "        if component is not None:\n",
    "            self.component = component\n",
    "            self.logger = component.logger\n",
    "            self.name = component.name\n",
    "            self.data_io = component.data_io\n",
    "        else:\n",
    "            self.component = None\n",
    "            self.logger = set_logger ('comparator', filename=None)\n",
    "            self.name = name\n",
    "            if data_io is not None:\n",
    "                self.data_io = data_io_factory (data_io)\n",
    "        \n",
    "    def compare_objects (self, left, right, message='', **kwargs):\n",
    "        if left != right:\n",
    "            return message + f'{left}!={right}'\n",
    "        else:\n",
    "            return ''\n",
    "        \n",
    "    def compare (self, left, right, message='', rtol=1e-07, atol=0, **kwargs):\n",
    "        if not type(left)==type(right):\n",
    "            return f'{message}{type(left)}!={type(right)}'\n",
    "        if isinstance(left, tuple) or isinstance(left, list):\n",
    "            try:\n",
    "                left = np.array(left, dtype=float)\n",
    "                right = np.array(right, dtype=float)\n",
    "            except:\n",
    "                for i, (x, y) in enumerate(zip(left, right)):\n",
    "                    result = self.compare (x, y, message=message+f'[{i}] ', rtol=rtol, atol=atol, **kwargs)\n",
    "                    if len(result) > 0:\n",
    "                        return result\n",
    "                return ''\n",
    "        elif isinstance (left, dict):\n",
    "            if sorted(left.keys()) != sorted(right.keys()):\n",
    "                return f'{message}{sorted(left.keys())}!={sorted(right.keys())}'\n",
    "            for k in left:\n",
    "                result = self.compare (left[k], right[k], message=message+f'[{k}] ', rtol=rtol, atol=atol,\n",
    "                                       **kwargs)\n",
    "                if len(result) > 0:\n",
    "                    return result\n",
    "            return ''\n",
    "            \n",
    "        if isinstance (left, np.ndarray):\n",
    "            if (left.dtype == np.object) and (right.dtype == np.object):\n",
    "                if left.tolist() != right.tolist():\n",
    "                    return message + f'{left}!={right}'\n",
    "                else:\n",
    "                    return ''\n",
    "            else:\n",
    "                try:\n",
    "                    np.testing.assert_allclose (left, right, rtol=rtol, atol=atol)\n",
    "                    return ''\n",
    "                except AssertionError as e:\n",
    "                    return message + str(e)\n",
    "        elif isinstance (left, pd.DataFrame):\n",
    "            try:\n",
    "                pd.testing.assert_frame_equal (left, right)\n",
    "                return ''\n",
    "            except AssertionError as e:\n",
    "                return message + str(e)\n",
    "        elif isinstance (left, str):\n",
    "            if left != right:\n",
    "                return message + f'{left}!={right}'\n",
    "            else:\n",
    "                return ''\n",
    "        elif np.isscalar (left) and np.isreal (left):\n",
    "            if np.isclose (left, right, rtol=rtol, atol=atol):\n",
    "                return ''\n",
    "            else:\n",
    "                return message + f'{left} not close to {right}'\n",
    "        else:\n",
    "            return self.compare_objects (left, right, message=message, rtol=1e-07, atol=0, **kwargs)\n",
    "        return ''\n",
    "        \n",
    "    def assert_equal (self, item1, item2=None, split=None, raise_error=True, verbose=None, \n",
    "                      **kwargs):\n",
    "        \"\"\"\n",
    "        Check whether the transformed data is the same as the reference data stored in given path.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path_reference_results: str\n",
    "            Path where reference results are stored. The path does not include the \n",
    "            file name, since this is stored as a field of data_io.\n",
    "        assert_equal_func: function, optional\n",
    "            Function used to check whether the values are the same. By defaut, \n",
    "            `pd.testing.assert_frame_equal` is used, which assumes the data type is \n",
    "            DataFrame.\n",
    "        \n",
    "        \"\"\"\n",
    "        if verbose is not None:\n",
    "            self.logger.setLevel(get_logging_level (verbose))\n",
    "        self.logger.info (f'comparing results for {self.name}')\n",
    "        if item2 is None:\n",
    "            item2 = item1\n",
    "            self.logger.info (f'loading our results...')\n",
    "            item1 = self.data_io.load_result (split=split)\n",
    "        elif type(item1) is str:\n",
    "            self.logger.info (f'loading others results...')\n",
    "            item1 = self.data_io.load_result (split=split, path_results=item1)\n",
    "        if type(item2) is str:\n",
    "            item2 = self.data_io.load_result (split=split, path_results=item2)\n",
    "        difference = self.compare (item1, item2, **kwargs)\n",
    "        if len(difference) == 0:\n",
    "            self.logger.info (f'Results are equal.\\n')\n",
    "            if not raise_error:\n",
    "                if verbose is not None:\n",
    "                    self.logger.setLevel(get_logging_level (self.component.verbose))\n",
    "                return True\n",
    "        else:\n",
    "            if raise_error:\n",
    "                raise AssertionError (f'Component {self.name} => results are different: {difference}')\n",
    "            else:\n",
    "                self.logger.warning (f'Component {self.name} => results are different: {difference}')\n",
    "                if verbose is not None:\n",
    "                    self.logger.setLevel(get_logging_level (self.component.verbose))\n",
    "                return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b303b65-5f25-4a2c-a4c8-222705c8cc23",
   "metadata": {},
   "source": [
    "### Example / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff2ab1-13e1-44a3-b25f-35619e285052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "class MyTransformComparator ():\n",
    "    def __init__ (self, noise=1e-10, different = False, **kwargs):\n",
    "        self.noise = noise\n",
    "        self.different = different\n",
    "        self.name = 'mine'\n",
    "        self.logger = None\n",
    "        self.data_io = None\n",
    "\n",
    "    def _generate_noise (self):\n",
    "        while True:\n",
    "            noise = np.random.rand() * self.noise\n",
    "            if noise > self.noise/10:\n",
    "                break\n",
    "        return noise\n",
    "\n",
    "    def __call__ (self):\n",
    "        df = pd.DataFrame ([[1.0,2.0],[3.0,4.0]], columns=['a','b']) + self._generate_noise ()\n",
    "        if self.different:\n",
    "            df = df+10\n",
    "        x = np.array([[10.0,20.0],[30.0,40.0]]) + self._generate_noise ()\n",
    "        result = dict(sequence=[[1.0,2.0], x+1, dict(vector=x, data=df)],\n",
    "                      array=x+10)\n",
    "        return result\n",
    "        \n",
    "def test_comparator ():\n",
    "    tr = MyTransformComparator ()\n",
    "    tr2= MyTransformComparator ()\n",
    "    comp = Comparator (tr)\n",
    "    result = comp.compare (tr(), tr2())\n",
    "    assert len(result)==0\n",
    "\n",
    "    tr2= MyTransformComparator (different=True)\n",
    "    result = comp.compare (tr(), tr2())\n",
    "    print (result)\n",
    "    assert len(result)>0\n",
    "\n",
    "    tr2= MyTransformComparator (noise=1e-10)\n",
    "    result = comp.compare (tr(), tr2())\n",
    "    assert len(result)==0\n",
    "\n",
    "    tr2= MyTransformComparator (noise=1e-1)\n",
    "    result = comp.compare (tr(), tr2())\n",
    "    print(result)\n",
    "    assert len(result)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910f258-51ea-4306-9380-def891de99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_comparator, tag='dummy', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107868c0-282c-4dac-a115-53587147ba42",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c96e08-8f84-492c-a54f-8b06e819aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compare (left, right, **kwargs):\n",
    "    c = Comparator ()\n",
    "    return c.compare (left, right, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4eba1-ebef-4dea-9235-4e626e8b80c4",
   "metadata": {},
   "source": [
    "### Example / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713cd7c-903d-4c31-b17c-faf7fd67959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "def test_compare ():\n",
    "    tr = MyTransformComparator ()\n",
    "    tr2= MyTransformComparator ()\n",
    "    result = compare (tr(), tr2())\n",
    "    assert len(result)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684c2a0-9df0-44ab-9824-6f41f3ee3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running test_compare\n"
     ]
    }
   ],
   "source": [
    "tst.run (test_compare, tag='dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08fa7b5-e599-4b7a-a0b6-223ecd940bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports tests.core.test_utils\n",
    "#@pytest.mark.reference_fails\n",
    "def test_comparator2 ():\n",
    "    comp = Comparator ()\n",
    "    X = np.array([1,2,3])\n",
    "    Y = np.array([1,2,3])\n",
    "    comp.assert_equal (X, Y)\n",
    "    \n",
    "    #os.makedirs ('test_comparator2')\n",
    "    #joblib.dump (X, 'test_comparator2/res1.pk')\n",
    "    #joblib.dump (Y, 'test_comparator2/res1.pk')\n",
    "    comp.assert_equal (X, Y, data_io='PickleIO', name='res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a18cc1-3bf8-4968-ab73-0c2b784f32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.run (test_comparator2, tag='dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42801546-0bc5-4c68-9d62-eaa9786039ad",
   "metadata": {},
   "source": [
    "## camel_to_snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ad9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def camel_to_snake (name):\n",
    "    \"\"\"\n",
    "    Convert CamelCase to snake_case.\n",
    "\n",
    "    Used for converting classes names to file names where \n",
    "    the corresponding computation is stored:\n",
    "        https://stackoverflow.com/questions/1175208/elegant-python-function-to-convert-camelcase-to-snake-case\n",
    "    \"\"\"\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def snake_to_camel (name):\n",
    "    return ''.join(word.title() for word in name.split('_'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsblocks)",
   "language": "python",
   "name": "dsblocks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
